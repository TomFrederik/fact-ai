Cuda available? False
2021-03-19 11:21:42,617	INFO services.py:1173 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
2021-03-19 11:21:46,642	WARNING function_runner.py:540 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.
== Status ==
Memory usage on this node: 5.9/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 2/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 1/180 (1 RUNNING)
+--------------------+----------+-------+--------------+-------+-------+----------+
| Trial name         | status   | loc   |   batch_size |   eta |    lr |   sec_lr |
|--------------------+----------+-------+--------------+-------+-------+----------|
| _inner_e8deb_00000 | RUNNING  |       |           32 |     0 | 0.001 |    0.001 |
+--------------------+----------+-------+--------------+-------+-------+----------+


[2m[36m(pid=35855)[0m Starting run with seed 0 - lr 0.001 - sec_lr 0.001 - bs 128
[2m[36m(pid=35861)[0m Starting run with seed 0 - lr 0.001 - sec_lr 0.001 - bs 32
[2m[36m(pid=35855)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35855)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35861)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35861)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35855)[0m GPU available: False, used: False
[2m[36m(pid=35855)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35861)[0m GPU available: False, used: False
[2m[36m(pid=35861)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35848)[0m Starting run with seed 0 - lr 0.01 - sec_lr 0.001 - bs 32
[2m[36m(pid=35848)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35848)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35848)[0m GPU available: False, used: False
[2m[36m(pid=35848)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35855)[0m 
[2m[36m(pid=35855)[0m   | Name      | Type              | Params
[2m[36m(pid=35855)[0m ------------------------------------------------
[2m[36m(pid=35855)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35855)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35855)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35855)[0m ------------------------------------------------
[2m[36m(pid=35855)[0m 8.7 K     Trainable params
[2m[36m(pid=35855)[0m 0         Non-trainable params
[2m[36m(pid=35855)[0m 8.7 K     Total params
[2m[36m(pid=35857)[0m Starting run with seed 0 - lr 0.001 - sec_lr 0.001 - bs 64
[2m[36m(pid=35855)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35855)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35861)[0m 
[2m[36m(pid=35861)[0m   | Name      | Type              | Params
[2m[36m(pid=35861)[0m ------------------------------------------------
[2m[36m(pid=35861)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35861)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35861)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35861)[0m ------------------------------------------------
[2m[36m(pid=35861)[0m 8.7 K     Trainable params
[2m[36m(pid=35861)[0m 0         Non-trainable params
[2m[36m(pid=35861)[0m 8.7 K     Total params
[2m[36m(pid=35861)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35861)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35857)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35857)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35857)[0m GPU available: False, used: False
[2m[36m(pid=35857)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35848)[0m 
[2m[36m(pid=35848)[0m   | Name      | Type              | Params
[2m[36m(pid=35848)[0m ------------------------------------------------
[2m[36m(pid=35848)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35848)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35848)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35848)[0m ------------------------------------------------
[2m[36m(pid=35848)[0m 8.7 K     Trainable params
[2m[36m(pid=35848)[0m 0         Non-trainable params
[2m[36m(pid=35848)[0m 8.7 K     Total params
[2m[36m(pid=35848)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35848)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35857)[0m 
[2m[36m(pid=35857)[0m   | Name      | Type              | Params
[2m[36m(pid=35857)[0m ------------------------------------------------
[2m[36m(pid=35857)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35857)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35857)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35857)[0m ------------------------------------------------
[2m[36m(pid=35857)[0m 8.7 K     Trainable params
[2m[36m(pid=35857)[0m 0         Non-trainable params
[2m[36m(pid=35857)[0m 8.7 K     Total params
[2m[36m(pid=35857)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35857)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35852)[0m Starting run with seed 0 - lr 0.1 - sec_lr 0.001 - bs 32
[2m[36m(pid=35873)[0m Starting run with seed 0 - lr 0.01 - sec_lr 0.001 - bs 512
[2m[36m(pid=35852)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35852)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35873)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35873)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35852)[0m GPU available: False, used: False
[2m[36m(pid=35852)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35873)[0m GPU available: False, used: False
[2m[36m(pid=35873)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35852)[0m 
[2m[36m(pid=35852)[0m   | Name      | Type              | Params
[2m[36m(pid=35852)[0m ------------------------------------------------
[2m[36m(pid=35852)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35852)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35852)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35852)[0m ------------------------------------------------
[2m[36m(pid=35852)[0m 8.7 K     Trainable params
[2m[36m(pid=35852)[0m 0         Non-trainable params
[2m[36m(pid=35852)[0m 8.7 K     Total params
[2m[36m(pid=35852)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35852)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35873)[0m 
[2m[36m(pid=35873)[0m   | Name      | Type              | Params
[2m[36m(pid=35873)[0m ------------------------------------------------
[2m[36m(pid=35873)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35873)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35873)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35873)[0m ------------------------------------------------
[2m[36m(pid=35873)[0m 8.7 K     Trainable params
[2m[36m(pid=35873)[0m 0         Non-trainable params
[2m[36m(pid=35873)[0m 8.7 K     Total params
[2m[36m(pid=35873)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35873)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35889)[0m Starting run with seed 0 - lr 1 - sec_lr 0.001 - bs 32
[2m[36m(pid=35889)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35889)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35848)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35848)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35889)[0m GPU available: False, used: False
[2m[36m(pid=35889)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35860)[0m Starting run with seed 0 - lr 2 - sec_lr 0.001 - bs 256
[2m[36m(pid=35851)[0m Starting run with seed 0 - lr 0.1 - sec_lr 0.001 - bs 512
[2m[36m(pid=35861)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35861)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35860)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35860)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35851)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35851)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35851)[0m GPU available: False, used: False
[2m[36m(pid=35851)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35855)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35855)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35889)[0m 
[2m[36m(pid=35889)[0m   | Name      | Type              | Params
[2m[36m(pid=35889)[0m ------------------------------------------------
[2m[36m(pid=35889)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35889)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35889)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35889)[0m ------------------------------------------------
[2m[36m(pid=35889)[0m 8.7 K     Trainable params
[2m[36m(pid=35889)[0m 0         Non-trainable params
[2m[36m(pid=35889)[0m 8.7 K     Total params
[2m[36m(pid=35889)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35889)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35860)[0m GPU available: False, used: False
[2m[36m(pid=35860)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35861)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35861)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35851)[0m 
[2m[36m(pid=35851)[0m   | Name      | Type              | Params
[2m[36m(pid=35851)[0m ------------------------------------------------
[2m[36m(pid=35851)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35851)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35851)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35851)[0m ------------------------------------------------
[2m[36m(pid=35851)[0m 8.7 K     Trainable params
[2m[36m(pid=35851)[0m 0         Non-trainable params
[2m[36m(pid=35851)[0m 8.7 K     Total params
[2m[36m(pid=35851)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35851)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35857)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35857)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35860)[0m 
[2m[36m(pid=35860)[0m   | Name      | Type              | Params
[2m[36m(pid=35860)[0m ------------------------------------------------
[2m[36m(pid=35860)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35860)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35860)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35860)[0m ------------------------------------------------
[2m[36m(pid=35860)[0m 8.7 K     Trainable params
[2m[36m(pid=35860)[0m 0         Non-trainable params
[2m[36m(pid=35860)[0m 8.7 K     Total params
[2m[36m(pid=35860)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35860)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35864)[0m Starting run with seed 0 - lr 2 - sec_lr 0.001 - bs 512
[2m[36m(pid=35885)[0m Starting run with seed 0 - lr 0.1 - sec_lr 0.001 - bs 64
[2m[36m(pid=35848)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35848)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35864)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35864)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35885)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35885)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35885)[0m GPU available: False, used: False
[2m[36m(pid=35885)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35881)[0m Starting run with seed 0 - lr 0.01 - sec_lr 0.001 - bs 256
[2m[36m(pid=35878)[0m Starting run with seed 0 - lr 0.01 - sec_lr 0.001 - bs 64
[2m[36m(pid=35856)[0m Starting run with seed 0 - lr 0.001 - sec_lr 0.001 - bs 256
[2m[36m(pid=35855)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35855)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35864)[0m GPU available: False, used: False
[2m[36m(pid=35864)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35881)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35881)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35878)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35878)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35856)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35856)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35857)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35857)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35885)[0m 
[2m[36m(pid=35885)[0m   | Name      | Type              | Params
[2m[36m(pid=35885)[0m ------------------------------------------------
[2m[36m(pid=35885)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35885)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35885)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35885)[0m ------------------------------------------------
[2m[36m(pid=35885)[0m 8.7 K     Trainable params
[2m[36m(pid=35885)[0m 0         Non-trainable params
[2m[36m(pid=35885)[0m 8.7 K     Total params
[2m[36m(pid=35885)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35885)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35881)[0m GPU available: False, used: False
[2m[36m(pid=35881)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35878)[0m GPU available: False, used: False
[2m[36m(pid=35878)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35856)[0m GPU available: False, used: False
[2m[36m(pid=35856)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35886)[0m Starting run with seed 0 - lr 0.001 - sec_lr 0.001 - bs 512
[2m[36m(pid=35864)[0m 
[2m[36m(pid=35864)[0m   | Name      | Type              | Params
[2m[36m(pid=35864)[0m ------------------------------------------------
[2m[36m(pid=35864)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35864)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35864)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35864)[0m ------------------------------------------------
[2m[36m(pid=35864)[0m 8.7 K     Trainable params
[2m[36m(pid=35864)[0m 0         Non-trainable params
[2m[36m(pid=35864)[0m 8.7 K     Total params
[2m[36m(pid=35864)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35864)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35886)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35886)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35840)[0m Starting run with seed 0 - lr 0.1 - sec_lr 0.001 - bs 256
[2m[36m(pid=35886)[0m GPU available: False, used: False
[2m[36m(pid=35886)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35840)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35840)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35881)[0m 
[2m[36m(pid=35881)[0m   | Name      | Type              | Params
[2m[36m(pid=35881)[0m ------------------------------------------------
[2m[36m(pid=35881)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35881)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35881)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35881)[0m ------------------------------------------------
[2m[36m(pid=35881)[0m 8.7 K     Trainable params
[2m[36m(pid=35881)[0m 0         Non-trainable params
[2m[36m(pid=35881)[0m 8.7 K     Total params
[2m[36m(pid=35878)[0m 
[2m[36m(pid=35878)[0m   | Name      | Type              | Params
[2m[36m(pid=35878)[0m ------------------------------------------------
[2m[36m(pid=35878)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35878)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35878)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35878)[0m ------------------------------------------------
[2m[36m(pid=35878)[0m 8.7 K     Trainable params
[2m[36m(pid=35878)[0m 0         Non-trainable params
[2m[36m(pid=35878)[0m 8.7 K     Total params
[2m[36m(pid=35878)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35878)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35827)[0m Starting run with seed 0 - lr 1 - sec_lr 0.001 - bs 512
[2m[36m(pid=35856)[0m 
[2m[36m(pid=35856)[0m   | Name      | Type              | Params
[2m[36m(pid=35856)[0m ------------------------------------------------
[2m[36m(pid=35856)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35856)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35856)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35856)[0m ------------------------------------------------
[2m[36m(pid=35856)[0m 8.7 K     Trainable params
[2m[36m(pid=35856)[0m 0         Non-trainable params
[2m[36m(pid=35856)[0m 8.7 K     Total params
[2m[36m(pid=35856)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35856)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35840)[0m GPU available: False, used: False
[2m[36m(pid=35840)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35827)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35827)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35827)[0m GPU available: False, used: False
[2m[36m(pid=35827)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35881)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35881)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35868)[0m Starting run with seed 0 - lr 0.1 - sec_lr 0.001 - bs 128
[2m[36m(pid=35871)[0m Starting run with seed 0 - lr 0.01 - sec_lr 0.001 - bs 128
[2m[36m(pid=35868)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35868)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35871)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35871)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35886)[0m 
[2m[36m(pid=35886)[0m   | Name      | Type              | Params
[2m[36m(pid=35886)[0m ------------------------------------------------
[2m[36m(pid=35886)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35886)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35886)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35886)[0m ------------------------------------------------
[2m[36m(pid=35886)[0m 8.7 K     Trainable params
[2m[36m(pid=35886)[0m 0         Non-trainable params
[2m[36m(pid=35886)[0m 8.7 K     Total params
[2m[36m(pid=35886)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35886)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35840)[0m 
[2m[36m(pid=35840)[0m   | Name      | Type              | Params
[2m[36m(pid=35840)[0m ------------------------------------------------
[2m[36m(pid=35840)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35840)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35840)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35840)[0m ------------------------------------------------
[2m[36m(pid=35840)[0m 8.7 K     Trainable params
[2m[36m(pid=35840)[0m 0         Non-trainable params
[2m[36m(pid=35840)[0m 8.7 K     Total params
[2m[36m(pid=35840)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35840)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35868)[0m GPU available: False, used: False
[2m[36m(pid=35868)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35871)[0m GPU available: False, used: False
[2m[36m(pid=35871)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35866)[0m Starting run with seed 0 - lr 2 - sec_lr 0.001 - bs 64
[2m[36m(pid=35833)[0m Starting run with seed 0 - lr 1 - sec_lr 0.001 - bs 256
[2m[36m(pid=35852)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35852)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35827)[0m 
[2m[36m(pid=35827)[0m   | Name      | Type              | Params
[2m[36m(pid=35827)[0m ------------------------------------------------
[2m[36m(pid=35827)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35827)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35827)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35827)[0m ------------------------------------------------
[2m[36m(pid=35827)[0m 8.7 K     Trainable params
[2m[36m(pid=35827)[0m 0         Non-trainable params
[2m[36m(pid=35827)[0m 8.7 K     Total params
[2m[36m(pid=35827)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35827)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35868)[0m 
[2m[36m(pid=35868)[0m   | Name      | Type              | Params
[2m[36m(pid=35868)[0m ------------------------------------------------
[2m[36m(pid=35868)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35868)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35868)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35868)[0m ------------------------------------------------
[2m[36m(pid=35868)[0m 8.7 K     Trainable params
[2m[36m(pid=35868)[0m 0         Non-trainable params
[2m[36m(pid=35868)[0m 8.7 K     Total params
[2m[36m(pid=35866)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35866)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35833)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35833)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35893)[0m Starting run with seed 0 - lr 1 - sec_lr 0.001 - bs 64
[2m[36m(pid=35821)[0m Starting run with seed 0 - lr 5 - sec_lr 0.001 - bs 32
[2m[36m(pid=35868)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35868)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35871)[0m 
[2m[36m(pid=35871)[0m   | Name      | Type              | Params
[2m[36m(pid=35871)[0m ------------------------------------------------
[2m[36m(pid=35871)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35871)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35871)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35871)[0m ------------------------------------------------
[2m[36m(pid=35871)[0m 8.7 K     Trainable params
[2m[36m(pid=35871)[0m 0         Non-trainable params
[2m[36m(pid=35871)[0m 8.7 K     Total params
[2m[36m(pid=35871)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35871)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35866)[0m GPU available: False, used: False
[2m[36m(pid=35866)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35833)[0m GPU available: False, used: False
[2m[36m(pid=35833)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35893)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35893)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35821)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35821)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35803)[0m Starting run with seed 0 - lr 2 - sec_lr 0.001 - bs 32
[2m[36m(pid=35893)[0m GPU available: False, used: False
[2m[36m(pid=35893)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35803)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35803)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35873)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35873)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35833)[0m 
[2m[36m(pid=35833)[0m   | Name      | Type              | Params
[2m[36m(pid=35833)[0m ------------------------------------------------
[2m[36m(pid=35833)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35833)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35833)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35833)[0m ------------------------------------------------
[2m[36m(pid=35833)[0m 8.7 K     Trainable params
[2m[36m(pid=35833)[0m 0         Non-trainable params
[2m[36m(pid=35833)[0m 8.7 K     Total params
[2m[36m(pid=35833)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35833)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35829)[0m Starting run with seed 0 - lr 1 - sec_lr 0.001 - bs 128
[2m[36m(pid=35821)[0m GPU available: False, used: False
[2m[36m(pid=35821)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35803)[0m GPU available: False, used: False
[2m[36m(pid=35803)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35829)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35829)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35852)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35852)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35866)[0m 
[2m[36m(pid=35866)[0m   | Name      | Type              | Params
[2m[36m(pid=35866)[0m ------------------------------------------------
[2m[36m(pid=35866)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35866)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35866)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35866)[0m ------------------------------------------------
[2m[36m(pid=35866)[0m 8.7 K     Trainable params
[2m[36m(pid=35866)[0m 0         Non-trainable params
[2m[36m(pid=35866)[0m 8.7 K     Total params
[2m[36m(pid=35866)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35866)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35829)[0m GPU available: False, used: False
[2m[36m(pid=35829)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35893)[0m 
[2m[36m(pid=35893)[0m   | Name      | Type              | Params
[2m[36m(pid=35893)[0m ------------------------------------------------
[2m[36m(pid=35893)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35893)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35893)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35893)[0m ------------------------------------------------
[2m[36m(pid=35893)[0m 8.7 K     Trainable params
[2m[36m(pid=35893)[0m 0         Non-trainable params
[2m[36m(pid=35893)[0m 8.7 K     Total params
[2m[36m(pid=35893)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35893)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35821)[0m 
[2m[36m(pid=35821)[0m   | Name      | Type              | Params
[2m[36m(pid=35821)[0m ------------------------------------------------
[2m[36m(pid=35821)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35821)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35821)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35821)[0m ------------------------------------------------
[2m[36m(pid=35821)[0m 8.7 K     Trainable params
[2m[36m(pid=35821)[0m 0         Non-trainable params
[2m[36m(pid=35821)[0m 8.7 K     Total params
[2m[36m(pid=35821)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35821)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35803)[0m 
[2m[36m(pid=35803)[0m   | Name      | Type              | Params
[2m[36m(pid=35803)[0m ------------------------------------------------
[2m[36m(pid=35803)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35803)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35803)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35803)[0m ------------------------------------------------
[2m[36m(pid=35803)[0m 8.7 K     Trainable params
[2m[36m(pid=35803)[0m 0         Non-trainable params
[2m[36m(pid=35803)[0m 8.7 K     Total params
[2m[36m(pid=35803)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35803)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35829)[0m 
[2m[36m(pid=35829)[0m   | Name      | Type              | Params
[2m[36m(pid=35829)[0m ------------------------------------------------
[2m[36m(pid=35829)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35829)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35829)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35829)[0m ------------------------------------------------
[2m[36m(pid=35829)[0m 8.7 K     Trainable params
[2m[36m(pid=35829)[0m 0         Non-trainable params
[2m[36m(pid=35829)[0m 8.7 K     Total params
[2m[36m(pid=35889)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35889)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35851)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35851)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35829)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35829)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35858)[0m Starting run with seed 0 - lr 2 - sec_lr 0.001 - bs 128
[2m[36m(pid=35873)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35873)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35858)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35858)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35858)[0m GPU available: False, used: False
[2m[36m(pid=35858)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35860)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35860)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35851)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35851)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35858)[0m 
[2m[36m(pid=35858)[0m   | Name      | Type              | Params
[2m[36m(pid=35858)[0m ------------------------------------------------
[2m[36m(pid=35858)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35858)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35858)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35858)[0m ------------------------------------------------
[2m[36m(pid=35858)[0m 8.7 K     Trainable params
[2m[36m(pid=35858)[0m 0         Non-trainable params
[2m[36m(pid=35858)[0m 8.7 K     Total params
[2m[36m(pid=35858)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35858)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35889)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35889)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35864)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35864)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35885)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35885)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35881)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35881)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35878)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35878)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35860)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35860)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35856)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35856)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35864)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35864)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35885)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35885)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35878)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35878)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35886)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35886)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35827)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35827)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35881)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35881)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35840)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35840)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35856)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35856)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35868)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35868)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35871)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35871)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35833)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35833)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35866)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35866)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35893)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35893)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35827)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35827)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35803)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35803)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35829)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35829)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35886)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35886)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35821)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35821)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35840)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35840)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35868)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35868)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35871)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35871)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35866)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35866)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35833)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35833)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35893)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35893)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35829)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35829)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35803)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35803)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35821)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35821)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35858)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35858)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35858)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35858)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35827)[0m time to fit was 77.91579270362854
[2m[36m(pid=35827)[0m GPU available: False, used: False
[2m[36m(pid=35827)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35827)[0m 
[2m[36m(pid=35827)[0m   | Name      | Type              | Params
[2m[36m(pid=35827)[0m ------------------------------------------------
[2m[36m(pid=35827)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35827)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35827)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35827)[0m ------------------------------------------------
[2m[36m(pid=35827)[0m 8.7 K     Trainable params
[2m[36m(pid=35827)[0m 0         Non-trainable params
[2m[36m(pid=35827)[0m 8.7 K     Total params
[2m[36m(pid=35833)[0m time to fit was 95.93043160438538
[2m[36m(pid=35833)[0m GPU available: False, used: False
[2m[36m(pid=35833)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35833)[0m 
[2m[36m(pid=35833)[0m   | Name      | Type              | Params
[2m[36m(pid=35833)[0m ------------------------------------------------
[2m[36m(pid=35833)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35833)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35833)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35833)[0m ------------------------------------------------
[2m[36m(pid=35833)[0m 8.7 K     Trainable params
[2m[36m(pid=35833)[0m 0         Non-trainable params
[2m[36m(pid=35833)[0m 8.7 K     Total params
[2m[36m(pid=35851)[0m time to fit was 111.63219881057739
[2m[36m(pid=35851)[0m GPU available: False, used: False
[2m[36m(pid=35851)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35851)[0m 
[2m[36m(pid=35851)[0m   | Name      | Type              | Params
[2m[36m(pid=35851)[0m ------------------------------------------------
[2m[36m(pid=35851)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35851)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35851)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35851)[0m ------------------------------------------------
[2m[36m(pid=35851)[0m 8.7 K     Trainable params
[2m[36m(pid=35851)[0m 0         Non-trainable params
[2m[36m(pid=35851)[0m 8.7 K     Total params
[2m[36m(pid=35864)[0m time to fit was 126.27166652679443
[2m[36m(pid=35864)[0m GPU available: False, used: False
[2m[36m(pid=35864)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35864)[0m 
[2m[36m(pid=35864)[0m   | Name      | Type              | Params
[2m[36m(pid=35864)[0m ------------------------------------------------
[2m[36m(pid=35864)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35864)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35864)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35864)[0m ------------------------------------------------
[2m[36m(pid=35864)[0m 8.7 K     Trainable params
[2m[36m(pid=35864)[0m 0         Non-trainable params
[2m[36m(pid=35864)[0m 8.7 K     Total params
[2m[36m(pid=35840)[0m time to fit was 131.3101134300232
[2m[36m(pid=35840)[0m GPU available: False, used: False
[2m[36m(pid=35840)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35840)[0m 
[2m[36m(pid=35840)[0m   | Name      | Type              | Params
[2m[36m(pid=35840)[0m ------------------------------------------------
[2m[36m(pid=35840)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35840)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35840)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35840)[0m ------------------------------------------------
[2m[36m(pid=35840)[0m 8.7 K     Trainable params
[2m[36m(pid=35840)[0m 0         Non-trainable params
[2m[36m(pid=35840)[0m 8.7 K     Total params
[2m[36m(pid=35829)[0m time to fit was 198.74440479278564
[2m[36m(pid=35829)[0m GPU available: False, used: False
[2m[36m(pid=35829)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35829)[0m 
[2m[36m(pid=35829)[0m   | Name      | Type              | Params
[2m[36m(pid=35829)[0m ------------------------------------------------
[2m[36m(pid=35829)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35829)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35829)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35829)[0m ------------------------------------------------
[2m[36m(pid=35829)[0m 8.7 K     Trainable params
[2m[36m(pid=35829)[0m 0         Non-trainable params
[2m[36m(pid=35829)[0m 8.7 K     Total params
[2m[36m(pid=35858)[0m time to fit was 211.88446974754333
[2m[36m(pid=35858)[0m GPU available: False, used: False
[2m[36m(pid=35858)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35858)[0m 
[2m[36m(pid=35858)[0m   | Name      | Type              | Params
[2m[36m(pid=35858)[0m ------------------------------------------------
[2m[36m(pid=35858)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35858)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35858)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35858)[0m ------------------------------------------------
[2m[36m(pid=35858)[0m 8.7 K     Trainable params
[2m[36m(pid=35858)[0m 0         Non-trainable params
[2m[36m(pid=35858)[0m 8.7 K     Total params
[2m[36m(pid=35827)[0m time to fit was 142.88891553878784
[2m[36m(pid=35827)[0m GPU available: False, used: False
[2m[36m(pid=35827)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35827)[0m 
[2m[36m(pid=35827)[0m   | Name      | Type              | Params
[2m[36m(pid=35827)[0m ------------------------------------------------
[2m[36m(pid=35827)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35827)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35827)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35827)[0m ------------------------------------------------
[2m[36m(pid=35827)[0m 8.7 K     Trainable params
[2m[36m(pid=35827)[0m 0         Non-trainable params
[2m[36m(pid=35827)[0m 8.7 K     Total params
[2m[36m(pid=35868)[0m time to fit was 225.15873003005981
[2m[36m(pid=35868)[0m GPU available: False, used: False
[2m[36m(pid=35868)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35868)[0m 
[2m[36m(pid=35868)[0m   | Name      | Type              | Params
[2m[36m(pid=35868)[0m ------------------------------------------------
[2m[36m(pid=35868)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35868)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35868)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35868)[0m ------------------------------------------------
[2m[36m(pid=35868)[0m 8.7 K     Trainable params
[2m[36m(pid=35868)[0m 0         Non-trainable params
[2m[36m(pid=35868)[0m 8.7 K     Total params
[2m[36m(pid=35860)[0m time to fit was 229.24049925804138
[2m[36m(pid=35860)[0m GPU available: False, used: False
[2m[36m(pid=35860)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35860)[0m 
[2m[36m(pid=35860)[0m   | Name      | Type              | Params
[2m[36m(pid=35860)[0m ------------------------------------------------
[2m[36m(pid=35860)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35860)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35860)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35860)[0m ------------------------------------------------
[2m[36m(pid=35860)[0m 8.7 K     Trainable params
[2m[36m(pid=35860)[0m 0         Non-trainable params
[2m[36m(pid=35860)[0m 8.7 K     Total params
[2m[36m(pid=35864)[0m time to fit was 110.6985993385315
[2m[36m(pid=35864)[0m GPU available: False, used: False
[2m[36m(pid=35864)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35864)[0m 
[2m[36m(pid=35864)[0m   | Name      | Type              | Params
[2m[36m(pid=35864)[0m ------------------------------------------------
[2m[36m(pid=35864)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35864)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35864)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35864)[0m ------------------------------------------------
[2m[36m(pid=35864)[0m 8.7 K     Trainable params
[2m[36m(pid=35864)[0m 0         Non-trainable params
[2m[36m(pid=35864)[0m 8.7 K     Total params
[2m[36m(pid=35833)[0m time to fit was 142.98647785186768
[2m[36m(pid=35833)[0m GPU available: False, used: False
[2m[36m(pid=35833)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35833)[0m 
[2m[36m(pid=35833)[0m   | Name      | Type              | Params
[2m[36m(pid=35833)[0m ------------------------------------------------
[2m[36m(pid=35833)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35833)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35833)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35833)[0m ------------------------------------------------
[2m[36m(pid=35833)[0m 8.7 K     Trainable params
[2m[36m(pid=35833)[0m 0         Non-trainable params
[2m[36m(pid=35833)[0m 8.7 K     Total params
[2m[36m(pid=35873)[0m time to fit was 244.61597728729248
[2m[36m(pid=35873)[0m GPU available: False, used: False
[2m[36m(pid=35873)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35873)[0m 
[2m[36m(pid=35873)[0m   | Name      | Type              | Params
[2m[36m(pid=35873)[0m ------------------------------------------------
[2m[36m(pid=35873)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35873)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35873)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35873)[0m ------------------------------------------------
[2m[36m(pid=35873)[0m 8.7 K     Trainable params
[2m[36m(pid=35873)[0m 0         Non-trainable params
[2m[36m(pid=35873)[0m 8.7 K     Total params
[2m[36m(pid=35886)[0m time to fit was 247.55915880203247
[2m[36m(pid=35886)[0m GPU available: False, used: False
[2m[36m(pid=35886)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35886)[0m 
[2m[36m(pid=35886)[0m   | Name      | Type              | Params
[2m[36m(pid=35886)[0m ------------------------------------------------
[2m[36m(pid=35886)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35886)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35886)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35886)[0m ------------------------------------------------
[2m[36m(pid=35886)[0m 8.7 K     Trainable params
[2m[36m(pid=35886)[0m 0         Non-trainable params
[2m[36m(pid=35886)[0m 8.7 K     Total params
[2m[36m(pid=35866)[0m time to fit was 254.4250192642212
[2m[36m(pid=35866)[0m GPU available: False, used: False
[2m[36m(pid=35866)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35866)[0m 
[2m[36m(pid=35866)[0m   | Name      | Type              | Params
[2m[36m(pid=35866)[0m ------------------------------------------------
[2m[36m(pid=35866)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35866)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35866)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35866)[0m ------------------------------------------------
[2m[36m(pid=35866)[0m 8.7 K     Trainable params
[2m[36m(pid=35866)[0m 0         Non-trainable params
[2m[36m(pid=35866)[0m 8.7 K     Total params
[2m[36m(pid=35840)[0m time to fit was 136.6434781551361
[2m[36m(pid=35840)[0m GPU available: False, used: False
[2m[36m(pid=35840)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35840)[0m 
[2m[36m(pid=35840)[0m   | Name      | Type              | Params
[2m[36m(pid=35840)[0m ------------------------------------------------
[2m[36m(pid=35840)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35840)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35840)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35840)[0m ------------------------------------------------
[2m[36m(pid=35840)[0m 8.7 K     Trainable params
[2m[36m(pid=35840)[0m 0         Non-trainable params
[2m[36m(pid=35840)[0m 8.7 K     Total params
[2m[36m(pid=35851)[0m time to fit was 173.27423930168152
[2m[36m(pid=35851)[0m GPU available: False, used: False
[2m[36m(pid=35851)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35851)[0m 
[2m[36m(pid=35851)[0m   | Name      | Type              | Params
[2m[36m(pid=35851)[0m ------------------------------------------------
[2m[36m(pid=35851)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35851)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35851)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35851)[0m ------------------------------------------------
[2m[36m(pid=35851)[0m 8.7 K     Trainable params
[2m[36m(pid=35851)[0m 0         Non-trainable params
[2m[36m(pid=35851)[0m 8.7 K     Total params
[2m[36m(pid=35885)[0m time to fit was 297.506454706192
[2m[36m(pid=35885)[0m GPU available: False, used: False
[2m[36m(pid=35885)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35885)[0m 
[2m[36m(pid=35885)[0m   | Name      | Type              | Params
[2m[36m(pid=35885)[0m ------------------------------------------------
[2m[36m(pid=35885)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35885)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35885)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35885)[0m ------------------------------------------------
[2m[36m(pid=35885)[0m 8.7 K     Trainable params
[2m[36m(pid=35885)[0m 0         Non-trainable params
[2m[36m(pid=35885)[0m 8.7 K     Total params
[2m[36m(pid=35827)[0m time to fit was 104.43799161911011
[2m[36m(pid=35827)[0m GPU available: False, used: False
[2m[36m(pid=35827)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35827)[0m 
[2m[36m(pid=35827)[0m   | Name      | Type              | Params
[2m[36m(pid=35827)[0m ------------------------------------------------
[2m[36m(pid=35827)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35827)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35827)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35827)[0m ------------------------------------------------
[2m[36m(pid=35827)[0m 8.7 K     Trainable params
[2m[36m(pid=35827)[0m 0         Non-trainable params
[2m[36m(pid=35827)[0m 8.7 K     Total params
[2m[36m(pid=35893)[0m time to fit was 338.07582449913025
[2m[36m(pid=35893)[0m GPU available: False, used: False
[2m[36m(pid=35893)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35893)[0m 
[2m[36m(pid=35893)[0m   | Name      | Type              | Params
[2m[36m(pid=35893)[0m ------------------------------------------------
[2m[36m(pid=35893)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35893)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35893)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35893)[0m ------------------------------------------------
[2m[36m(pid=35893)[0m 8.7 K     Trainable params
[2m[36m(pid=35893)[0m 0         Non-trainable params
[2m[36m(pid=35893)[0m 8.7 K     Total params
[2m[36m(pid=35856)[0m time to fit was 354.3864243030548
[2m[36m(pid=35856)[0m GPU available: False, used: False
[2m[36m(pid=35856)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35856)[0m 
[2m[36m(pid=35856)[0m   | Name      | Type              | Params
[2m[36m(pid=35856)[0m ------------------------------------------------
[2m[36m(pid=35856)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35856)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35856)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35856)[0m ------------------------------------------------
[2m[36m(pid=35856)[0m 8.7 K     Trainable params
[2m[36m(pid=35856)[0m 0         Non-trainable params
[2m[36m(pid=35856)[0m 8.7 K     Total params
[2m[36m(pid=35881)[0m time to fit was 357.4953844547272
[2m[36m(pid=35881)[0m GPU available: False, used: False
[2m[36m(pid=35881)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35881)[0m 
[2m[36m(pid=35881)[0m   | Name      | Type              | Params
[2m[36m(pid=35881)[0m ------------------------------------------------
[2m[36m(pid=35881)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35881)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35881)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35881)[0m ------------------------------------------------
[2m[36m(pid=35881)[0m 8.7 K     Trainable params
[2m[36m(pid=35881)[0m 0         Non-trainable params
[2m[36m(pid=35881)[0m 8.7 K     Total params
[2m[36m(pid=35851)[0m time to fit was 104.60115814208984
[2m[36m(pid=35851)[0m GPU available: False, used: False
[2m[36m(pid=35851)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35851)[0m 
[2m[36m(pid=35851)[0m   | Name      | Type              | Params
[2m[36m(pid=35851)[0m ------------------------------------------------
[2m[36m(pid=35851)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35851)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35851)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35851)[0m ------------------------------------------------
[2m[36m(pid=35851)[0m 8.7 K     Trainable params
[2m[36m(pid=35851)[0m 0         Non-trainable params
[2m[36m(pid=35851)[0m 8.7 K     Total params
[2m[36m(pid=35829)[0m time to fit was 201.44906997680664
[2m[36m(pid=35829)[0m GPU available: False, used: False
[2m[36m(pid=35829)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35829)[0m 
[2m[36m(pid=35829)[0m   | Name      | Type              | Params
[2m[36m(pid=35829)[0m ------------------------------------------------
[2m[36m(pid=35829)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35829)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35829)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35829)[0m ------------------------------------------------
[2m[36m(pid=35829)[0m 8.7 K     Trainable params
[2m[36m(pid=35829)[0m 0         Non-trainable params
[2m[36m(pid=35829)[0m 8.7 K     Total params
[2m[36m(pid=35827)[0m time to fit was 84.83051943778992
[2m[36m(pid=35827)[0m GPU available: False, used: False
[2m[36m(pid=35827)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35827)[0m 
[2m[36m(pid=35827)[0m   | Name      | Type              | Params
[2m[36m(pid=35827)[0m ------------------------------------------------
[2m[36m(pid=35827)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35827)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35827)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35827)[0m ------------------------------------------------
[2m[36m(pid=35827)[0m 8.7 K     Trainable params
[2m[36m(pid=35827)[0m 0         Non-trainable params
[2m[36m(pid=35827)[0m 8.7 K     Total params
[2m[36m(pid=35864)[0m time to fit was 174.73654985427856
[2m[36m(pid=35864)[0m GPU available: False, used: False
[2m[36m(pid=35864)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35864)[0m 
[2m[36m(pid=35864)[0m   | Name      | Type              | Params
[2m[36m(pid=35864)[0m ------------------------------------------------
[2m[36m(pid=35864)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35864)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35864)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35864)[0m ------------------------------------------------
[2m[36m(pid=35864)[0m 8.7 K     Trainable params
[2m[36m(pid=35864)[0m 0         Non-trainable params
[2m[36m(pid=35864)[0m 8.7 K     Total params
[2m[36m(pid=35840)[0m time to fit was 151.74131751060486
[2m[36m(pid=35840)[0m GPU available: False, used: False
[2m[36m(pid=35840)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35840)[0m 
[2m[36m(pid=35840)[0m   | Name      | Type              | Params
[2m[36m(pid=35840)[0m ------------------------------------------------
[2m[36m(pid=35840)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35840)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35840)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35840)[0m ------------------------------------------------
[2m[36m(pid=35840)[0m 8.7 K     Trainable params
[2m[36m(pid=35840)[0m 0         Non-trainable params
[2m[36m(pid=35840)[0m 8.7 K     Total params
[2m[36m(pid=35860)[0m time to fit was 194.2823498249054
[2m[36m(pid=35860)[0m GPU available: False, used: False
[2m[36m(pid=35860)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35860)[0m 
[2m[36m(pid=35860)[0m   | Name      | Type              | Params
[2m[36m(pid=35860)[0m ------------------------------------------------
[2m[36m(pid=35860)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35860)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35860)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35860)[0m ------------------------------------------------
[2m[36m(pid=35860)[0m 8.7 K     Trainable params
[2m[36m(pid=35860)[0m 0         Non-trainable params
[2m[36m(pid=35860)[0m 8.7 K     Total params
[2m[36m(pid=35833)[0m time to fit was 185.1807062625885
[2m[36m(pid=35833)[0m GPU available: False, used: False
[2m[36m(pid=35833)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35833)[0m 
[2m[36m(pid=35833)[0m   | Name      | Type              | Params
[2m[36m(pid=35833)[0m ------------------------------------------------
[2m[36m(pid=35833)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35833)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35833)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35833)[0m ------------------------------------------------
[2m[36m(pid=35833)[0m 8.7 K     Trainable params
[2m[36m(pid=35833)[0m 0         Non-trainable params
[2m[36m(pid=35833)[0m 8.7 K     Total params
[2m[36m(pid=35871)[0m time to fit was 426.2702786922455
[2m[36m(pid=35871)[0m GPU available: False, used: False
[2m[36m(pid=35871)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35871)[0m 
[2m[36m(pid=35871)[0m   | Name      | Type              | Params
[2m[36m(pid=35871)[0m ------------------------------------------------
[2m[36m(pid=35871)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35871)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35871)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35871)[0m ------------------------------------------------
[2m[36m(pid=35871)[0m 8.7 K     Trainable params
[2m[36m(pid=35871)[0m 0         Non-trainable params
[2m[36m(pid=35871)[0m 8.7 K     Total params
[2m[36m(pid=35821)[0m time to fit was 480.3505301475525
[2m[36m(pid=35821)[0m GPU available: False, used: False
[2m[36m(pid=35821)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35821)[0m 
[2m[36m(pid=35821)[0m   | Name      | Type              | Params
[2m[36m(pid=35821)[0m ------------------------------------------------
[2m[36m(pid=35821)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35821)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35821)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35821)[0m ------------------------------------------------
[2m[36m(pid=35821)[0m 8.7 K     Trainable params
[2m[36m(pid=35821)[0m 0         Non-trainable params
[2m[36m(pid=35821)[0m 8.7 K     Total params
[2m[36m(pid=35873)[0m time to fit was 245.5618076324463
[2m[36m(pid=35873)[0m GPU available: False, used: False
[2m[36m(pid=35873)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35873)[0m 
[2m[36m(pid=35873)[0m   | Name      | Type              | Params
[2m[36m(pid=35873)[0m ------------------------------------------------
[2m[36m(pid=35873)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35873)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35873)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35873)[0m ------------------------------------------------
[2m[36m(pid=35873)[0m 8.7 K     Trainable params
[2m[36m(pid=35873)[0m 0         Non-trainable params
[2m[36m(pid=35873)[0m 8.7 K     Total params
[2m[36m(pid=35827)[0m time to fit was 83.79936289787292
Result for _inner_e8deb_00019:
  auc: 0.9138412833213806
  date: 2021-03-19_11-30-08
  done: false
  experiment_id: 2fa6c72c92494ee582875059dc001a95
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35827
  time_since_restore: 495.22823572158813
  time_this_iter_s: 495.22823572158813
  time_total_s: 495.22823572158813
  timestamp: 1616149808
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00019
  
[2m[36m(pid=35827)[0m Finished run with seed 0 - lr 1 - sec_lr 0.001 - bs 512 - mean val auc: 0.9138412833213806
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 27/180 (1 PENDING, 26 RUNNING)
+--------------------+----------+-------+--------------+-------+-------+----------+
| Trial name         | status   | loc   |   batch_size |   eta |    lr |   sec_lr |
|--------------------+----------+-------+--------------+-------+-------+----------|
| _inner_e8deb_00000 | RUNNING  |       |           32 |     0 | 0.001 |    0.001 |
| _inner_e8deb_00001 | RUNNING  |       |           64 |     0 | 0.001 |    0.001 |
| _inner_e8deb_00002 | RUNNING  |       |          128 |     0 | 0.001 |    0.001 |
| _inner_e8deb_00003 | RUNNING  |       |          256 |     0 | 0.001 |    0.001 |
| _inner_e8deb_00004 | RUNNING  |       |          512 |     0 | 0.001 |    0.001 |
| _inner_e8deb_00005 | RUNNING  |       |           32 |     0 | 0.01  |    0.001 |
| _inner_e8deb_00006 | RUNNING  |       |           64 |     0 | 0.01  |    0.001 |
| _inner_e8deb_00007 | RUNNING  |       |          128 |     0 | 0.01  |    0.001 |
| _inner_e8deb_00008 | RUNNING  |       |          256 |     0 | 0.01  |    0.001 |
| _inner_e8deb_00009 | RUNNING  |       |          512 |     0 | 0.01  |    0.001 |
| _inner_e8deb_00010 | RUNNING  |       |           32 |     0 | 0.1   |    0.001 |
| _inner_e8deb_00011 | RUNNING  |       |           64 |     0 | 0.1   |    0.001 |
| _inner_e8deb_00012 | RUNNING  |       |          128 |     0 | 0.1   |    0.001 |
| _inner_e8deb_00013 | RUNNING  |       |          256 |     0 | 0.1   |    0.001 |
| _inner_e8deb_00014 | RUNNING  |       |          512 |     0 | 0.1   |    0.001 |
| _inner_e8deb_00015 | RUNNING  |       |           32 |     0 | 1     |    0.001 |
| _inner_e8deb_00016 | RUNNING  |       |           64 |     0 | 1     |    0.001 |
| _inner_e8deb_00017 | RUNNING  |       |          128 |     0 | 1     |    0.001 |
| _inner_e8deb_00018 | RUNNING  |       |          256 |     0 | 1     |    0.001 |
| _inner_e8deb_00026 | PENDING  |       |           64 |     0 | 5     |    0.001 |
+--------------------+----------+-------+--------------+-------+-------+----------+
... 7 more trials not shown (7 RUNNING)


Result for _inner_e8deb_00019:
  auc: 0.9138412833213806
  date: 2021-03-19_11-30-08
  done: true
  experiment_id: 2fa6c72c92494ee582875059dc001a95
  experiment_tag: 19_batch_size=512,eta=0.0,lr=1,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35827
  time_since_restore: 495.22823572158813
  time_this_iter_s: 495.22823572158813
  time_total_s: 495.22823572158813
  timestamp: 1616149808
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00019
  
[2m[36m(pid=35886)[0m time to fit was 247.5713472366333
[2m[36m(pid=35886)[0m GPU available: False, used: False
[2m[36m(pid=35886)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35886)[0m 
[2m[36m(pid=35886)[0m   | Name      | Type              | Params
[2m[36m(pid=35886)[0m ------------------------------------------------
[2m[36m(pid=35886)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35886)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35886)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35886)[0m ------------------------------------------------
[2m[36m(pid=35886)[0m 8.7 K     Trainable params
[2m[36m(pid=35886)[0m 0         Non-trainable params
[2m[36m(pid=35886)[0m 8.7 K     Total params
[2m[36m(pid=35826)[0m Starting run with seed 0 - lr 5 - sec_lr 0.001 - bs 64
[2m[36m(pid=35826)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35826)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35826)[0m GPU available: False, used: False
[2m[36m(pid=35826)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35826)[0m 
[2m[36m(pid=35826)[0m   | Name      | Type              | Params
[2m[36m(pid=35826)[0m ------------------------------------------------
[2m[36m(pid=35826)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35826)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35826)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35826)[0m ------------------------------------------------
[2m[36m(pid=35826)[0m 8.7 K     Trainable params
[2m[36m(pid=35826)[0m 0         Non-trainable params
[2m[36m(pid=35826)[0m 8.7 K     Total params
[2m[36m(pid=35826)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35826)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35826)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35826)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35826)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35826)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35868)[0m time to fit was 284.61983132362366
[2m[36m(pid=35868)[0m GPU available: False, used: False
[2m[36m(pid=35868)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35868)[0m 
[2m[36m(pid=35868)[0m   | Name      | Type              | Params
[2m[36m(pid=35868)[0m ------------------------------------------------
[2m[36m(pid=35868)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35868)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35868)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35868)[0m ------------------------------------------------
[2m[36m(pid=35868)[0m 8.7 K     Trainable params
[2m[36m(pid=35868)[0m 0         Non-trainable params
[2m[36m(pid=35868)[0m 8.7 K     Total params
[2m[36m(pid=35840)[0m time to fit was 130.79612946510315
[2m[36m(pid=35840)[0m GPU available: False, used: False
[2m[36m(pid=35840)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35840)[0m 
[2m[36m(pid=35840)[0m   | Name      | Type              | Params
[2m[36m(pid=35840)[0m ------------------------------------------------
[2m[36m(pid=35840)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35840)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35840)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35840)[0m ------------------------------------------------
[2m[36m(pid=35840)[0m 8.7 K     Trainable params
[2m[36m(pid=35840)[0m 0         Non-trainable params
[2m[36m(pid=35840)[0m 8.7 K     Total params
[2m[36m(pid=35851)[0m time to fit was 162.60738611221313
[2m[36m(pid=35851)[0m GPU available: False, used: False
[2m[36m(pid=35851)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35851)[0m 
[2m[36m(pid=35851)[0m   | Name      | Type              | Params
[2m[36m(pid=35851)[0m ------------------------------------------------
[2m[36m(pid=35851)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35851)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35851)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35851)[0m ------------------------------------------------
[2m[36m(pid=35851)[0m 8.7 K     Trainable params
[2m[36m(pid=35851)[0m 0         Non-trainable params
[2m[36m(pid=35851)[0m 8.7 K     Total params
[2m[36m(pid=35889)[0m time to fit was 557.6523869037628
[2m[36m(pid=35864)[0m time to fit was 144.87774777412415
[2m[36m(pid=35889)[0m GPU available: False, used: False
[2m[36m(pid=35889)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35864)[0m GPU available: False, used: False
[2m[36m(pid=35864)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35864)[0m 
[2m[36m(pid=35864)[0m   | Name      | Type              | Params
[2m[36m(pid=35864)[0m ------------------------------------------------
[2m[36m(pid=35864)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35864)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35864)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35864)[0m ------------------------------------------------
[2m[36m(pid=35864)[0m 8.7 K     Trainable params
[2m[36m(pid=35864)[0m 0         Non-trainable params
[2m[36m(pid=35864)[0m 8.7 K     Total params
[2m[36m(pid=35889)[0m 
[2m[36m(pid=35889)[0m   | Name      | Type              | Params
[2m[36m(pid=35889)[0m ------------------------------------------------
[2m[36m(pid=35889)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35889)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35889)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35889)[0m ------------------------------------------------
[2m[36m(pid=35889)[0m 8.7 K     Trainable params
[2m[36m(pid=35889)[0m 0         Non-trainable params
[2m[36m(pid=35889)[0m 8.7 K     Total params
[2m[36m(pid=35860)[0m time to fit was 150.721741437912
[2m[36m(pid=35860)[0m GPU available: False, used: False
[2m[36m(pid=35860)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35860)[0m 
[2m[36m(pid=35860)[0m   | Name      | Type              | Params
[2m[36m(pid=35860)[0m ------------------------------------------------
[2m[36m(pid=35860)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35860)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35860)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35860)[0m ------------------------------------------------
[2m[36m(pid=35860)[0m 8.7 K     Trainable params
[2m[36m(pid=35860)[0m 0         Non-trainable params
[2m[36m(pid=35860)[0m 8.7 K     Total params
[2m[36m(pid=35833)[0m time to fit was 156.37464690208435
[2m[36m(pid=35833)[0m GPU available: False, used: False
[2m[36m(pid=35833)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35833)[0m 
[2m[36m(pid=35833)[0m   | Name      | Type              | Params
[2m[36m(pid=35833)[0m ------------------------------------------------
[2m[36m(pid=35833)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35833)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35833)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35833)[0m ------------------------------------------------
[2m[36m(pid=35833)[0m 8.7 K     Trainable params
[2m[36m(pid=35833)[0m 0         Non-trainable params
[2m[36m(pid=35833)[0m 8.7 K     Total params
[2m[36m(pid=35858)[0m time to fit was 372.3762626647949
[2m[36m(pid=35858)[0m GPU available: False, used: False
[2m[36m(pid=35858)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35858)[0m 
[2m[36m(pid=35858)[0m   | Name      | Type              | Params
[2m[36m(pid=35858)[0m ------------------------------------------------
[2m[36m(pid=35858)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35858)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35858)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35858)[0m ------------------------------------------------
[2m[36m(pid=35858)[0m 8.7 K     Trainable params
[2m[36m(pid=35858)[0m 0         Non-trainable params
[2m[36m(pid=35858)[0m 8.7 K     Total params
[2m[36m(pid=35855)[0m time to fit was 593.2323434352875
[2m[36m(pid=35855)[0m GPU available: False, used: False
[2m[36m(pid=35855)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35855)[0m 
[2m[36m(pid=35855)[0m   | Name      | Type              | Params
[2m[36m(pid=35855)[0m ------------------------------------------------
[2m[36m(pid=35855)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35855)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35855)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35855)[0m ------------------------------------------------
[2m[36m(pid=35855)[0m 8.7 K     Trainable params
[2m[36m(pid=35855)[0m 0         Non-trainable params
[2m[36m(pid=35855)[0m 8.7 K     Total params
[2m[36m(pid=35852)[0m time to fit was 645.3537383079529
[2m[36m(pid=35852)[0m GPU available: False, used: False
[2m[36m(pid=35852)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35852)[0m 
[2m[36m(pid=35852)[0m   | Name      | Type              | Params
[2m[36m(pid=35852)[0m ------------------------------------------------
[2m[36m(pid=35852)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35852)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35852)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35852)[0m ------------------------------------------------
[2m[36m(pid=35852)[0m 8.7 K     Trainable params
[2m[36m(pid=35852)[0m 0         Non-trainable params
[2m[36m(pid=35852)[0m 8.7 K     Total params
[2m[36m(pid=35851)[0m time to fit was 114.70806431770325
Result for _inner_e8deb_00014:
  auc: 0.9128791332244873
  date: 2021-03-19_11-33-00
  done: false
  experiment_id: 59012e45d8754e8298d60d6dd8221794
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35851
  time_since_restore: 668.1411380767822
  time_this_iter_s: 668.1411380767822
  time_total_s: 668.1411380767822
  timestamp: 1616149980
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00014
  
[2m[36m(pid=35851)[0m Finished run with seed 0 - lr 0.1 - sec_lr 0.001 - bs 512 - mean val auc: 0.9128791332244873
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 28/180 (1 PENDING, 26 RUNNING, 1 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |                     |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00003 | RUNNING    |                     |          256 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00004 | RUNNING    |                     |          512 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |                     |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |                     |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00008 | RUNNING    |                     |          256 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00009 | RUNNING    |                     |          512 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |                     |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00011 | RUNNING    |                     |           64 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00012 | RUNNING    |                     |          128 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00013 | RUNNING    |                     |          256 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00014 | RUNNING    | 145.101.32.82:35851 |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00015 | RUNNING    |                     |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00016 | RUNNING    |                     |           64 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00017 | RUNNING    |                     |          128 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00027 | PENDING    |                     |          128 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00019 | TERMINATED |                     |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 8 more trials not shown (8 RUNNING)


Result for _inner_e8deb_00014:
  auc: 0.9128791332244873
  date: 2021-03-19_11-33-00
  done: true
  experiment_id: 59012e45d8754e8298d60d6dd8221794
  experiment_tag: 14_batch_size=512,eta=0.0,lr=0.1,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35851
  time_since_restore: 668.1411380767822
  time_this_iter_s: 668.1411380767822
  time_total_s: 668.1411380767822
  timestamp: 1616149980
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00014
  
[2m[36m(pid=35829)[0m time to fit was 272.76015043258667
[2m[36m(pid=35829)[0m GPU available: False, used: False
[2m[36m(pid=35829)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35829)[0m 
[2m[36m(pid=35829)[0m   | Name      | Type              | Params
[2m[36m(pid=35829)[0m ------------------------------------------------
[2m[36m(pid=35829)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35829)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35829)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35829)[0m ------------------------------------------------
[2m[36m(pid=35829)[0m 8.7 K     Trainable params
[2m[36m(pid=35829)[0m 0         Non-trainable params
[2m[36m(pid=35829)[0m 8.7 K     Total params
[2m[36m(pid=35845)[0m Starting run with seed 0 - lr 5 - sec_lr 0.001 - bs 128
[2m[36m(pid=35845)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35845)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35845)[0m GPU available: False, used: False
[2m[36m(pid=35845)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35845)[0m 
[2m[36m(pid=35845)[0m   | Name      | Type              | Params
[2m[36m(pid=35845)[0m ------------------------------------------------
[2m[36m(pid=35845)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35845)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35845)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35845)[0m ------------------------------------------------
[2m[36m(pid=35845)[0m 8.7 K     Trainable params
[2m[36m(pid=35845)[0m 0         Non-trainable params
[2m[36m(pid=35845)[0m 8.7 K     Total params
[2m[36m(pid=35845)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35845)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35845)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35845)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35845)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35845)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35856)[0m time to fit was 354.2479929924011
[2m[36m(pid=35856)[0m GPU available: False, used: False
[2m[36m(pid=35856)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35856)[0m 
[2m[36m(pid=35856)[0m   | Name      | Type              | Params
[2m[36m(pid=35856)[0m ------------------------------------------------
[2m[36m(pid=35856)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35856)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35856)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35856)[0m ------------------------------------------------
[2m[36m(pid=35856)[0m 8.7 K     Trainable params
[2m[36m(pid=35856)[0m 0         Non-trainable params
[2m[36m(pid=35856)[0m 8.7 K     Total params
[2m[36m(pid=35881)[0m time to fit was 356.28417587280273
[2m[36m(pid=35881)[0m GPU available: False, used: False
[2m[36m(pid=35881)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35881)[0m 
[2m[36m(pid=35881)[0m   | Name      | Type              | Params
[2m[36m(pid=35881)[0m ------------------------------------------------
[2m[36m(pid=35881)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35881)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35881)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35881)[0m ------------------------------------------------
[2m[36m(pid=35881)[0m 8.7 K     Trainable params
[2m[36m(pid=35881)[0m 0         Non-trainable params
[2m[36m(pid=35881)[0m 8.7 K     Total params
[2m[36m(pid=35833)[0m time to fit was 143.50620126724243
Result for _inner_e8deb_00018:
  auc: 0.9137318730354309
  date: 2021-03-19_11-33-58
  done: false
  experiment_id: 46d4fbbd89b240708d6de577dc140140
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35833
  time_since_restore: 725.2947599887848
  time_this_iter_s: 725.2947599887848
  time_total_s: 725.2947599887848
  timestamp: 1616150038
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00018
  
[2m[36m(pid=35833)[0m Finished run with seed 0 - lr 1 - sec_lr 0.001 - bs 256 - mean val auc: 0.9137318730354309
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 29/180 (1 PENDING, 26 RUNNING, 2 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |       |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00003 | RUNNING    |       |          256 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00004 | RUNNING    |       |          512 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |       |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00008 | RUNNING    |       |          256 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00009 | RUNNING    |       |          512 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00011 | RUNNING    |       |           64 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00012 | RUNNING    |       |          128 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00013 | RUNNING    |       |          256 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    |       |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00016 | RUNNING    |       |           64 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00017 | RUNNING    |       |          128 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00028 | PENDING    |       |          256 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00019 | TERMINATED |       |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 9 more trials not shown (9 RUNNING)


Result for _inner_e8deb_00018:
  auc: 0.9137318730354309
  date: 2021-03-19_11-33-58
  done: true
  experiment_id: 46d4fbbd89b240708d6de577dc140140
  experiment_tag: 18_batch_size=256,eta=0.0,lr=1,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35833
  time_since_restore: 725.2947599887848
  time_this_iter_s: 725.2947599887848
  time_total_s: 725.2947599887848
  timestamp: 1616150038
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00018
  
[2m[36m(pid=35826)[0m time to fit was 227.02288222312927
[2m[36m(pid=35826)[0m GPU available: False, used: False
[2m[36m(pid=35826)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35873)[0m time to fit was 243.31624603271484
[2m[36m(pid=35826)[0m 
[2m[36m(pid=35826)[0m   | Name      | Type              | Params
[2m[36m(pid=35826)[0m ------------------------------------------------
[2m[36m(pid=35826)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35826)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35826)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35826)[0m ------------------------------------------------
[2m[36m(pid=35826)[0m 8.7 K     Trainable params
[2m[36m(pid=35826)[0m 0         Non-trainable params
[2m[36m(pid=35826)[0m 8.7 K     Total params
[2m[36m(pid=35873)[0m GPU available: False, used: False
[2m[36m(pid=35873)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35873)[0m 
[2m[36m(pid=35873)[0m   | Name      | Type              | Params
[2m[36m(pid=35873)[0m ------------------------------------------------
[2m[36m(pid=35873)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35873)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35873)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35873)[0m ------------------------------------------------
[2m[36m(pid=35873)[0m 8.7 K     Trainable params
[2m[36m(pid=35873)[0m 0         Non-trainable params
[2m[36m(pid=35873)[0m 8.7 K     Total params
[2m[36m(pid=35795)[0m Starting run with seed 0 - lr 5 - sec_lr 0.001 - bs 256
[2m[36m(pid=35795)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35795)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35795)[0m GPU available: False, used: False
[2m[36m(pid=35795)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35795)[0m 
[2m[36m(pid=35795)[0m   | Name      | Type              | Params
[2m[36m(pid=35795)[0m ------------------------------------------------
[2m[36m(pid=35795)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35795)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35795)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35795)[0m ------------------------------------------------
[2m[36m(pid=35795)[0m 8.7 K     Trainable params
[2m[36m(pid=35795)[0m 0         Non-trainable params
[2m[36m(pid=35795)[0m 8.7 K     Total params
[2m[36m(pid=35795)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35795)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35840)[0m time to fit was 184.51303791999817
[2m[36m(pid=35795)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35795)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35840)[0m Finished run with seed 0 - lr 0.1 - sec_lr 0.001 - bs 256 - mean val auc: 0.912858533859253
Result for _inner_e8deb_00013:
  auc: 0.912858533859253
  date: 2021-03-19_11-34-09
  done: false
  experiment_id: 9c9d48e3576e40799f17598a61e364ed
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35840
  time_since_restore: 736.5636427402496
  time_this_iter_s: 736.5636427402496
  time_total_s: 736.5636427402496
  timestamp: 1616150049
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00013
  
[2m[36m(pid=35840)[0m 
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 30/180 (1 PENDING, 26 RUNNING, 3 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |                     |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00003 | RUNNING    |                     |          256 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00004 | RUNNING    |                     |          512 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |                     |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |                     |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00008 | RUNNING    |                     |          256 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00009 | RUNNING    |                     |          512 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |                     |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00011 | RUNNING    |                     |           64 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00012 | RUNNING    |                     |          128 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00013 | RUNNING    | 145.101.32.82:35840 |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00015 | RUNNING    |                     |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00016 | RUNNING    |                     |           64 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00029 | PENDING    |                     |          512 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00014 | TERMINATED |                     |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00018 | TERMINATED |                     |          256 |     0 | 1     |    0.001 |      1 |          725.295 | 0.913732 |
| _inner_e8deb_00019 | TERMINATED |                     |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 10 more trials not shown (10 RUNNING)


Result for _inner_e8deb_00013:
  auc: 0.912858533859253
  date: 2021-03-19_11-34-09
  done: true
  experiment_id: 9c9d48e3576e40799f17598a61e364ed
  experiment_tag: 13_batch_size=256,eta=0.0,lr=0.1,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35840
  time_since_restore: 736.5636427402496
  time_this_iter_s: 736.5636427402496
  time_total_s: 736.5636427402496
  timestamp: 1616150049
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00013
  
[2m[36m(pid=35795)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35795)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35893)[0m time to fit was 398.2728979587555
[2m[36m(pid=35893)[0m GPU available: False, used: False
[2m[36m(pid=35893)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35893)[0m 
[2m[36m(pid=35893)[0m   | Name      | Type              | Params
[2m[36m(pid=35893)[0m ------------------------------------------------
[2m[36m(pid=35893)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35893)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35893)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35893)[0m ------------------------------------------------
[2m[36m(pid=35893)[0m 8.7 K     Trainable params
[2m[36m(pid=35893)[0m 0         Non-trainable params
[2m[36m(pid=35893)[0m 8.7 K     Total params
[2m[36m(pid=35886)[0m time to fit was 242.00536727905273
[2m[36m(pid=35886)[0m GPU available: False, used: False
[2m[36m(pid=35886)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35886)[0m 
[2m[36m(pid=35886)[0m   | Name      | Type              | Params
[2m[36m(pid=35886)[0m ------------------------------------------------
[2m[36m(pid=35886)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35886)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35886)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35886)[0m ------------------------------------------------
[2m[36m(pid=35886)[0m 8.7 K     Trainable params
[2m[36m(pid=35886)[0m 0         Non-trainable params
[2m[36m(pid=35886)[0m 8.7 K     Total params
[2m[36m(pid=35868)[0m time to fit was 230.84257292747498
[2m[36m(pid=35868)[0m GPU available: False, used: False
[2m[36m(pid=35868)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35868)[0m 
[2m[36m(pid=35868)[0m   | Name      | Type              | Params
[2m[36m(pid=35868)[0m ------------------------------------------------
[2m[36m(pid=35868)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35868)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35868)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35868)[0m ------------------------------------------------
[2m[36m(pid=35868)[0m 8.7 K     Trainable params
[2m[36m(pid=35868)[0m 0         Non-trainable params
[2m[36m(pid=35868)[0m 8.7 K     Total params
[2m[36m(pid=35832)[0m Starting run with seed 0 - lr 5 - sec_lr 0.001 - bs 512
[2m[36m(pid=35832)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35832)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35832)[0m GPU available: False, used: False
[2m[36m(pid=35832)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35832)[0m 
[2m[36m(pid=35832)[0m   | Name      | Type              | Params
[2m[36m(pid=35832)[0m ------------------------------------------------
[2m[36m(pid=35832)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35832)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35832)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35832)[0m ------------------------------------------------
[2m[36m(pid=35832)[0m 8.7 K     Trainable params
[2m[36m(pid=35832)[0m 0         Non-trainable params
[2m[36m(pid=35832)[0m 8.7 K     Total params
[2m[36m(pid=35832)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35832)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35832)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35832)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35832)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35832)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35878)[0m time to fit was 752.235276222229
[2m[36m(pid=35878)[0m GPU available: False, used: False
[2m[36m(pid=35878)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35878)[0m 
[2m[36m(pid=35878)[0m   | Name      | Type              | Params
[2m[36m(pid=35878)[0m ------------------------------------------------
[2m[36m(pid=35878)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35878)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35878)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35878)[0m ------------------------------------------------
[2m[36m(pid=35878)[0m 8.7 K     Trainable params
[2m[36m(pid=35878)[0m 0         Non-trainable params
[2m[36m(pid=35878)[0m 8.7 K     Total params
[2m[36m(pid=35866)[0m time to fit was 509.70395278930664
[2m[36m(pid=35866)[0m GPU available: False, used: False
[2m[36m(pid=35866)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35866)[0m 
[2m[36m(pid=35866)[0m   | Name      | Type              | Params
[2m[36m(pid=35866)[0m ------------------------------------------------
[2m[36m(pid=35866)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35866)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35866)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35866)[0m ------------------------------------------------
[2m[36m(pid=35866)[0m 8.7 K     Trainable params
[2m[36m(pid=35866)[0m 0         Non-trainable params
[2m[36m(pid=35866)[0m 8.7 K     Total params
[2m[36m(pid=35885)[0m time to fit was 484.7314944267273
[2m[36m(pid=35885)[0m GPU available: False, used: False
[2m[36m(pid=35885)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35885)[0m 
[2m[36m(pid=35885)[0m   | Name      | Type              | Params
[2m[36m(pid=35885)[0m ------------------------------------------------
[2m[36m(pid=35885)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35885)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35885)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35885)[0m ------------------------------------------------
[2m[36m(pid=35885)[0m 8.7 K     Trainable params
[2m[36m(pid=35885)[0m 0         Non-trainable params
[2m[36m(pid=35885)[0m 8.7 K     Total params
[2m[36m(pid=35864)[0m time to fit was 233.28767824172974
Result for _inner_e8deb_00024:
  auc: 0.9104395866394043
  date: 2021-03-19_11-35-04
  done: false
  experiment_id: 55dfd989784d4f9fbebfbc7f999fab95
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35864
  time_since_restore: 791.1870934963226
  time_this_iter_s: 791.1870934963226
  time_total_s: 791.1870934963226
  timestamp: 1616150104
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00024
  
[2m[36m(pid=35864)[0m Finished run with seed 0 - lr 2 - sec_lr 0.001 - bs 512 - mean val auc: 0.9104395866394043
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 31/180 (1 PENDING, 26 RUNNING, 4 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |       |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00003 | RUNNING    |       |          256 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00004 | RUNNING    |       |          512 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |       |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00008 | RUNNING    |       |          256 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00009 | RUNNING    |       |          512 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00011 | RUNNING    |       |           64 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00012 | RUNNING    |       |          128 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    |       |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00016 | RUNNING    |       |           64 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00030 | PENDING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00018 | TERMINATED |       |          256 |     0 | 1     |    0.001 |      1 |          725.295 | 0.913732 |
| _inner_e8deb_00019 | TERMINATED |       |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 11 more trials not shown (11 RUNNING)


Result for _inner_e8deb_00024:
  auc: 0.9104395866394043
  date: 2021-03-19_11-35-04
  done: true
  experiment_id: 55dfd989784d4f9fbebfbc7f999fab95
  experiment_tag: 24_batch_size=512,eta=0.0,lr=2,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35864
  time_since_restore: 791.1870934963226
  time_this_iter_s: 791.1870934963226
  time_total_s: 791.1870934963226
  timestamp: 1616150104
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00024
  
[2m[36m(pid=35838)[0m Starting run with seed 0 - lr 0.001 - sec_lr 0.01 - bs 32
[2m[36m(pid=35838)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35838)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35838)[0m GPU available: False, used: False
[2m[36m(pid=35838)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35838)[0m 
[2m[36m(pid=35838)[0m   | Name      | Type              | Params
[2m[36m(pid=35838)[0m ------------------------------------------------
[2m[36m(pid=35838)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35838)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35838)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35838)[0m ------------------------------------------------
[2m[36m(pid=35838)[0m 8.7 K     Trainable params
[2m[36m(pid=35838)[0m 0         Non-trainable params
[2m[36m(pid=35838)[0m 8.7 K     Total params
[2m[36m(pid=35838)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35838)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35838)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35838)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35838)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35838)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35845)[0m time to fit was 125.43434977531433
[2m[36m(pid=35845)[0m GPU available: False, used: False
[2m[36m(pid=35845)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35845)[0m 
[2m[36m(pid=35845)[0m   | Name      | Type              | Params
[2m[36m(pid=35845)[0m ------------------------------------------------
[2m[36m(pid=35845)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35845)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35845)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35845)[0m ------------------------------------------------
[2m[36m(pid=35845)[0m 8.7 K     Trainable params
[2m[36m(pid=35845)[0m 0         Non-trainable params
[2m[36m(pid=35845)[0m 8.7 K     Total params
[2m[36m(pid=35860)[0m time to fit was 239.21711564064026
[2m[36m(pid=35860)[0m GPU available: False, used: False
[2m[36m(pid=35860)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35860)[0m 
[2m[36m(pid=35860)[0m   | Name      | Type              | Params
[2m[36m(pid=35860)[0m ------------------------------------------------
[2m[36m(pid=35860)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35860)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35860)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35860)[0m ------------------------------------------------
[2m[36m(pid=35860)[0m 8.7 K     Trainable params
[2m[36m(pid=35860)[0m 0         Non-trainable params
[2m[36m(pid=35860)[0m 8.7 K     Total params
[2m[36m(pid=35858)[0m time to fit was 288.98909091949463
[2m[36m(pid=35858)[0m GPU available: False, used: False
[2m[36m(pid=35858)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35858)[0m 
[2m[36m(pid=35858)[0m   | Name      | Type              | Params
[2m[36m(pid=35858)[0m ------------------------------------------------
[2m[36m(pid=35858)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35858)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35858)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35858)[0m ------------------------------------------------
[2m[36m(pid=35858)[0m 8.7 K     Trainable params
[2m[36m(pid=35858)[0m 0         Non-trainable params
[2m[36m(pid=35858)[0m 8.7 K     Total params
[2m[36m(pid=35821)[0m time to fit was 428.5038285255432
[2m[36m(pid=35821)[0m GPU available: False, used: False
[2m[36m(pid=35821)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35821)[0m 
[2m[36m(pid=35821)[0m   | Name      | Type              | Params
[2m[36m(pid=35821)[0m ------------------------------------------------
[2m[36m(pid=35821)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35821)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35821)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35821)[0m ------------------------------------------------
[2m[36m(pid=35821)[0m 8.7 K     Trainable params
[2m[36m(pid=35821)[0m 0         Non-trainable params
[2m[36m(pid=35821)[0m 8.7 K     Total params
[2m[36m(pid=35829)[0m time to fit was 241.71715688705444
[2m[36m(pid=35829)[0m GPU available: False, used: False
[2m[36m(pid=35829)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35829)[0m 
[2m[36m(pid=35829)[0m   | Name      | Type              | Params
[2m[36m(pid=35829)[0m ------------------------------------------------
[2m[36m(pid=35829)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35829)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35829)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35829)[0m ------------------------------------------------
[2m[36m(pid=35829)[0m 8.7 K     Trainable params
[2m[36m(pid=35829)[0m 0         Non-trainable params
[2m[36m(pid=35829)[0m 8.7 K     Total params
[2m[36m(pid=35845)[0m time to fit was 121.82302117347717
[2m[36m(pid=35845)[0m GPU available: False, used: False
[2m[36m(pid=35845)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35845)[0m 
[2m[36m(pid=35845)[0m   | Name      | Type              | Params
[2m[36m(pid=35845)[0m ------------------------------------------------
[2m[36m(pid=35845)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35845)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35845)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35845)[0m ------------------------------------------------
[2m[36m(pid=35845)[0m 8.7 K     Trainable params
[2m[36m(pid=35845)[0m 0         Non-trainable params
[2m[36m(pid=35845)[0m 8.7 K     Total params
[2m[36m(pid=35868)[0m time to fit was 217.7316529750824
[2m[36m(pid=35868)[0m GPU available: False, used: False
[2m[36m(pid=35868)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35868)[0m 
[2m[36m(pid=35868)[0m   | Name      | Type              | Params
[2m[36m(pid=35868)[0m ------------------------------------------------
[2m[36m(pid=35868)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35868)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35868)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35868)[0m ------------------------------------------------
[2m[36m(pid=35868)[0m 8.7 K     Trainable params
[2m[36m(pid=35868)[0m 0         Non-trainable params
[2m[36m(pid=35868)[0m 8.7 K     Total params
[2m[36m(pid=35873)[0m time to fit was 236.99753713607788
[2m[36m(pid=35873)[0m GPU available: False, used: False
[2m[36m(pid=35873)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35873)[0m 
[2m[36m(pid=35873)[0m   | Name      | Type              | Params
[2m[36m(pid=35873)[0m ------------------------------------------------
[2m[36m(pid=35873)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35873)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35873)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35873)[0m ------------------------------------------------
[2m[36m(pid=35873)[0m 8.7 K     Trainable params
[2m[36m(pid=35873)[0m 0         Non-trainable params
[2m[36m(pid=35873)[0m 8.7 K     Total params
[2m[36m(pid=35832)[0m time to fit was 225.2324960231781
[2m[36m(pid=35832)[0m GPU available: False, used: False
[2m[36m(pid=35832)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35832)[0m 
[2m[36m(pid=35832)[0m   | Name      | Type              | Params
[2m[36m(pid=35832)[0m ------------------------------------------------
[2m[36m(pid=35832)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35832)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35832)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35832)[0m ------------------------------------------------
[2m[36m(pid=35832)[0m 8.7 K     Trainable params
[2m[36m(pid=35832)[0m 0         Non-trainable params
[2m[36m(pid=35832)[0m 8.7 K     Total params
[2m[36m(pid=35860)[0m time to fit was 160.62251019477844
[2m[36m(pid=35860)[0m Finished run with seed 0 - lr 2 - sec_lr 0.001 - bs 256 - mean val auc: 0.911411702632904
Result for _inner_e8deb_00023:
  auc: 0.911411702632904
  date: 2021-03-19_11-38-08
  done: false
  experiment_id: 0bbf4b14e14644f8809abcbe75d12b6e
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35860
  time_since_restore: 975.4628055095673
  time_this_iter_s: 975.4628055095673
  time_total_s: 975.4628055095673
  timestamp: 1616150288
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00023
  
== Status ==
Memory usage on this node: 9.6/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 32/180 (1 PENDING, 26 RUNNING, 5 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |       |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00003 | RUNNING    |       |          256 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00004 | RUNNING    |       |          512 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |       |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00008 | RUNNING    |       |          256 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00009 | RUNNING    |       |          512 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00011 | RUNNING    |       |           64 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00012 | RUNNING    |       |          128 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    |       |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00031 | PENDING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00018 | TERMINATED |       |          256 |     0 | 1     |    0.001 |      1 |          725.295 | 0.913732 |
| _inner_e8deb_00019 | TERMINATED |       |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
| _inner_e8deb_00024 | TERMINATED |       |          512 |     0 | 2     |    0.001 |      1 |          791.187 | 0.91044  |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 12 more trials not shown (12 RUNNING)


Result for _inner_e8deb_00023:
  auc: 0.911411702632904
  date: 2021-03-19_11-38-08
  done: true
  experiment_id: 0bbf4b14e14644f8809abcbe75d12b6e
  experiment_tag: 23_batch_size=256,eta=0.0,lr=2,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35860
  time_since_restore: 975.4628055095673
  time_this_iter_s: 975.4628055095673
  time_total_s: 975.4628055095673
  timestamp: 1616150288
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00023
  
[2m[36m(pid=35886)[0m time to fit was 239.2712733745575
[2m[36m(pid=35886)[0m GPU available: False, used: False
[2m[36m(pid=35886)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35886)[0m 
[2m[36m(pid=35886)[0m   | Name      | Type              | Params
[2m[36m(pid=35886)[0m ------------------------------------------------
[2m[36m(pid=35886)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35886)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35886)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35886)[0m ------------------------------------------------
[2m[36m(pid=35886)[0m 8.7 K     Trainable params
[2m[36m(pid=35886)[0m 0         Non-trainable params
[2m[36m(pid=35886)[0m 8.7 K     Total params
[2m[36m(pid=35818)[0m Starting run with seed 0 - lr 0.001 - sec_lr 0.01 - bs 64
[2m[36m(pid=35818)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35818)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35818)[0m GPU available: False, used: False
[2m[36m(pid=35818)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35818)[0m 
[2m[36m(pid=35818)[0m   | Name      | Type              | Params
[2m[36m(pid=35818)[0m ------------------------------------------------
[2m[36m(pid=35818)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35818)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35818)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35818)[0m ------------------------------------------------
[2m[36m(pid=35818)[0m 8.7 K     Trainable params
[2m[36m(pid=35818)[0m 0         Non-trainable params
[2m[36m(pid=35818)[0m 8.7 K     Total params
[2m[36m(pid=35818)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35818)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35818)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35818)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35818)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35818)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35871)[0m time to fit was 579.109130859375
[2m[36m(pid=35871)[0m GPU available: False, used: False
[2m[36m(pid=35871)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35871)[0m 
[2m[36m(pid=35871)[0m   | Name      | Type              | Params
[2m[36m(pid=35871)[0m ------------------------------------------------
[2m[36m(pid=35871)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35871)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35871)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35871)[0m ------------------------------------------------
[2m[36m(pid=35871)[0m 8.7 K     Trainable params
[2m[36m(pid=35871)[0m 0         Non-trainable params
[2m[36m(pid=35871)[0m 8.7 K     Total params
[2m[36m(pid=35832)[0m time to fit was 59.20949125289917
[2m[36m(pid=35832)[0m GPU available: False, used: False
[2m[36m(pid=35832)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35832)[0m 
[2m[36m(pid=35832)[0m   | Name      | Type              | Params
[2m[36m(pid=35832)[0m ------------------------------------------------
[2m[36m(pid=35832)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35832)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35832)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35832)[0m ------------------------------------------------
[2m[36m(pid=35832)[0m 8.7 K     Trainable params
[2m[36m(pid=35832)[0m 0         Non-trainable params
[2m[36m(pid=35832)[0m 8.7 K     Total params
[2m[36m(pid=35857)[0m time to fit was 1034.4735934734344
[2m[36m(pid=35857)[0m GPU available: False, used: False
[2m[36m(pid=35857)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35857)[0m 
[2m[36m(pid=35857)[0m   | Name      | Type              | Params
[2m[36m(pid=35857)[0m ------------------------------------------------
[2m[36m(pid=35857)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35857)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35857)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35857)[0m ------------------------------------------------
[2m[36m(pid=35857)[0m 8.7 K     Trainable params
[2m[36m(pid=35857)[0m 0         Non-trainable params
[2m[36m(pid=35857)[0m 8.7 K     Total params
[2m[36m(pid=35795)[0m time to fit was 302.69883131980896
[2m[36m(pid=35795)[0m GPU available: False, used: False
[2m[36m(pid=35795)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35795)[0m 
[2m[36m(pid=35795)[0m   | Name      | Type              | Params
[2m[36m(pid=35795)[0m ------------------------------------------------
[2m[36m(pid=35795)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35795)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35795)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35795)[0m ------------------------------------------------
[2m[36m(pid=35795)[0m 8.7 K     Trainable params
[2m[36m(pid=35795)[0m 0         Non-trainable params
[2m[36m(pid=35795)[0m 8.7 K     Total params
[2m[36m(pid=35845)[0m time to fit was 120.85819697380066
[2m[36m(pid=35845)[0m GPU available: False, used: False
[2m[36m(pid=35845)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35845)[0m 
[2m[36m(pid=35845)[0m   | Name      | Type              | Params
[2m[36m(pid=35845)[0m ------------------------------------------------
[2m[36m(pid=35845)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35845)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35845)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35845)[0m ------------------------------------------------
[2m[36m(pid=35845)[0m 8.7 K     Trainable params
[2m[36m(pid=35845)[0m 0         Non-trainable params
[2m[36m(pid=35845)[0m 8.7 K     Total params
[2m[36m(pid=35856)[0m time to fit was 347.02466917037964
[2m[36m(pid=35856)[0m GPU available: False, used: False
[2m[36m(pid=35856)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35856)[0m 
[2m[36m(pid=35856)[0m   | Name      | Type              | Params
[2m[36m(pid=35856)[0m ------------------------------------------------
[2m[36m(pid=35856)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35856)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35856)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35856)[0m ------------------------------------------------
[2m[36m(pid=35856)[0m 8.7 K     Trainable params
[2m[36m(pid=35856)[0m 0         Non-trainable params
[2m[36m(pid=35856)[0m 8.7 K     Total params
[2m[36m(pid=35881)[0m time to fit was 347.47797298431396
[2m[36m(pid=35881)[0m GPU available: False, used: False
[2m[36m(pid=35881)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35881)[0m 
[2m[36m(pid=35881)[0m   | Name      | Type              | Params
[2m[36m(pid=35881)[0m ------------------------------------------------
[2m[36m(pid=35881)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35881)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35881)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35881)[0m ------------------------------------------------
[2m[36m(pid=35881)[0m 8.7 K     Trainable params
[2m[36m(pid=35881)[0m 0         Non-trainable params
[2m[36m(pid=35881)[0m 8.7 K     Total params
[2m[36m(pid=35893)[0m time to fit was 348.67408657073975
[2m[36m(pid=35893)[0m GPU available: False, used: False
[2m[36m(pid=35893)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35893)[0m 
[2m[36m(pid=35893)[0m   | Name      | Type              | Params
[2m[36m(pid=35893)[0m ------------------------------------------------
[2m[36m(pid=35893)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35893)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35893)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35893)[0m ------------------------------------------------
[2m[36m(pid=35893)[0m 8.7 K     Trainable params
[2m[36m(pid=35893)[0m 0         Non-trainable params
[2m[36m(pid=35893)[0m 8.7 K     Total params
[2m[36m(pid=35832)[0m time to fit was 55.64706015586853
[2m[36m(pid=35832)[0m GPU available: False, used: False
[2m[36m(pid=35832)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35832)[0m 
[2m[36m(pid=35832)[0m   | Name      | Type              | Params
[2m[36m(pid=35832)[0m ------------------------------------------------
[2m[36m(pid=35832)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35832)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35832)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35832)[0m ------------------------------------------------
[2m[36m(pid=35832)[0m 8.7 K     Trainable params
[2m[36m(pid=35832)[0m 0         Non-trainable params
[2m[36m(pid=35832)[0m 8.7 K     Total params
[2m[36m(pid=35848)[0m time to fit was 1130.6119573116302
[2m[36m(pid=35848)[0m GPU available: False, used: False
[2m[36m(pid=35848)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35848)[0m 
[2m[36m(pid=35848)[0m   | Name      | Type              | Params
[2m[36m(pid=35848)[0m ------------------------------------------------
[2m[36m(pid=35848)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35848)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35848)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35848)[0m ------------------------------------------------
[2m[36m(pid=35848)[0m 8.7 K     Trainable params
[2m[36m(pid=35848)[0m 0         Non-trainable params
[2m[36m(pid=35848)[0m 8.7 K     Total params
[2m[36m(pid=35829)[0m time to fit was 215.08838939666748
Result for _inner_e8deb_00017:
  auc: 0.9134082913398742
  date: 2021-03-19_11-40-44
  done: false
  experiment_id: 0fd1881da3f845ca926e5a5ad7e662b0
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35829
  time_since_restore: 1131.2041342258453
  time_this_iter_s: 1131.2041342258453
  time_total_s: 1131.2041342258453
  timestamp: 1616150444
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00017
  
[2m[36m(pid=35829)[0m Finished run with seed 0 - lr 1 - sec_lr 0.001 - bs 128 - mean val auc: 0.9134082913398742
== Status ==
Memory usage on this node: 9.6/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 33/180 (1 PENDING, 26 RUNNING, 6 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |       |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00003 | RUNNING    |       |          256 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00004 | RUNNING    |       |          512 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |       |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00008 | RUNNING    |       |          256 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00009 | RUNNING    |       |          512 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00011 | RUNNING    |       |           64 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00012 | RUNNING    |       |          128 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00032 | PENDING    |       |          128 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00018 | TERMINATED |       |          256 |     0 | 1     |    0.001 |      1 |          725.295 | 0.913732 |
| _inner_e8deb_00019 | TERMINATED |       |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
| _inner_e8deb_00023 | TERMINATED |       |          256 |     0 | 2     |    0.001 |      1 |          975.463 | 0.911412 |
| _inner_e8deb_00024 | TERMINATED |       |          512 |     0 | 2     |    0.001 |      1 |          791.187 | 0.91044  |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 13 more trials not shown (13 RUNNING)


Result for _inner_e8deb_00017:
  auc: 0.9134082913398742
  date: 2021-03-19_11-40-44
  done: true
  experiment_id: 0fd1881da3f845ca926e5a5ad7e662b0
  experiment_tag: 17_batch_size=128,eta=0.0,lr=1,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35829
  time_since_restore: 1131.2041342258453
  time_this_iter_s: 1131.2041342258453
  time_total_s: 1131.2041342258453
  timestamp: 1616150444
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00017
  
[2m[36m(pid=35803)[0m time to fit was 1131.7971217632294
[2m[36m(pid=35803)[0m GPU available: False, used: False
[2m[36m(pid=35803)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35803)[0m 
[2m[36m(pid=35803)[0m   | Name      | Type              | Params
[2m[36m(pid=35803)[0m ------------------------------------------------
[2m[36m(pid=35803)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35803)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35803)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35803)[0m ------------------------------------------------
[2m[36m(pid=35803)[0m 8.7 K     Trainable params
[2m[36m(pid=35803)[0m 0         Non-trainable params
[2m[36m(pid=35803)[0m 8.7 K     Total params
[2m[36m(pid=35823)[0m Starting run with seed 0 - lr 0.001 - sec_lr 0.01 - bs 128
[2m[36m(pid=35823)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35823)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35823)[0m GPU available: False, used: False
[2m[36m(pid=35823)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35823)[0m 
[2m[36m(pid=35823)[0m   | Name      | Type              | Params
[2m[36m(pid=35823)[0m ------------------------------------------------
[2m[36m(pid=35823)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35823)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35823)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35823)[0m ------------------------------------------------
[2m[36m(pid=35823)[0m 8.7 K     Trainable params
[2m[36m(pid=35823)[0m 0         Non-trainable params
[2m[36m(pid=35823)[0m 8.7 K     Total params
[2m[36m(pid=35823)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35823)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35823)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35823)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35823)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35823)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35795)[0m time to fit was 106.22423982620239
[2m[36m(pid=35795)[0m GPU available: False, used: False
[2m[36m(pid=35795)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35795)[0m 
[2m[36m(pid=35795)[0m   | Name      | Type              | Params
[2m[36m(pid=35795)[0m ------------------------------------------------
[2m[36m(pid=35795)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35795)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35795)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35795)[0m ------------------------------------------------
[2m[36m(pid=35795)[0m 8.7 K     Trainable params
[2m[36m(pid=35795)[0m 0         Non-trainable params
[2m[36m(pid=35795)[0m 8.7 K     Total params
[2m[36m(pid=35832)[0m time to fit was 66.65227460861206
[2m[36m(pid=35832)[0m GPU available: False, used: False
[2m[36m(pid=35832)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35832)[0m 
[2m[36m(pid=35832)[0m   | Name      | Type              | Params
[2m[36m(pid=35832)[0m ------------------------------------------------
[2m[36m(pid=35832)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35832)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35832)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35832)[0m ------------------------------------------------
[2m[36m(pid=35832)[0m 8.7 K     Trainable params
[2m[36m(pid=35832)[0m 0         Non-trainable params
[2m[36m(pid=35832)[0m 8.7 K     Total params
[2m[36m(pid=35845)[0m time to fit was 122.28675508499146
[2m[36m(pid=35845)[0m GPU available: False, used: False
[2m[36m(pid=35845)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35855)[0m time to fit was 577.5178604125977
[2m[36m(pid=35855)[0m GPU available: False, used: False
[2m[36m(pid=35855)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35845)[0m 
[2m[36m(pid=35845)[0m   | Name      | Type              | Params
[2m[36m(pid=35845)[0m ------------------------------------------------
[2m[36m(pid=35845)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35845)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35845)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35845)[0m ------------------------------------------------
[2m[36m(pid=35845)[0m 8.7 K     Trainable params
[2m[36m(pid=35845)[0m 0         Non-trainable params
[2m[36m(pid=35845)[0m 8.7 K     Total params
[2m[36m(pid=35855)[0m 
[2m[36m(pid=35855)[0m   | Name      | Type              | Params
[2m[36m(pid=35855)[0m ------------------------------------------------
[2m[36m(pid=35855)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35855)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35855)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35855)[0m ------------------------------------------------
[2m[36m(pid=35855)[0m 8.7 K     Trainable params
[2m[36m(pid=35855)[0m 0         Non-trainable params
[2m[36m(pid=35855)[0m 8.7 K     Total params
[2m[36m(pid=35858)[0m time to fit was 295.9749822616577
[2m[36m(pid=35858)[0m GPU available: False, used: False
[2m[36m(pid=35858)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35858)[0m 
[2m[36m(pid=35858)[0m   | Name      | Type              | Params
[2m[36m(pid=35858)[0m ------------------------------------------------
[2m[36m(pid=35858)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35858)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35858)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35858)[0m ------------------------------------------------
[2m[36m(pid=35858)[0m 8.7 K     Trainable params
[2m[36m(pid=35858)[0m 0         Non-trainable params
[2m[36m(pid=35858)[0m 8.7 K     Total params
[2m[36m(pid=35868)[0m time to fit was 237.42817449569702
[2m[36m(pid=35868)[0m Finished run with seed 0 - lr 0.1 - sec_lr 0.001 - bs 128 - mean val auc: 0.9138000965118408
Result for _inner_e8deb_00012:
  auc: 0.9138000965118408
  date: 2021-03-19_11-41-50
  done: false
  experiment_id: 153e31c5b93a425eacb9419000d8fd1c
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35868
  time_since_restore: 1197.1476483345032
  time_this_iter_s: 1197.1476483345032
  time_total_s: 1197.1476483345032
  timestamp: 1616150510
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00012
  
== Status ==
Memory usage on this node: 9.7/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 34/180 (1 PENDING, 26 RUNNING, 7 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |       |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00003 | RUNNING    |       |          256 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00004 | RUNNING    |       |          512 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |       |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00008 | RUNNING    |       |          256 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00009 | RUNNING    |       |          512 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00011 | RUNNING    |       |           64 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00033 | PENDING    |       |          256 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00017 | TERMINATED |       |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
| _inner_e8deb_00018 | TERMINATED |       |          256 |     0 | 1     |    0.001 |      1 |          725.295 | 0.913732 |
| _inner_e8deb_00019 | TERMINATED |       |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
| _inner_e8deb_00023 | TERMINATED |       |          256 |     0 | 2     |    0.001 |      1 |          975.463 | 0.911412 |
| _inner_e8deb_00024 | TERMINATED |       |          512 |     0 | 2     |    0.001 |      1 |          791.187 | 0.91044  |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 14 more trials not shown (14 RUNNING)


Result for _inner_e8deb_00012:
  auc: 0.9138000965118408
  date: 2021-03-19_11-41-50
  done: true
  experiment_id: 153e31c5b93a425eacb9419000d8fd1c
  experiment_tag: 12_batch_size=128,eta=0.0,lr=0.1,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35868
  time_since_restore: 1197.1476483345032
  time_this_iter_s: 1197.1476483345032
  time_total_s: 1197.1476483345032
  timestamp: 1616150510
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00012
  
[2m[36m(pid=35873)[0m time to fit was 235.51368474960327
Result for _inner_e8deb_00009:
  auc: 0.9121008276939392
  date: 2021-03-19_11-41-59
  done: false
  experiment_id: 50ca23a32aba4ae2bc27cc029e941c21
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35873
  time_since_restore: 1207.2250158786774
  time_this_iter_s: 1207.2250158786774
  time_total_s: 1207.2250158786774
  timestamp: 1616150519
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00009
  
[2m[36m(pid=35873)[0m Finished run with seed 0 - lr 0.01 - sec_lr 0.001 - bs 512 - mean val auc: 0.9121008276939392
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 35/180 (1 PENDING, 26 RUNNING, 8 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |                     |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00003 | RUNNING    |                     |          256 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00004 | RUNNING    |                     |          512 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |                     |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |                     |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00008 | RUNNING    |                     |          256 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00009 | RUNNING    | 145.101.32.82:35873 |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | RUNNING    |                     |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00034 | PENDING    |                     |          512 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |                     |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00017 | TERMINATED |                     |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
| _inner_e8deb_00018 | TERMINATED |                     |          256 |     0 | 1     |    0.001 |      1 |          725.295 | 0.913732 |
| _inner_e8deb_00019 | TERMINATED |                     |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
| _inner_e8deb_00023 | TERMINATED |                     |          256 |     0 | 2     |    0.001 |      1 |          975.463 | 0.911412 |
| _inner_e8deb_00024 | TERMINATED |                     |          512 |     0 | 2     |    0.001 |      1 |          791.187 | 0.91044  |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 15 more trials not shown (15 RUNNING)


Result for _inner_e8deb_00009:
  auc: 0.9121008276939392
  date: 2021-03-19_11-41-59
  done: true
  experiment_id: 50ca23a32aba4ae2bc27cc029e941c21
  experiment_tag: 9_batch_size=512,eta=0.0,lr=0.01,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35873
  time_since_restore: 1207.2250158786774
  time_this_iter_s: 1207.2250158786774
  time_total_s: 1207.2250158786774
  timestamp: 1616150519
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00009
  
[2m[36m(pid=35810)[0m Starting run with seed 0 - lr 0.001 - sec_lr 0.01 - bs 256
[2m[36m(pid=35810)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35810)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35810)[0m GPU available: False, used: False
[2m[36m(pid=35810)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35810)[0m 
[2m[36m(pid=35810)[0m   | Name      | Type              | Params
[2m[36m(pid=35810)[0m ------------------------------------------------
[2m[36m(pid=35810)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35810)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35810)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35810)[0m ------------------------------------------------
[2m[36m(pid=35810)[0m 8.7 K     Trainable params
[2m[36m(pid=35810)[0m 0         Non-trainable params
[2m[36m(pid=35810)[0m 8.7 K     Total params
[2m[36m(pid=35810)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35810)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35810)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35810)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35810)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35810)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35885)[0m time to fit was 427.449880361557
[2m[36m(pid=35885)[0m GPU available: False, used: False
[2m[36m(pid=35885)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35885)[0m 
[2m[36m(pid=35885)[0m   | Name      | Type              | Params
[2m[36m(pid=35885)[0m ------------------------------------------------
[2m[36m(pid=35885)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35885)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35885)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35885)[0m ------------------------------------------------
[2m[36m(pid=35885)[0m 8.7 K     Trainable params
[2m[36m(pid=35885)[0m 0         Non-trainable params
[2m[36m(pid=35885)[0m 8.7 K     Total params
[2m[36m(pid=35886)[0m time to fit was 237.53449392318726
Result for _inner_e8deb_00004:
  auc: 0.890098226070404
  date: 2021-03-19_11-42-08
  done: false
  experiment_id: 57b93ed8fe74466d84f073ba5c316afa
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35886
  time_since_restore: 1215.2106914520264
  time_this_iter_s: 1215.2106914520264
  time_total_s: 1215.2106914520264
  timestamp: 1616150528
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00004
  
[2m[36m(pid=35886)[0m Finished run with seed 0 - lr 0.001 - sec_lr 0.001 - bs 512 - mean val auc: 0.890098226070404
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 36/180 (1 PENDING, 26 RUNNING, 9 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |                     |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00003 | RUNNING    |                     |          256 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00004 | RUNNING    | 145.101.32.82:35886 |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |                     |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |                     |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00008 | RUNNING    |                     |          256 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |                     |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00035 | PENDING    |                     |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |                     |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00017 | TERMINATED |                     |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
| _inner_e8deb_00018 | TERMINATED |                     |          256 |     0 | 1     |    0.001 |      1 |          725.295 | 0.913732 |
| _inner_e8deb_00019 | TERMINATED |                     |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
| _inner_e8deb_00023 | TERMINATED |                     |          256 |     0 | 2     |    0.001 |      1 |          975.463 | 0.911412 |
| _inner_e8deb_00024 | TERMINATED |                     |          512 |     0 | 2     |    0.001 |      1 |          791.187 | 0.91044  |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 16 more trials not shown (16 RUNNING)


Result for _inner_e8deb_00004:
  auc: 0.890098226070404
  date: 2021-03-19_11-42-08
  done: true
  experiment_id: 57b93ed8fe74466d84f073ba5c316afa
  experiment_tag: 4_batch_size=512,eta=0.0,lr=0.001,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35886
  time_since_restore: 1215.2106914520264
  time_this_iter_s: 1215.2106914520264
  time_total_s: 1215.2106914520264
  timestamp: 1616150528
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00004
  
[2m[36m(pid=35791)[0m Starting run with seed 0 - lr 0.001 - sec_lr 0.01 - bs 512
[2m[36m(pid=35791)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35791)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35791)[0m GPU available: False, used: False
[2m[36m(pid=35791)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35791)[0m 
[2m[36m(pid=35791)[0m   | Name      | Type              | Params
[2m[36m(pid=35791)[0m ------------------------------------------------
[2m[36m(pid=35791)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35791)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35791)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35791)[0m ------------------------------------------------
[2m[36m(pid=35791)[0m 8.7 K     Trainable params
[2m[36m(pid=35791)[0m 0         Non-trainable params
[2m[36m(pid=35791)[0m 8.7 K     Total params
[2m[36m(pid=35791)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35791)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35791)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35791)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35791)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35791)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35802)[0m Starting run with seed 0 - lr 0.01 - sec_lr 0.01 - bs 32
[2m[36m(pid=35802)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35802)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35802)[0m GPU available: False, used: False
[2m[36m(pid=35802)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35802)[0m 
[2m[36m(pid=35802)[0m   | Name      | Type              | Params
[2m[36m(pid=35802)[0m ------------------------------------------------
[2m[36m(pid=35802)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35802)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35802)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35802)[0m ------------------------------------------------
[2m[36m(pid=35802)[0m 8.7 K     Trainable params
[2m[36m(pid=35802)[0m 0         Non-trainable params
[2m[36m(pid=35802)[0m 8.7 K     Total params
[2m[36m(pid=35802)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35802)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35802)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35802)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35802)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35802)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35889)[0m time to fit was 700.5347294807434
[2m[36m(pid=35889)[0m GPU available: False, used: False
[2m[36m(pid=35889)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35889)[0m 
[2m[36m(pid=35889)[0m   | Name      | Type              | Params
[2m[36m(pid=35889)[0m ------------------------------------------------
[2m[36m(pid=35889)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35889)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35889)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35889)[0m ------------------------------------------------
[2m[36m(pid=35889)[0m 8.7 K     Trainable params
[2m[36m(pid=35889)[0m 0         Non-trainable params
[2m[36m(pid=35889)[0m 8.7 K     Total params
[2m[36m(pid=35852)[0m time to fit was 622.7609176635742
[2m[36m(pid=35852)[0m GPU available: False, used: False
[2m[36m(pid=35852)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35852)[0m 
[2m[36m(pid=35852)[0m   | Name      | Type              | Params
[2m[36m(pid=35852)[0m ------------------------------------------------
[2m[36m(pid=35852)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35852)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35852)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35852)[0m ------------------------------------------------
[2m[36m(pid=35852)[0m 8.7 K     Trainable params
[2m[36m(pid=35852)[0m 0         Non-trainable params
[2m[36m(pid=35852)[0m 8.7 K     Total params
[2m[36m(pid=35845)[0m time to fit was 130.8951976299286
Result for _inner_e8deb_00027:
  auc: 0.5180504560470581
  date: 2021-03-19_11-43-33
  done: false
  experiment_id: e922fae180fa4734baa09b5cc70536fe
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35845
  time_since_restore: 622.5467207431793
  time_this_iter_s: 622.5467207431793
  time_total_s: 622.5467207431793
  timestamp: 1616150613
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00027
  
[2m[36m(pid=35845)[0m Finished run with seed 0 - lr 5 - sec_lr 0.001 - bs 128 - mean val auc: 0.5180504560470581
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 37/180 (1 PENDING, 26 RUNNING, 10 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |       |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00003 | RUNNING    |       |          256 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |       |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00008 | RUNNING    |       |          256 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00011 | RUNNING    |       |           64 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00036 | PENDING    |       |           64 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00017 | TERMINATED |       |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
| _inner_e8deb_00018 | TERMINATED |       |          256 |     0 | 1     |    0.001 |      1 |          725.295 | 0.913732 |
| _inner_e8deb_00019 | TERMINATED |       |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
| _inner_e8deb_00023 | TERMINATED |       |          256 |     0 | 2     |    0.001 |      1 |          975.463 | 0.911412 |
| _inner_e8deb_00024 | TERMINATED |       |          512 |     0 | 2     |    0.001 |      1 |          791.187 | 0.91044  |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 17 more trials not shown (16 RUNNING)


Result for _inner_e8deb_00027:
  auc: 0.5180504560470581
  date: 2021-03-19_11-43-33
  done: true
  experiment_id: e922fae180fa4734baa09b5cc70536fe
  experiment_tag: 27_batch_size=128,eta=0.0,lr=5,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35845
  time_since_restore: 622.5467207431793
  time_this_iter_s: 622.5467207431793
  time_total_s: 622.5467207431793
  timestamp: 1616150613
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00027
  
[2m[36m(pid=35806)[0m Starting run with seed 0 - lr 0.01 - sec_lr 0.01 - bs 64
[2m[36m(pid=35806)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35806)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35806)[0m GPU available: False, used: False
[2m[36m(pid=35806)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35806)[0m 
[2m[36m(pid=35806)[0m   | Name      | Type              | Params
[2m[36m(pid=35806)[0m ------------------------------------------------
[2m[36m(pid=35806)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35806)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35806)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35806)[0m ------------------------------------------------
[2m[36m(pid=35806)[0m 8.7 K     Trainable params
[2m[36m(pid=35806)[0m 0         Non-trainable params
[2m[36m(pid=35806)[0m 8.7 K     Total params
[2m[36m(pid=35806)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35806)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35806)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35806)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35806)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35806)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35866)[0m time to fit was 548.590001821518
[2m[36m(pid=35866)[0m GPU available: False, used: False
[2m[36m(pid=35866)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35866)[0m 
[2m[36m(pid=35866)[0m   | Name      | Type              | Params
[2m[36m(pid=35866)[0m ------------------------------------------------
[2m[36m(pid=35866)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35866)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35866)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35866)[0m ------------------------------------------------
[2m[36m(pid=35866)[0m 8.7 K     Trainable params
[2m[36m(pid=35866)[0m 0         Non-trainable params
[2m[36m(pid=35866)[0m 8.7 K     Total params
[2m[36m(pid=35821)[0m time to fit was 419.97177028656006
[2m[36m(pid=35821)[0m GPU available: False, used: False
[2m[36m(pid=35821)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35821)[0m 
[2m[36m(pid=35821)[0m   | Name      | Type              | Params
[2m[36m(pid=35821)[0m ------------------------------------------------
[2m[36m(pid=35821)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35821)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35821)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35821)[0m ------------------------------------------------
[2m[36m(pid=35821)[0m 8.7 K     Trainable params
[2m[36m(pid=35821)[0m 0         Non-trainable params
[2m[36m(pid=35821)[0m 8.7 K     Total params
[2m[36m(pid=35878)[0m time to fit was 626.7261729240417
[2m[36m(pid=35878)[0m GPU available: False, used: False
[2m[36m(pid=35878)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35878)[0m 
[2m[36m(pid=35878)[0m   | Name      | Type              | Params
[2m[36m(pid=35878)[0m ------------------------------------------------
[2m[36m(pid=35878)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35878)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35878)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35878)[0m ------------------------------------------------
[2m[36m(pid=35878)[0m 8.7 K     Trainable params
[2m[36m(pid=35878)[0m 0         Non-trainable params
[2m[36m(pid=35878)[0m 8.7 K     Total params
[2m[36m(pid=35832)[0m time to fit was 230.88834881782532
Result for _inner_e8deb_00029:
  auc: 0.7142040133476257
  date: 2021-03-19_11-44-57
  done: false
  experiment_id: 4913c3884ff14291903e8bc12af26fef
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35832
  time_since_restore: 638.8938715457916
  time_this_iter_s: 638.8938715457916
  time_total_s: 638.8938715457916
  timestamp: 1616150697
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00029
  
[2m[36m(pid=35832)[0m Finished run with seed 0 - lr 5 - sec_lr 0.001 - bs 512 - mean val auc: 0.7142040133476257
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 38/180 (1 PENDING, 26 RUNNING, 11 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |       |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00003 | RUNNING    |       |          256 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |       |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00008 | RUNNING    |       |          256 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00011 | RUNNING    |       |           64 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00037 | PENDING    |       |          128 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00017 | TERMINATED |       |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
| _inner_e8deb_00018 | TERMINATED |       |          256 |     0 | 1     |    0.001 |      1 |          725.295 | 0.913732 |
| _inner_e8deb_00019 | TERMINATED |       |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
| _inner_e8deb_00023 | TERMINATED |       |          256 |     0 | 2     |    0.001 |      1 |          975.463 | 0.911412 |
| _inner_e8deb_00024 | TERMINATED |       |          512 |     0 | 2     |    0.001 |      1 |          791.187 | 0.91044  |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 18 more trials not shown (16 RUNNING, 1 TERMINATED)


Result for _inner_e8deb_00029:
  auc: 0.7142040133476257
  date: 2021-03-19_11-44-57
  done: true
  experiment_id: 4913c3884ff14291903e8bc12af26fef
  experiment_tag: 29_batch_size=512,eta=0.0,lr=5,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35832
  time_since_restore: 638.8938715457916
  time_this_iter_s: 638.8938715457916
  time_total_s: 638.8938715457916
  timestamp: 1616150697
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00029
  
[2m[36m(pid=35820)[0m Starting run with seed 0 - lr 0.01 - sec_lr 0.01 - bs 128
[2m[36m(pid=35820)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35820)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35820)[0m GPU available: False, used: False
[2m[36m(pid=35820)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35820)[0m 
[2m[36m(pid=35820)[0m   | Name      | Type              | Params
[2m[36m(pid=35820)[0m ------------------------------------------------
[2m[36m(pid=35820)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35820)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35820)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35820)[0m ------------------------------------------------
[2m[36m(pid=35820)[0m 8.7 K     Trainable params
[2m[36m(pid=35820)[0m 0         Non-trainable params
[2m[36m(pid=35820)[0m 8.7 K     Total params
[2m[36m(pid=35820)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35820)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35820)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35820)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35820)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35820)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35856)[0m time to fit was 341.08703446388245
[2m[36m(pid=35856)[0m GPU available: False, used: False
[2m[36m(pid=35856)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35856)[0m 
[2m[36m(pid=35856)[0m   | Name      | Type              | Params
[2m[36m(pid=35856)[0m ------------------------------------------------
[2m[36m(pid=35856)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35856)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35856)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35856)[0m ------------------------------------------------
[2m[36m(pid=35856)[0m 8.7 K     Trainable params
[2m[36m(pid=35856)[0m 0         Non-trainable params
[2m[36m(pid=35856)[0m 8.7 K     Total params
[2m[36m(pid=35881)[0m time to fit was 340.97168803215027
[2m[36m(pid=35881)[0m GPU available: False, used: False
[2m[36m(pid=35881)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35881)[0m 
[2m[36m(pid=35881)[0m   | Name      | Type              | Params
[2m[36m(pid=35881)[0m ------------------------------------------------
[2m[36m(pid=35881)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35881)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35881)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35881)[0m ------------------------------------------------
[2m[36m(pid=35881)[0m 8.7 K     Trainable params
[2m[36m(pid=35881)[0m 0         Non-trainable params
[2m[36m(pid=35881)[0m 8.7 K     Total params
[2m[36m(pid=35858)[0m time to fit was 252.6340913772583
Result for _inner_e8deb_00022:
  auc: 0.8304531335830688
  date: 2021-03-19_11-45-37
  done: false
  experiment_id: fa252496c55d473dab469f8a2b41277e
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35858
  time_since_restore: 1423.3489291667938
  time_this_iter_s: 1423.3489291667938
  time_total_s: 1423.3489291667938
  timestamp: 1616150737
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00022
  
[2m[36m(pid=35858)[0m Finished run with seed 0 - lr 2 - sec_lr 0.001 - bs 128 - mean val auc: 0.8304531335830688
== Status ==
Memory usage on this node: 9.6/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 39/180 (1 PENDING, 26 RUNNING, 12 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |       |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00003 | RUNNING    |       |          256 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |       |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00008 | RUNNING    |       |          256 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00011 | RUNNING    |       |           64 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00038 | PENDING    |       |          256 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00017 | TERMINATED |       |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
| _inner_e8deb_00018 | TERMINATED |       |          256 |     0 | 1     |    0.001 |      1 |          725.295 | 0.913732 |
| _inner_e8deb_00019 | TERMINATED |       |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
| _inner_e8deb_00023 | TERMINATED |       |          256 |     0 | 2     |    0.001 |      1 |          975.463 | 0.911412 |
| _inner_e8deb_00024 | TERMINATED |       |          512 |     0 | 2     |    0.001 |      1 |          791.187 | 0.91044  |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 19 more trials not shown (16 RUNNING, 2 TERMINATED)


Result for _inner_e8deb_00022:
  auc: 0.8304531335830688
  date: 2021-03-19_11-45-37
  done: true
  experiment_id: fa252496c55d473dab469f8a2b41277e
  experiment_tag: 22_batch_size=128,eta=0.0,lr=2,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35858
  time_since_restore: 1423.3489291667938
  time_this_iter_s: 1423.3489291667938
  time_total_s: 1423.3489291667938
  timestamp: 1616150737
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00022
  
[2m[36m(pid=35792)[0m Starting run with seed 0 - lr 0.01 - sec_lr 0.01 - bs 256
[2m[36m(pid=35792)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35792)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35792)[0m GPU available: False, used: False
[2m[36m(pid=35792)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35792)[0m 
[2m[36m(pid=35792)[0m   | Name      | Type              | Params
[2m[36m(pid=35792)[0m ------------------------------------------------
[2m[36m(pid=35792)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35792)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35792)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35792)[0m ------------------------------------------------
[2m[36m(pid=35792)[0m 8.7 K     Trainable params
[2m[36m(pid=35792)[0m 0         Non-trainable params
[2m[36m(pid=35792)[0m 8.7 K     Total params
[2m[36m(pid=35792)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35792)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35792)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35792)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35792)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35792)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35791)[0m time to fit was 227.63841462135315
[2m[36m(pid=35791)[0m GPU available: False, used: False
[2m[36m(pid=35791)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35791)[0m 
[2m[36m(pid=35791)[0m   | Name      | Type              | Params
[2m[36m(pid=35791)[0m ------------------------------------------------
[2m[36m(pid=35791)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35791)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35791)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35791)[0m ------------------------------------------------
[2m[36m(pid=35791)[0m 8.7 K     Trainable params
[2m[36m(pid=35791)[0m 0         Non-trainable params
[2m[36m(pid=35791)[0m 8.7 K     Total params
[2m[36m(pid=35795)[0m time to fit was 331.7222545146942
[2m[36m(pid=35795)[0m GPU available: False, used: False
[2m[36m(pid=35795)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35795)[0m 
[2m[36m(pid=35795)[0m   | Name      | Type              | Params
[2m[36m(pid=35795)[0m ------------------------------------------------
[2m[36m(pid=35795)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35795)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35795)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35795)[0m ------------------------------------------------
[2m[36m(pid=35795)[0m 8.7 K     Trainable params
[2m[36m(pid=35795)[0m 0         Non-trainable params
[2m[36m(pid=35795)[0m 8.7 K     Total params
[2m[36m(pid=35810)[0m time to fit was 332.7318584918976
[2m[36m(pid=35810)[0m GPU available: False, used: False
[2m[36m(pid=35810)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35810)[0m 
[2m[36m(pid=35810)[0m   | Name      | Type              | Params
[2m[36m(pid=35810)[0m ------------------------------------------------
[2m[36m(pid=35810)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35810)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35810)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35810)[0m ------------------------------------------------
[2m[36m(pid=35810)[0m 8.7 K     Trainable params
[2m[36m(pid=35810)[0m 0         Non-trainable params
[2m[36m(pid=35810)[0m 8.7 K     Total params
[2m[36m(pid=35885)[0m time to fit was 334.9778735637665
[2m[36m(pid=35885)[0m GPU available: False, used: False
[2m[36m(pid=35885)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35885)[0m 
[2m[36m(pid=35885)[0m   | Name      | Type              | Params
[2m[36m(pid=35885)[0m ------------------------------------------------
[2m[36m(pid=35885)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35885)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35885)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35885)[0m ------------------------------------------------
[2m[36m(pid=35885)[0m 8.7 K     Trainable params
[2m[36m(pid=35885)[0m 0         Non-trainable params
[2m[36m(pid=35885)[0m 8.7 K     Total params
[2m[36m(pid=35893)[0m time to fit was 462.6901240348816
[2m[36m(pid=35893)[0m GPU available: False, used: False
[2m[36m(pid=35893)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35893)[0m 
[2m[36m(pid=35893)[0m   | Name      | Type              | Params
[2m[36m(pid=35893)[0m ------------------------------------------------
[2m[36m(pid=35893)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35893)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35893)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35893)[0m ------------------------------------------------
[2m[36m(pid=35893)[0m 8.7 K     Trainable params
[2m[36m(pid=35893)[0m 0         Non-trainable params
[2m[36m(pid=35893)[0m 8.7 K     Total params
[2m[36m(pid=35871)[0m time to fit was 561.9617855548859
[2m[36m(pid=35871)[0m GPU available: False, used: False
[2m[36m(pid=35871)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35871)[0m 
[2m[36m(pid=35871)[0m   | Name      | Type              | Params
[2m[36m(pid=35871)[0m ------------------------------------------------
[2m[36m(pid=35871)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35871)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35871)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35871)[0m ------------------------------------------------
[2m[36m(pid=35871)[0m 8.7 K     Trainable params
[2m[36m(pid=35871)[0m 0         Non-trainable params
[2m[36m(pid=35871)[0m 8.7 K     Total params
[2m[36m(pid=35791)[0m time to fit was 226.65826201438904
[2m[36m(pid=35791)[0m GPU available: False, used: False
[2m[36m(pid=35791)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35791)[0m 
[2m[36m(pid=35791)[0m   | Name      | Type              | Params
[2m[36m(pid=35791)[0m ------------------------------------------------
[2m[36m(pid=35791)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35791)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35791)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35791)[0m ------------------------------------------------
[2m[36m(pid=35791)[0m 8.7 K     Trainable params
[2m[36m(pid=35791)[0m 0         Non-trainable params
[2m[36m(pid=35791)[0m 8.7 K     Total params
[2m[36m(pid=35823)[0m time to fit was 554.6035120487213
[2m[36m(pid=35823)[0m GPU available: False, used: False
[2m[36m(pid=35823)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35823)[0m 
[2m[36m(pid=35823)[0m   | Name      | Type              | Params
[2m[36m(pid=35823)[0m ------------------------------------------------
[2m[36m(pid=35823)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35823)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35823)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35823)[0m ------------------------------------------------
[2m[36m(pid=35823)[0m 8.7 K     Trainable params
[2m[36m(pid=35823)[0m 0         Non-trainable params
[2m[36m(pid=35823)[0m 8.7 K     Total params
[2m[36m(pid=35820)[0m time to fit was 304.2400782108307
[2m[36m(pid=35820)[0m GPU available: False, used: False
[2m[36m(pid=35820)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35820)[0m 
[2m[36m(pid=35820)[0m   | Name      | Type              | Params
[2m[36m(pid=35820)[0m ------------------------------------------------
[2m[36m(pid=35820)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35820)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35820)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35820)[0m ------------------------------------------------
[2m[36m(pid=35820)[0m 8.7 K     Trainable params
[2m[36m(pid=35820)[0m 0         Non-trainable params
[2m[36m(pid=35820)[0m 8.7 K     Total params
[2m[36m(pid=35826)[0m time to fit was 980.597410440445
[2m[36m(pid=35826)[0m GPU available: False, used: False
[2m[36m(pid=35826)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35826)[0m 
[2m[36m(pid=35826)[0m   | Name      | Type              | Params
[2m[36m(pid=35826)[0m ------------------------------------------------
[2m[36m(pid=35826)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35826)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35826)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35826)[0m ------------------------------------------------
[2m[36m(pid=35826)[0m 8.7 K     Trainable params
[2m[36m(pid=35826)[0m 0         Non-trainable params
[2m[36m(pid=35826)[0m 8.7 K     Total params
[2m[36m(pid=35855)[0m time to fit was 555.9994876384735
[2m[36m(pid=35855)[0m GPU available: False, used: False
[2m[36m(pid=35855)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35855)[0m 
[2m[36m(pid=35855)[0m   | Name      | Type              | Params
[2m[36m(pid=35855)[0m ------------------------------------------------
[2m[36m(pid=35855)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35855)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35855)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35855)[0m ------------------------------------------------
[2m[36m(pid=35855)[0m 8.7 K     Trainable params
[2m[36m(pid=35855)[0m 0         Non-trainable params
[2m[36m(pid=35855)[0m 8.7 K     Total params
[2m[36m(pid=35856)[0m time to fit was 332.7760362625122
Result for _inner_e8deb_00003:
  auc: 0.8973612785339355
  date: 2021-03-19_11-50-43
  done: false
  experiment_id: 84ce48f4529c42d2a2df1729ed464c88
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35856
  time_since_restore: 1730.8119959831238
  time_this_iter_s: 1730.8119959831238
  time_total_s: 1730.8119959831238
  timestamp: 1616151043
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00003
  
[2m[36m(pid=35856)[0m Finished run with seed 0 - lr 0.001 - sec_lr 0.001 - bs 256 - mean val auc: 0.8973612785339355
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 40/180 (1 PENDING, 26 RUNNING, 13 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |                     |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00003 | RUNNING    | 145.101.32.82:35856 |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |                     |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |                     |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00008 | RUNNING    |                     |          256 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |                     |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00011 | RUNNING    |                     |           64 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00039 | PENDING    |                     |          512 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |                     |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00017 | TERMINATED |                     |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
| _inner_e8deb_00018 | TERMINATED |                     |          256 |     0 | 1     |    0.001 |      1 |          725.295 | 0.913732 |
| _inner_e8deb_00019 | TERMINATED |                     |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
| _inner_e8deb_00022 | TERMINATED |                     |          128 |     0 | 2     |    0.001 |      1 |         1423.35  | 0.830453 |
| _inner_e8deb_00023 | TERMINATED |                     |          256 |     0 | 2     |    0.001 |      1 |          975.463 | 0.911412 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 20 more trials not shown (16 RUNNING, 3 TERMINATED)


Result for _inner_e8deb_00003:
  auc: 0.8973612785339355
  date: 2021-03-19_11-50-43
  done: true
  experiment_id: 84ce48f4529c42d2a2df1729ed464c88
  experiment_tag: 3_batch_size=256,eta=0.0,lr=0.001,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35856
  time_since_restore: 1730.8119959831238
  time_this_iter_s: 1730.8119959831238
  time_total_s: 1730.8119959831238
  timestamp: 1616151043
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00003
  
[2m[36m(pid=35881)[0m time to fit was 334.71438932418823
Result for _inner_e8deb_00008:
  auc: 0.9118127346038818
  date: 2021-03-19_11-50-51
  done: false
  experiment_id: 2546a81914ea466694ccf0af66aec8e5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35881
  time_since_restore: 1738.3210163116455
  time_this_iter_s: 1738.3210163116455
  time_total_s: 1738.3210163116455
  timestamp: 1616151051
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00008
  
[2m[36m(pid=35881)[0m Finished run with seed 0 - lr 0.01 - sec_lr 0.001 - bs 256 - mean val auc: 0.9118127346038818
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 41/180 (1 PENDING, 26 RUNNING, 14 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |                     |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |                     |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |                     |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00008 | RUNNING    | 145.101.32.82:35881 |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00010 | RUNNING    |                     |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00011 | RUNNING    |                     |           64 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    |                     |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00040 | PENDING    |                     |           32 |     0 | 0.1   |    0.01  |        |                  |          |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |                     |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00017 | TERMINATED |                     |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
| _inner_e8deb_00018 | TERMINATED |                     |          256 |     0 | 1     |    0.001 |      1 |          725.295 | 0.913732 |
| _inner_e8deb_00019 | TERMINATED |                     |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
| _inner_e8deb_00022 | TERMINATED |                     |          128 |     0 | 2     |    0.001 |      1 |         1423.35  | 0.830453 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 21 more trials not shown (16 RUNNING, 4 TERMINATED)


Result for _inner_e8deb_00008:
  auc: 0.9118127346038818
  date: 2021-03-19_11-50-51
  done: true
  experiment_id: 2546a81914ea466694ccf0af66aec8e5
  experiment_tag: 8_batch_size=256,eta=0.0,lr=0.01,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35881
  time_since_restore: 1738.3210163116455
  time_this_iter_s: 1738.3210163116455
  time_total_s: 1738.3210163116455
  timestamp: 1616151051
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00008
  
[2m[36m(pid=35780)[0m Starting run with seed 0 - lr 0.01 - sec_lr 0.01 - bs 512
[2m[36m(pid=35780)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35780)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35780)[0m GPU available: False, used: False
[2m[36m(pid=35780)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35780)[0m 
[2m[36m(pid=35780)[0m   | Name      | Type              | Params
[2m[36m(pid=35780)[0m ------------------------------------------------
[2m[36m(pid=35780)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35780)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35780)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35780)[0m ------------------------------------------------
[2m[36m(pid=35780)[0m 8.7 K     Trainable params
[2m[36m(pid=35780)[0m 0         Non-trainable params
[2m[36m(pid=35780)[0m 8.7 K     Total params
[2m[36m(pid=35780)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35780)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35780)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35780)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35780)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35780)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35787)[0m Starting run with seed 0 - lr 0.1 - sec_lr 0.01 - bs 32
[2m[36m(pid=35787)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35787)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35787)[0m GPU available: False, used: False
[2m[36m(pid=35787)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35787)[0m 
[2m[36m(pid=35787)[0m   | Name      | Type              | Params
[2m[36m(pid=35787)[0m ------------------------------------------------
[2m[36m(pid=35787)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35787)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35787)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35787)[0m ------------------------------------------------
[2m[36m(pid=35787)[0m 8.7 K     Trainable params
[2m[36m(pid=35787)[0m 0         Non-trainable params
[2m[36m(pid=35787)[0m 8.7 K     Total params
[2m[36m(pid=35787)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35787)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35787)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35787)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35787)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35787)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35792)[0m time to fit was 327.01815700531006
[2m[36m(pid=35792)[0m GPU available: False, used: False
[2m[36m(pid=35792)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35792)[0m 
[2m[36m(pid=35792)[0m   | Name      | Type              | Params
[2m[36m(pid=35792)[0m ------------------------------------------------
[2m[36m(pid=35792)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35792)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35792)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35792)[0m ------------------------------------------------
[2m[36m(pid=35792)[0m 8.7 K     Trainable params
[2m[36m(pid=35792)[0m 0         Non-trainable params
[2m[36m(pid=35792)[0m 8.7 K     Total params
[2m[36m(pid=35803)[0m time to fit was 635.1800787448883
[2m[36m(pid=35803)[0m GPU available: False, used: False
[2m[36m(pid=35803)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35803)[0m 
[2m[36m(pid=35803)[0m   | Name      | Type              | Params
[2m[36m(pid=35803)[0m ------------------------------------------------
[2m[36m(pid=35803)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35803)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35803)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35803)[0m ------------------------------------------------
[2m[36m(pid=35803)[0m 8.7 K     Trainable params
[2m[36m(pid=35803)[0m 0         Non-trainable params
[2m[36m(pid=35803)[0m 8.7 K     Total params
[2m[36m(pid=35795)[0m time to fit was 299.56375670433044
[2m[36m(pid=35795)[0m GPU available: False, used: False
[2m[36m(pid=35795)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35795)[0m 
[2m[36m(pid=35795)[0m   | Name      | Type              | Params
[2m[36m(pid=35795)[0m ------------------------------------------------
[2m[36m(pid=35795)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35795)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35795)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35795)[0m ------------------------------------------------
[2m[36m(pid=35795)[0m 8.7 K     Trainable params
[2m[36m(pid=35795)[0m 0         Non-trainable params
[2m[36m(pid=35795)[0m 8.7 K     Total params
[2m[36m(pid=35866)[0m time to fit was 476.04601526260376
[2m[36m(pid=35866)[0m GPU available: False, used: False
[2m[36m(pid=35866)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35866)[0m 
[2m[36m(pid=35866)[0m   | Name      | Type              | Params
[2m[36m(pid=35866)[0m ------------------------------------------------
[2m[36m(pid=35866)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35866)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35866)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35866)[0m ------------------------------------------------
[2m[36m(pid=35866)[0m 8.7 K     Trainable params
[2m[36m(pid=35866)[0m 0         Non-trainable params
[2m[36m(pid=35866)[0m 8.7 K     Total params
[2m[36m(pid=35795)[0m time to fit was 68.19991707801819
Result for _inner_e8deb_00028:
  auc: 0.7120776057243348
  date: 2021-03-19_11-52-37
  done: false
  experiment_id: 2ee1e034855d4fb9a83cb9f3ad656d63
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35795
  time_since_restore: 1109.743822813034
  time_this_iter_s: 1109.743822813034
  time_total_s: 1109.743822813034
  timestamp: 1616151157
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00028
  
[2m[36m(pid=35795)[0m Finished run with seed 0 - lr 5 - sec_lr 0.001 - bs 256 - mean val auc: 0.7120776057243348
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 42/180 (1 PENDING, 26 RUNNING, 15 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |       |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |       |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00011 | RUNNING    |       |           64 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    |       |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00016 | RUNNING    |       |           64 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00041 | PENDING    |       |           64 |     0 | 0.1   |    0.01  |        |                  |          |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00017 | TERMINATED |       |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
| _inner_e8deb_00018 | TERMINATED |       |          256 |     0 | 1     |    0.001 |      1 |          725.295 | 0.913732 |
| _inner_e8deb_00019 | TERMINATED |       |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 22 more trials not shown (16 RUNNING, 5 TERMINATED)


Result for _inner_e8deb_00028:
  auc: 0.7120776057243348
  date: 2021-03-19_11-52-37
  done: true
  experiment_id: 2ee1e034855d4fb9a83cb9f3ad656d63
  experiment_tag: 28_batch_size=256,eta=0.0,lr=5,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35795
  time_since_restore: 1109.743822813034
  time_this_iter_s: 1109.743822813034
  time_total_s: 1109.743822813034
  timestamp: 1616151157
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00028
  
[2m[36m(pid=35790)[0m Starting run with seed 0 - lr 0.1 - sec_lr 0.01 - bs 64
[2m[36m(pid=35790)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35790)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35790)[0m GPU available: False, used: False
[2m[36m(pid=35790)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35790)[0m 
[2m[36m(pid=35790)[0m   | Name      | Type              | Params
[2m[36m(pid=35790)[0m ------------------------------------------------
[2m[36m(pid=35790)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35790)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35790)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35790)[0m ------------------------------------------------
[2m[36m(pid=35790)[0m 8.7 K     Trainable params
[2m[36m(pid=35790)[0m 0         Non-trainable params
[2m[36m(pid=35790)[0m 8.7 K     Total params
[2m[36m(pid=35790)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35790)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35790)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35790)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35790)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35790)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35885)[0m time to fit was 315.42810225486755
Result for _inner_e8deb_00011:
  auc: 0.9137662768363952
  date: 2021-03-19_11-52-54
  done: false
  experiment_id: d74990d3bb7d4891b1975c4cf416fccd
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35885
  time_since_restore: 1861.5590660572052
  time_this_iter_s: 1861.5590660572052
  time_total_s: 1861.5590660572052
  timestamp: 1616151174
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00011
  
[2m[36m(pid=35885)[0m Finished run with seed 0 - lr 0.1 - sec_lr 0.001 - bs 64 - mean val auc: 0.9137662768363952
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 43/180 (1 PENDING, 26 RUNNING, 16 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |                     |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |                     |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |                     |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |                     |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00011 | RUNNING    | 145.101.32.82:35885 |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00015 | RUNNING    |                     |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00016 | RUNNING    |                     |           64 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00042 | PENDING    |                     |          128 |     0 | 0.1   |    0.01  |        |                  |          |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |                     |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00017 | TERMINATED |                     |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
| _inner_e8deb_00018 | TERMINATED |                     |          256 |     0 | 1     |    0.001 |      1 |          725.295 | 0.913732 |
| _inner_e8deb_00019 | TERMINATED |                     |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 23 more trials not shown (16 RUNNING, 6 TERMINATED)


Result for _inner_e8deb_00011:
  auc: 0.9137662768363952
  date: 2021-03-19_11-52-54
  done: true
  experiment_id: d74990d3bb7d4891b1975c4cf416fccd
  experiment_tag: 11_batch_size=64,eta=0.0,lr=0.1,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35885
  time_since_restore: 1861.5590660572052
  time_this_iter_s: 1861.5590660572052
  time_total_s: 1861.5590660572052
  timestamp: 1616151174
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00011
  
[2m[36m(pid=35893)[0m time to fit was 315.5086450576782
Result for _inner_e8deb_00016:
  auc: 0.913413155078888
  date: 2021-03-19_11-52-58
  done: false
  experiment_id: 02222cf252d348c3a9ae2ffbff9cfe5a
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35893
  time_since_restore: 1864.567682981491
  time_this_iter_s: 1864.567682981491
  time_total_s: 1864.567682981491
  timestamp: 1616151178
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00016
  
[2m[36m(pid=35893)[0m Finished run with seed 0 - lr 1 - sec_lr 0.001 - bs 64 - mean val auc: 0.913413155078888
Result for _inner_e8deb_00016:
  auc: 0.913413155078888
  date: 2021-03-19_11-52-58
  done: true
  experiment_id: 02222cf252d348c3a9ae2ffbff9cfe5a
  experiment_tag: 16_batch_size=64,eta=0.0,lr=1,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35893
  time_since_restore: 1864.567682981491
  time_this_iter_s: 1864.567682981491
  time_total_s: 1864.567682981491
  timestamp: 1616151178
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00016
  
[2m[36m(pid=35810)[0m time to fit was 328.80275774002075
[2m[36m(pid=35810)[0m GPU available: False, used: False
[2m[36m(pid=35810)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35810)[0m 
[2m[36m(pid=35810)[0m   | Name      | Type              | Params
[2m[36m(pid=35810)[0m ------------------------------------------------
[2m[36m(pid=35810)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35810)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35810)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35810)[0m ------------------------------------------------
[2m[36m(pid=35810)[0m 8.7 K     Trainable params
[2m[36m(pid=35810)[0m 0         Non-trainable params
[2m[36m(pid=35810)[0m 8.7 K     Total params
[2m[36m(pid=35781)[0m Starting run with seed 0 - lr 0.1 - sec_lr 0.01 - bs 128
[2m[36m(pid=35781)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35781)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35781)[0m GPU available: False, used: False
[2m[36m(pid=35781)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35781)[0m 
[2m[36m(pid=35781)[0m   | Name      | Type              | Params
[2m[36m(pid=35781)[0m ------------------------------------------------
[2m[36m(pid=35781)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35781)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35781)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35781)[0m ------------------------------------------------
[2m[36m(pid=35781)[0m 8.7 K     Trainable params
[2m[36m(pid=35781)[0m 0         Non-trainable params
[2m[36m(pid=35781)[0m 8.7 K     Total params
[2m[36m(pid=35781)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35781)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35781)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35781)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35781)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35781)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35779)[0m Starting run with seed 0 - lr 0.1 - sec_lr 0.01 - bs 256
[2m[36m(pid=35779)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35779)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35779)[0m GPU available: False, used: False
[2m[36m(pid=35779)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35779)[0m 
[2m[36m(pid=35779)[0m   | Name      | Type              | Params
[2m[36m(pid=35779)[0m ------------------------------------------------
[2m[36m(pid=35779)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35779)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35779)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35779)[0m ------------------------------------------------
[2m[36m(pid=35779)[0m 8.7 K     Trainable params
[2m[36m(pid=35779)[0m 0         Non-trainable params
[2m[36m(pid=35779)[0m 8.7 K     Total params
[2m[36m(pid=35779)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35779)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35779)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35779)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35779)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35779)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35791)[0m time to fit was 225.40053296089172
[2m[36m(pid=35791)[0m GPU available: False, used: False
[2m[36m(pid=35791)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35791)[0m 
[2m[36m(pid=35791)[0m   | Name      | Type              | Params
[2m[36m(pid=35791)[0m ------------------------------------------------
[2m[36m(pid=35791)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35791)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35791)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35791)[0m ------------------------------------------------
[2m[36m(pid=35791)[0m 8.7 K     Trainable params
[2m[36m(pid=35791)[0m 0         Non-trainable params
[2m[36m(pid=35791)[0m 8.7 K     Total params
[2m[36m(pid=35806)[0m time to fit was 586.7846460342407
[2m[36m(pid=35806)[0m GPU available: False, used: False
[2m[36m(pid=35806)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35806)[0m 
[2m[36m(pid=35806)[0m   | Name      | Type              | Params
[2m[36m(pid=35806)[0m ------------------------------------------------
[2m[36m(pid=35806)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35806)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35806)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35806)[0m ------------------------------------------------
[2m[36m(pid=35806)[0m 8.7 K     Trainable params
[2m[36m(pid=35806)[0m 0         Non-trainable params
[2m[36m(pid=35806)[0m 8.7 K     Total params
[2m[36m(pid=35861)[0m time to fit was 1915.8731379508972
[2m[36m(pid=35861)[0m GPU available: False, used: False
[2m[36m(pid=35861)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35861)[0m 
[2m[36m(pid=35861)[0m   | Name      | Type              | Params
[2m[36m(pid=35861)[0m ------------------------------------------------
[2m[36m(pid=35861)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35861)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35861)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35861)[0m ------------------------------------------------
[2m[36m(pid=35861)[0m 8.7 K     Trainable params
[2m[36m(pid=35861)[0m 0         Non-trainable params
[2m[36m(pid=35861)[0m 8.7 K     Total params
[2m[36m(pid=35826)[0m time to fit was 229.58200669288635
[2m[36m(pid=35826)[0m GPU available: False, used: False
[2m[36m(pid=35826)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35826)[0m 
[2m[36m(pid=35826)[0m   | Name      | Type              | Params
[2m[36m(pid=35826)[0m ------------------------------------------------
[2m[36m(pid=35826)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35826)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35826)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35826)[0m ------------------------------------------------
[2m[36m(pid=35826)[0m 8.7 K     Trainable params
[2m[36m(pid=35826)[0m 0         Non-trainable params
[2m[36m(pid=35826)[0m 8.7 K     Total params
[2m[36m(pid=35852)[0m time to fit was 677.4965348243713
[2m[36m(pid=35852)[0m GPU available: False, used: False
[2m[36m(pid=35852)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35852)[0m 
[2m[36m(pid=35852)[0m   | Name      | Type              | Params
[2m[36m(pid=35852)[0m ------------------------------------------------
[2m[36m(pid=35852)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35852)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35852)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35852)[0m ------------------------------------------------
[2m[36m(pid=35852)[0m 8.7 K     Trainable params
[2m[36m(pid=35852)[0m 0         Non-trainable params
[2m[36m(pid=35852)[0m 8.7 K     Total params
[2m[36m(pid=35780)[0m time to fit was 226.42917776107788
[2m[36m(pid=35780)[0m GPU available: False, used: False
[2m[36m(pid=35780)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35780)[0m 
[2m[36m(pid=35780)[0m   | Name      | Type              | Params
[2m[36m(pid=35780)[0m ------------------------------------------------
[2m[36m(pid=35780)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35780)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35780)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35780)[0m ------------------------------------------------
[2m[36m(pid=35780)[0m 8.7 K     Trainable params
[2m[36m(pid=35780)[0m 0         Non-trainable params
[2m[36m(pid=35780)[0m 8.7 K     Total params
[2m[36m(pid=35889)[0m time to fit was 719.1430115699768
[2m[36m(pid=35889)[0m GPU available: False, used: False
[2m[36m(pid=35889)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35889)[0m 
[2m[36m(pid=35889)[0m   | Name      | Type              | Params
[2m[36m(pid=35889)[0m ------------------------------------------------
[2m[36m(pid=35889)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35889)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35889)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35889)[0m ------------------------------------------------
[2m[36m(pid=35889)[0m 8.7 K     Trainable params
[2m[36m(pid=35889)[0m 0         Non-trainable params
[2m[36m(pid=35889)[0m 8.7 K     Total params
[2m[36m(pid=35818)[0m time to fit was 996.2165882587433
[2m[36m(pid=35818)[0m GPU available: False, used: False
[2m[36m(pid=35818)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35818)[0m 
[2m[36m(pid=35818)[0m   | Name      | Type              | Params
[2m[36m(pid=35818)[0m ------------------------------------------------
[2m[36m(pid=35818)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35818)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35818)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35818)[0m ------------------------------------------------
[2m[36m(pid=35818)[0m 8.7 K     Trainable params
[2m[36m(pid=35818)[0m 0         Non-trainable params
[2m[36m(pid=35818)[0m 8.7 K     Total params
[2m[36m(pid=35779)[0m time to fit was 125.72306251525879
[2m[36m(pid=35779)[0m GPU available: False, used: False
[2m[36m(pid=35779)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35779)[0m 
[2m[36m(pid=35779)[0m   | Name      | Type              | Params
[2m[36m(pid=35779)[0m ------------------------------------------------
[2m[36m(pid=35779)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35779)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35779)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35779)[0m ------------------------------------------------
[2m[36m(pid=35779)[0m 8.7 K     Trainable params
[2m[36m(pid=35779)[0m 0         Non-trainable params
[2m[36m(pid=35779)[0m 8.7 K     Total params
[2m[36m(pid=35802)[0m time to fit was 781.6836664676666
[2m[36m(pid=35802)[0m GPU available: False, used: False
[2m[36m(pid=35802)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35802)[0m 
[2m[36m(pid=35802)[0m   | Name      | Type              | Params
[2m[36m(pid=35802)[0m ------------------------------------------------
[2m[36m(pid=35802)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35802)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35802)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35802)[0m ------------------------------------------------
[2m[36m(pid=35802)[0m 8.7 K     Trainable params
[2m[36m(pid=35802)[0m 0         Non-trainable params
[2m[36m(pid=35802)[0m 8.7 K     Total params
[2m[36m(pid=35781)[0m time to fit was 139.71203780174255
[2m[36m(pid=35781)[0m GPU available: False, used: False
[2m[36m(pid=35781)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35781)[0m 
[2m[36m(pid=35781)[0m   | Name      | Type              | Params
[2m[36m(pid=35781)[0m ------------------------------------------------
[2m[36m(pid=35781)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35781)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35781)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35781)[0m ------------------------------------------------
[2m[36m(pid=35781)[0m 8.7 K     Trainable params
[2m[36m(pid=35781)[0m 0         Non-trainable params
[2m[36m(pid=35781)[0m 8.7 K     Total params
[2m[36m(pid=35857)[0m time to fit was 983.2445120811462
[2m[36m(pid=35857)[0m GPU available: False, used: False
[2m[36m(pid=35857)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35857)[0m 
[2m[36m(pid=35857)[0m   | Name      | Type              | Params
[2m[36m(pid=35857)[0m ------------------------------------------------
[2m[36m(pid=35857)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35857)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35857)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35857)[0m ------------------------------------------------
[2m[36m(pid=35857)[0m 8.7 K     Trainable params
[2m[36m(pid=35857)[0m 0         Non-trainable params
[2m[36m(pid=35857)[0m 8.7 K     Total params
[2m[36m(pid=35866)[0m time to fit was 273.41307520866394
Result for _inner_e8deb_00021:
  auc: 0.829033613204956
  date: 2021-03-19_11-56-17
  done: false
  experiment_id: 85d13f9ceecf4f13810d2011cee0cb58
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35866
  time_since_restore: 2063.585446834564
  time_this_iter_s: 2063.585446834564
  time_total_s: 2063.585446834564
  timestamp: 1616151377
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00021
  
[2m[36m(pid=35866)[0m Finished run with seed 0 - lr 2 - sec_lr 0.001 - bs 64 - mean val auc: 0.829033613204956
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 45/180 (1 PENDING, 26 RUNNING, 18 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |                     |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |                     |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |                     |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |                     |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    |                     |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00020 | RUNNING    |                     |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00021 | RUNNING    | 145.101.32.82:35866 |           64 |     0 | 2     |    0.001 |      1 |         2063.59  | 0.829034 |
| _inner_e8deb_00044 | PENDING    |                     |          512 |     0 | 0.1   |    0.01  |        |                  |          |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |                     |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00016 | TERMINATED |                     |           64 |     0 | 1     |    0.001 |      1 |         1864.57  | 0.913413 |
| _inner_e8deb_00017 | TERMINATED |                     |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 25 more trials not shown (16 RUNNING, 8 TERMINATED)


Result for _inner_e8deb_00021:
  auc: 0.829033613204956
  date: 2021-03-19_11-56-17
  done: true
  experiment_id: 85d13f9ceecf4f13810d2011cee0cb58
  experiment_tag: 21_batch_size=64,eta=0.0,lr=2,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35866
  time_since_restore: 2063.585446834564
  time_this_iter_s: 2063.585446834564
  time_total_s: 2063.585446834564
  timestamp: 1616151377
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00021
  
[2m[36m(pid=35778)[0m Starting run with seed 0 - lr 0.1 - sec_lr 0.01 - bs 512
[2m[36m(pid=35778)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35778)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35778)[0m GPU available: False, used: False
[2m[36m(pid=35778)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35778)[0m 
[2m[36m(pid=35778)[0m   | Name      | Type              | Params
[2m[36m(pid=35778)[0m ------------------------------------------------
[2m[36m(pid=35778)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35778)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35778)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35778)[0m ------------------------------------------------
[2m[36m(pid=35778)[0m 8.7 K     Trainable params
[2m[36m(pid=35778)[0m 0         Non-trainable params
[2m[36m(pid=35778)[0m 8.7 K     Total params
[2m[36m(pid=35778)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35778)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35778)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35778)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35778)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35778)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35792)[0m time to fit was 327.6981997489929
[2m[36m(pid=35792)[0m GPU available: False, used: False
[2m[36m(pid=35792)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35792)[0m 
[2m[36m(pid=35792)[0m   | Name      | Type              | Params
[2m[36m(pid=35792)[0m ------------------------------------------------
[2m[36m(pid=35792)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35792)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35792)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35792)[0m ------------------------------------------------
[2m[36m(pid=35792)[0m 8.7 K     Trainable params
[2m[36m(pid=35792)[0m 0         Non-trainable params
[2m[36m(pid=35792)[0m 8.7 K     Total params
[2m[36m(pid=35779)[0m time to fit was 115.18206524848938
[2m[36m(pid=35779)[0m GPU available: False, used: False
[2m[36m(pid=35779)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35779)[0m 
[2m[36m(pid=35779)[0m   | Name      | Type              | Params
[2m[36m(pid=35779)[0m ------------------------------------------------
[2m[36m(pid=35779)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35779)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35779)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35779)[0m ------------------------------------------------
[2m[36m(pid=35779)[0m 8.7 K     Trainable params
[2m[36m(pid=35779)[0m 0         Non-trainable params
[2m[36m(pid=35779)[0m 8.7 K     Total params
[2m[36m(pid=35871)[0m time to fit was 550.191068649292
[2m[36m(pid=35871)[0m GPU available: False, used: False
[2m[36m(pid=35871)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35871)[0m 
[2m[36m(pid=35871)[0m   | Name      | Type              | Params
[2m[36m(pid=35871)[0m ------------------------------------------------
[2m[36m(pid=35871)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35871)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35871)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35871)[0m ------------------------------------------------
[2m[36m(pid=35871)[0m 8.7 K     Trainable params
[2m[36m(pid=35871)[0m 0         Non-trainable params
[2m[36m(pid=35871)[0m 8.7 K     Total params
[2m[36m(pid=35791)[0m time to fit was 224.30049967765808
[2m[36m(pid=35791)[0m GPU available: False, used: False
[2m[36m(pid=35791)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35791)[0m 
[2m[36m(pid=35791)[0m   | Name      | Type              | Params
[2m[36m(pid=35791)[0m ------------------------------------------------
[2m[36m(pid=35791)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35791)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35791)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35791)[0m ------------------------------------------------
[2m[36m(pid=35791)[0m 8.7 K     Trainable params
[2m[36m(pid=35791)[0m 0         Non-trainable params
[2m[36m(pid=35791)[0m 8.7 K     Total params
[2m[36m(pid=35790)[0m time to fit was 271.4230787754059
[2m[36m(pid=35790)[0m GPU available: False, used: False
[2m[36m(pid=35790)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35790)[0m 
[2m[36m(pid=35790)[0m   | Name      | Type              | Params
[2m[36m(pid=35790)[0m ------------------------------------------------
[2m[36m(pid=35790)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35790)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35790)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35790)[0m ------------------------------------------------
[2m[36m(pid=35790)[0m 8.7 K     Trainable params
[2m[36m(pid=35790)[0m 0         Non-trainable params
[2m[36m(pid=35790)[0m 8.7 K     Total params
[2m[36m(pid=35778)[0m time to fit was 84.13314199447632
[2m[36m(pid=35778)[0m GPU available: False, used: False
[2m[36m(pid=35778)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35778)[0m 
[2m[36m(pid=35778)[0m   | Name      | Type              | Params
[2m[36m(pid=35778)[0m ------------------------------------------------
[2m[36m(pid=35778)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35778)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35778)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35778)[0m ------------------------------------------------
[2m[36m(pid=35778)[0m 8.7 K     Trainable params
[2m[36m(pid=35778)[0m 0         Non-trainable params
[2m[36m(pid=35778)[0m 8.7 K     Total params
[2m[36m(pid=35780)[0m time to fit was 226.72992157936096
[2m[36m(pid=35780)[0m GPU available: False, used: False
[2m[36m(pid=35780)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35780)[0m 
[2m[36m(pid=35780)[0m   | Name      | Type              | Params
[2m[36m(pid=35780)[0m ------------------------------------------------
[2m[36m(pid=35780)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35780)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35780)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35780)[0m ------------------------------------------------
[2m[36m(pid=35780)[0m 8.7 K     Trainable params
[2m[36m(pid=35780)[0m 0         Non-trainable params
[2m[36m(pid=35780)[0m 8.7 K     Total params
[2m[36m(pid=35810)[0m time to fit was 331.2719793319702
[2m[36m(pid=35810)[0m GPU available: False, used: False
[2m[36m(pid=35810)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35810)[0m 
[2m[36m(pid=35810)[0m   | Name      | Type              | Params
[2m[36m(pid=35810)[0m ------------------------------------------------
[2m[36m(pid=35810)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35810)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35810)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35810)[0m ------------------------------------------------
[2m[36m(pid=35810)[0m 8.7 K     Trainable params
[2m[36m(pid=35810)[0m 0         Non-trainable params
[2m[36m(pid=35810)[0m 8.7 K     Total params
[2m[36m(pid=35820)[0m time to fit was 520.784675359726
[2m[36m(pid=35820)[0m GPU available: False, used: False
[2m[36m(pid=35820)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35820)[0m 
[2m[36m(pid=35820)[0m   | Name      | Type              | Params
[2m[36m(pid=35820)[0m ------------------------------------------------
[2m[36m(pid=35820)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35820)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35820)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35820)[0m ------------------------------------------------
[2m[36m(pid=35820)[0m 8.7 K     Trainable params
[2m[36m(pid=35820)[0m 0         Non-trainable params
[2m[36m(pid=35820)[0m 8.7 K     Total params
[2m[36m(pid=35878)[0m time to fit was 848.8204669952393
[2m[36m(pid=35878)[0m GPU available: False, used: False
[2m[36m(pid=35878)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35878)[0m 
[2m[36m(pid=35878)[0m   | Name      | Type              | Params
[2m[36m(pid=35878)[0m ------------------------------------------------
[2m[36m(pid=35878)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35878)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35878)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35878)[0m ------------------------------------------------
[2m[36m(pid=35878)[0m 8.7 K     Trainable params
[2m[36m(pid=35878)[0m 0         Non-trainable params
[2m[36m(pid=35878)[0m 8.7 K     Total params
[2m[36m(pid=35823)[0m time to fit was 549.0511155128479
[2m[36m(pid=35823)[0m GPU available: False, used: False
[2m[36m(pid=35823)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35823)[0m 
[2m[36m(pid=35823)[0m   | Name      | Type              | Params
[2m[36m(pid=35823)[0m ------------------------------------------------
[2m[36m(pid=35823)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35823)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35823)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35823)[0m ------------------------------------------------
[2m[36m(pid=35823)[0m 8.7 K     Trainable params
[2m[36m(pid=35823)[0m 0         Non-trainable params
[2m[36m(pid=35823)[0m 8.7 K     Total params
[2m[36m(pid=35787)[0m time to fit was 511.7319333553314
[2m[36m(pid=35787)[0m GPU available: False, used: False
[2m[36m(pid=35787)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35787)[0m 
[2m[36m(pid=35787)[0m   | Name      | Type              | Params
[2m[36m(pid=35787)[0m ------------------------------------------------
[2m[36m(pid=35787)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35787)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35787)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35787)[0m ------------------------------------------------
[2m[36m(pid=35787)[0m 8.7 K     Trainable params
[2m[36m(pid=35787)[0m 0         Non-trainable params
[2m[36m(pid=35787)[0m 8.7 K     Total params
[2m[36m(pid=35826)[0m time to fit was 328.6906626224518
[2m[36m(pid=35826)[0m GPU available: False, used: False
[2m[36m(pid=35826)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35826)[0m 
[2m[36m(pid=35826)[0m   | Name      | Type              | Params
[2m[36m(pid=35826)[0m ------------------------------------------------
[2m[36m(pid=35826)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35826)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35826)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35826)[0m ------------------------------------------------
[2m[36m(pid=35826)[0m 8.7 K     Trainable params
[2m[36m(pid=35826)[0m 0         Non-trainable params
[2m[36m(pid=35826)[0m 8.7 K     Total params
[2m[36m(pid=35855)[0m time to fit was 552.5825266838074
[2m[36m(pid=35855)[0m GPU available: False, used: False
[2m[36m(pid=35855)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35855)[0m 
[2m[36m(pid=35855)[0m   | Name      | Type              | Params
[2m[36m(pid=35855)[0m ------------------------------------------------
[2m[36m(pid=35855)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35855)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35855)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35855)[0m ------------------------------------------------
[2m[36m(pid=35855)[0m 8.7 K     Trainable params
[2m[36m(pid=35855)[0m 0         Non-trainable params
[2m[36m(pid=35855)[0m 8.7 K     Total params
[2m[36m(pid=35781)[0m time to fit was 283.6048798561096
[2m[36m(pid=35781)[0m GPU available: False, used: False
[2m[36m(pid=35781)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35781)[0m 
[2m[36m(pid=35781)[0m   | Name      | Type              | Params
[2m[36m(pid=35781)[0m ------------------------------------------------
[2m[36m(pid=35781)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35781)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35781)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35781)[0m ------------------------------------------------
[2m[36m(pid=35781)[0m 8.7 K     Trainable params
[2m[36m(pid=35781)[0m 0         Non-trainable params
[2m[36m(pid=35781)[0m 8.7 K     Total params
[2m[36m(pid=35779)[0m time to fit was 179.7411184310913
[2m[36m(pid=35779)[0m GPU available: False, used: False
[2m[36m(pid=35779)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35779)[0m 
[2m[36m(pid=35779)[0m   | Name      | Type              | Params
[2m[36m(pid=35779)[0m ------------------------------------------------
[2m[36m(pid=35779)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35779)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35779)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35779)[0m ------------------------------------------------
[2m[36m(pid=35779)[0m 8.7 K     Trainable params
[2m[36m(pid=35779)[0m 0         Non-trainable params
[2m[36m(pid=35779)[0m 8.7 K     Total params
[2m[36m(pid=35778)[0m time to fit was 173.14010667800903
[2m[36m(pid=35778)[0m GPU available: False, used: False
[2m[36m(pid=35778)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35778)[0m 
[2m[36m(pid=35778)[0m   | Name      | Type              | Params
[2m[36m(pid=35778)[0m ------------------------------------------------
[2m[36m(pid=35778)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35778)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35778)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35778)[0m ------------------------------------------------
[2m[36m(pid=35778)[0m 8.7 K     Trainable params
[2m[36m(pid=35778)[0m 0         Non-trainable params
[2m[36m(pid=35778)[0m 8.7 K     Total params
[2m[36m(pid=35791)[0m time to fit was 230.98222017288208
[2m[36m(pid=35791)[0m Finished run with seed 0 - lr 0.001 - sec_lr 0.01 - bs 512 - mean val auc: 0.8901316523551941
Result for _inner_e8deb_00034:
  auc: 0.8901316523551941
  date: 2021-03-19_12-01-04
  done: false
  experiment_id: c3f03e0a0da349fd93e1cf534aec7d2e
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35791
  time_since_restore: 1136.1905307769775
  time_this_iter_s: 1136.1905307769775
  time_total_s: 1136.1905307769775
  timestamp: 1616151664
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00034
  
== Status ==
Memory usage on this node: 9.6/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 46/180 (1 PENDING, 26 RUNNING, 19 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |       |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |       |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    |       |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00020 | RUNNING    |       |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00025 | RUNNING    |       |           32 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00045 | PENDING    |       |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00016 | TERMINATED |       |           64 |     0 | 1     |    0.001 |      1 |         1864.57  | 0.913413 |
| _inner_e8deb_00017 | TERMINATED |       |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 26 more trials not shown (16 RUNNING, 9 TERMINATED)


Result for _inner_e8deb_00034:
  auc: 0.8901316523551941
  date: 2021-03-19_12-01-04
  done: true
  experiment_id: c3f03e0a0da349fd93e1cf534aec7d2e
  experiment_tag: 34_batch_size=512,eta=0.0,lr=0.001,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35791
  time_since_restore: 1136.1905307769775
  time_this_iter_s: 1136.1905307769775
  time_total_s: 1136.1905307769775
  timestamp: 1616151664
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00034
  
[2m[36m(pid=35786)[0m Starting run with seed 0 - lr 1 - sec_lr 0.01 - bs 32
[2m[36m(pid=35786)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35786)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35786)[0m GPU available: False, used: False
[2m[36m(pid=35786)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35786)[0m 
[2m[36m(pid=35786)[0m   | Name      | Type              | Params
[2m[36m(pid=35786)[0m ------------------------------------------------
[2m[36m(pid=35786)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35786)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35786)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35786)[0m ------------------------------------------------
[2m[36m(pid=35786)[0m 8.7 K     Trainable params
[2m[36m(pid=35786)[0m 0         Non-trainable params
[2m[36m(pid=35786)[0m 8.7 K     Total params
[2m[36m(pid=35786)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35786)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35786)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35786)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35786)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35786)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35779)[0m time to fit was 113.86993956565857
[2m[36m(pid=35779)[0m GPU available: False, used: False
[2m[36m(pid=35779)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35779)[0m 
[2m[36m(pid=35779)[0m   | Name      | Type              | Params
[2m[36m(pid=35779)[0m ------------------------------------------------
[2m[36m(pid=35779)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35779)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35779)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35779)[0m ------------------------------------------------
[2m[36m(pid=35779)[0m 8.7 K     Trainable params
[2m[36m(pid=35779)[0m 0         Non-trainable params
[2m[36m(pid=35779)[0m 8.7 K     Total params
[2m[36m(pid=35780)[0m time to fit was 225.83625149726868
[2m[36m(pid=35780)[0m GPU available: False, used: False
[2m[36m(pid=35780)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35780)[0m 
[2m[36m(pid=35780)[0m   | Name      | Type              | Params
[2m[36m(pid=35780)[0m ------------------------------------------------
[2m[36m(pid=35780)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35780)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35780)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35780)[0m ------------------------------------------------
[2m[36m(pid=35780)[0m 8.7 K     Trainable params
[2m[36m(pid=35780)[0m 0         Non-trainable params
[2m[36m(pid=35780)[0m 8.7 K     Total params
[2m[36m(pid=35792)[0m time to fit was 335.8830394744873
[2m[36m(pid=35792)[0m GPU available: False, used: False
[2m[36m(pid=35792)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35792)[0m 
[2m[36m(pid=35792)[0m   | Name      | Type              | Params
[2m[36m(pid=35792)[0m ------------------------------------------------
[2m[36m(pid=35792)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35792)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35792)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35792)[0m ------------------------------------------------
[2m[36m(pid=35792)[0m 8.7 K     Trainable params
[2m[36m(pid=35792)[0m 0         Non-trainable params
[2m[36m(pid=35792)[0m 8.7 K     Total params
[2m[36m(pid=35778)[0m time to fit was 98.67091989517212
[2m[36m(pid=35778)[0m GPU available: False, used: False
[2m[36m(pid=35778)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35778)[0m 
[2m[36m(pid=35778)[0m   | Name      | Type              | Params
[2m[36m(pid=35778)[0m ------------------------------------------------
[2m[36m(pid=35778)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35778)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35778)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35778)[0m ------------------------------------------------
[2m[36m(pid=35778)[0m 8.7 K     Trainable params
[2m[36m(pid=35778)[0m 0         Non-trainable params
[2m[36m(pid=35778)[0m 8.7 K     Total params
[2m[36m(pid=35781)[0m time to fit was 182.04954028129578
[2m[36m(pid=35781)[0m GPU available: False, used: False
[2m[36m(pid=35781)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35781)[0m 
[2m[36m(pid=35781)[0m   | Name      | Type              | Params
[2m[36m(pid=35781)[0m ------------------------------------------------
[2m[36m(pid=35781)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35781)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35781)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35781)[0m ------------------------------------------------
[2m[36m(pid=35781)[0m 8.7 K     Trainable params
[2m[36m(pid=35781)[0m 0         Non-trainable params
[2m[36m(pid=35781)[0m 8.7 K     Total params
[2m[36m(pid=35826)[0m time to fit was 209.63197088241577
Result for _inner_e8deb_00026:
  auc: 0.6429367661476135
  date: 2021-03-19_12-03-15
  done: false
  experiment_id: 00c5108bcc5c4ce581ad6a7bd4c77052
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35826
  time_since_restore: 1976.912023305893
  time_this_iter_s: 1976.912023305893
  time_total_s: 1976.912023305893
  timestamp: 1616151795
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00026
  
[2m[36m(pid=35826)[0m Finished run with seed 0 - lr 5 - sec_lr 0.001 - bs 64 - mean val auc: 0.6429367661476135
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 47/180 (1 PENDING, 26 RUNNING, 20 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |       |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |       |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    |       |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00020 | RUNNING    |       |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00025 | RUNNING    |       |           32 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00046 | PENDING    |       |           64 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00016 | TERMINATED |       |           64 |     0 | 1     |    0.001 |      1 |         1864.57  | 0.913413 |
| _inner_e8deb_00017 | TERMINATED |       |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 27 more trials not shown (16 RUNNING, 10 TERMINATED)


Result for _inner_e8deb_00026:
  auc: 0.6429367661476135
  date: 2021-03-19_12-03-15
  done: true
  experiment_id: 00c5108bcc5c4ce581ad6a7bd4c77052
  experiment_tag: 26_batch_size=64,eta=0.0,lr=5,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35826
  time_since_restore: 1976.912023305893
  time_this_iter_s: 1976.912023305893
  time_total_s: 1976.912023305893
  timestamp: 1616151795
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00026
  
[2m[36m(pid=35785)[0m Starting run with seed 0 - lr 1 - sec_lr 0.01 - bs 64
[2m[36m(pid=35785)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35785)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35785)[0m GPU available: False, used: False
[2m[36m(pid=35785)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35785)[0m 
[2m[36m(pid=35785)[0m   | Name      | Type              | Params
[2m[36m(pid=35785)[0m ------------------------------------------------
[2m[36m(pid=35785)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35785)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35785)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35785)[0m ------------------------------------------------
[2m[36m(pid=35785)[0m 8.7 K     Trainable params
[2m[36m(pid=35785)[0m 0         Non-trainable params
[2m[36m(pid=35785)[0m 8.7 K     Total params
[2m[36m(pid=35785)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35785)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35785)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35785)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35785)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35785)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35790)[0m time to fit was 391.622545003891
[2m[36m(pid=35790)[0m GPU available: False, used: False
[2m[36m(pid=35790)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35790)[0m 
[2m[36m(pid=35790)[0m   | Name      | Type              | Params
[2m[36m(pid=35790)[0m ------------------------------------------------
[2m[36m(pid=35790)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35790)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35790)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35790)[0m ------------------------------------------------
[2m[36m(pid=35790)[0m 8.7 K     Trainable params
[2m[36m(pid=35790)[0m 0         Non-trainable params
[2m[36m(pid=35790)[0m 8.7 K     Total params
[2m[36m(pid=35779)[0m time to fit was 119.49482035636902
[2m[36m(pid=35779)[0m Finished run with seed 0 - lr 0.1 - sec_lr 0.01 - bs 256 - mean val auc: 0.9129639506340027
Result for _inner_e8deb_00043:
  auc: 0.9129639506340027
  date: 2021-03-19_12-04-01
  done: false
  experiment_id: c52462dae8e84564abeb253d8bc7effa
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35779
  time_since_restore: 655.3865165710449
  time_this_iter_s: 655.3865165710449
  time_total_s: 655.3865165710449
  timestamp: 1616151841
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00043
  
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 48/180 (1 PENDING, 26 RUNNING, 21 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |       |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |       |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    |       |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00020 | RUNNING    |       |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00025 | RUNNING    |       |           32 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00047 | PENDING    |       |          128 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00016 | TERMINATED |       |           64 |     0 | 1     |    0.001 |      1 |         1864.57  | 0.913413 |
| _inner_e8deb_00017 | TERMINATED |       |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 28 more trials not shown (16 RUNNING, 11 TERMINATED)


Result for _inner_e8deb_00043:
  auc: 0.9129639506340027
  date: 2021-03-19_12-04-01
  done: true
  experiment_id: c52462dae8e84564abeb253d8bc7effa
  experiment_tag: 43_batch_size=256,eta=0.0,lr=0.1,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35779
  time_since_restore: 655.3865165710449
  time_this_iter_s: 655.3865165710449
  time_total_s: 655.3865165710449
  timestamp: 1616151841
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00043
  
[2m[36m(pid=35810)[0m time to fit was 332.46833395957947
[2m[36m(pid=35810)[0m GPU available: False, used: False
[2m[36m(pid=35810)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35810)[0m 
[2m[36m(pid=35810)[0m   | Name      | Type              | Params
[2m[36m(pid=35810)[0m ------------------------------------------------
[2m[36m(pid=35810)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35810)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35810)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35810)[0m ------------------------------------------------
[2m[36m(pid=35810)[0m 8.7 K     Trainable params
[2m[36m(pid=35810)[0m 0         Non-trainable params
[2m[36m(pid=35810)[0m 8.7 K     Total params
[2m[36m(pid=35777)[0m Starting run with seed 0 - lr 1 - sec_lr 0.01 - bs 128
[2m[36m(pid=35777)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35777)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35777)[0m GPU available: False, used: False
[2m[36m(pid=35777)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35777)[0m 
[2m[36m(pid=35777)[0m   | Name      | Type              | Params
[2m[36m(pid=35777)[0m ------------------------------------------------
[2m[36m(pid=35777)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35777)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35777)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35777)[0m ------------------------------------------------
[2m[36m(pid=35777)[0m 8.7 K     Trainable params
[2m[36m(pid=35777)[0m 0         Non-trainable params
[2m[36m(pid=35777)[0m 8.7 K     Total params
[2m[36m(pid=35777)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35777)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35777)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35777)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35777)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35777)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35778)[0m time to fit was 110.99343800544739
[2m[36m(pid=35778)[0m GPU available: False, used: False
[2m[36m(pid=35778)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35778)[0m 
[2m[36m(pid=35778)[0m   | Name      | Type              | Params
[2m[36m(pid=35778)[0m ------------------------------------------------
[2m[36m(pid=35778)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35778)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35778)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35778)[0m ------------------------------------------------
[2m[36m(pid=35778)[0m 8.7 K     Trainable params
[2m[36m(pid=35778)[0m 0         Non-trainable params
[2m[36m(pid=35778)[0m 8.7 K     Total params
[2m[36m(pid=35803)[0m time to fit was 783.8613073825836
[2m[36m(pid=35803)[0m GPU available: False, used: False
[2m[36m(pid=35803)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35803)[0m 
[2m[36m(pid=35803)[0m   | Name      | Type              | Params
[2m[36m(pid=35803)[0m ------------------------------------------------
[2m[36m(pid=35803)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35803)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35803)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35803)[0m ------------------------------------------------
[2m[36m(pid=35803)[0m 8.7 K     Trainable params
[2m[36m(pid=35803)[0m 0         Non-trainable params
[2m[36m(pid=35803)[0m 8.7 K     Total params
[2m[36m(pid=35806)[0m time to fit was 664.3838360309601
[2m[36m(pid=35806)[0m GPU available: False, used: False
[2m[36m(pid=35806)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35806)[0m 
[2m[36m(pid=35806)[0m   | Name      | Type              | Params
[2m[36m(pid=35806)[0m ------------------------------------------------
[2m[36m(pid=35806)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35806)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35806)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35806)[0m ------------------------------------------------
[2m[36m(pid=35806)[0m 8.7 K     Trainable params
[2m[36m(pid=35806)[0m 0         Non-trainable params
[2m[36m(pid=35806)[0m 8.7 K     Total params
[2m[36m(pid=35889)[0m time to fit was 597.3672044277191
[2m[36m(pid=35889)[0m GPU available: False, used: False
[2m[36m(pid=35889)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35889)[0m 
[2m[36m(pid=35889)[0m   | Name      | Type              | Params
[2m[36m(pid=35889)[0m ------------------------------------------------
[2m[36m(pid=35889)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35889)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35889)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35889)[0m ------------------------------------------------
[2m[36m(pid=35889)[0m 8.7 K     Trainable params
[2m[36m(pid=35889)[0m 0         Non-trainable params
[2m[36m(pid=35889)[0m 8.7 K     Total params
[2m[36m(pid=35778)[0m time to fit was 90.56814622879028
Result for _inner_e8deb_00044:
  auc: 0.913295567035675
  date: 2021-03-19_12-05-44
  done: false
  experiment_id: 9d5b3610c39c41e584d222a72a0062c6
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35778
  time_since_restore: 558.8925034999847
  time_this_iter_s: 558.8925034999847
  time_total_s: 558.8925034999847
  timestamp: 1616151944
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00044
  
[2m[36m(pid=35778)[0m Finished run with seed 0 - lr 0.1 - sec_lr 0.01 - bs 512 - mean val auc: 0.913295567035675
== Status ==
Memory usage on this node: 9.7/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 49/180 (1 PENDING, 26 RUNNING, 22 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |       |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    |       |          128 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    |       |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00020 | RUNNING    |       |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00025 | RUNNING    |       |           32 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00048 | PENDING    |       |          256 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00016 | TERMINATED |       |           64 |     0 | 1     |    0.001 |      1 |         1864.57  | 0.913413 |
| _inner_e8deb_00017 | TERMINATED |       |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 29 more trials not shown (16 RUNNING, 12 TERMINATED)


Result for _inner_e8deb_00044:
  auc: 0.913295567035675
  date: 2021-03-19_12-05-44
  done: true
  experiment_id: 9d5b3610c39c41e584d222a72a0062c6
  experiment_tag: 44_batch_size=512,eta=0.0,lr=0.1,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35778
  time_since_restore: 558.8925034999847
  time_this_iter_s: 558.8925034999847
  time_total_s: 558.8925034999847
  timestamp: 1616151944
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00044
  
[2m[36m(pid=35784)[0m Starting run with seed 0 - lr 1 - sec_lr 0.01 - bs 256
[2m[36m(pid=35784)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35784)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35784)[0m GPU available: False, used: False
[2m[36m(pid=35784)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35784)[0m 
[2m[36m(pid=35784)[0m   | Name      | Type              | Params
[2m[36m(pid=35784)[0m ------------------------------------------------
[2m[36m(pid=35784)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35784)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35784)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35784)[0m ------------------------------------------------
[2m[36m(pid=35784)[0m 8.7 K     Trainable params
[2m[36m(pid=35784)[0m 0         Non-trainable params
[2m[36m(pid=35784)[0m 8.7 K     Total params
[2m[36m(pid=35784)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35784)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35784)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35784)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35784)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35784)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35838)[0m time to fit was 1844.0577890872955
[2m[36m(pid=35838)[0m GPU available: False, used: False
[2m[36m(pid=35838)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35838)[0m 
[2m[36m(pid=35838)[0m   | Name      | Type              | Params
[2m[36m(pid=35838)[0m ------------------------------------------------
[2m[36m(pid=35838)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35838)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35838)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35838)[0m ------------------------------------------------
[2m[36m(pid=35838)[0m 8.7 K     Trainable params
[2m[36m(pid=35838)[0m 0         Non-trainable params
[2m[36m(pid=35838)[0m 8.7 K     Total params
[2m[36m(pid=35780)[0m time to fit was 225.44708585739136
[2m[36m(pid=35780)[0m GPU available: False, used: False
[2m[36m(pid=35780)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35780)[0m 
[2m[36m(pid=35780)[0m   | Name      | Type              | Params
[2m[36m(pid=35780)[0m ------------------------------------------------
[2m[36m(pid=35780)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35780)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35780)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35780)[0m ------------------------------------------------
[2m[36m(pid=35780)[0m 8.7 K     Trainable params
[2m[36m(pid=35780)[0m 0         Non-trainable params
[2m[36m(pid=35780)[0m 8.7 K     Total params
[2m[36m(pid=35871)[0m time to fit was 548.9201557636261
[2m[36m(pid=35871)[0m Finished run with seed 0 - lr 0.01 - sec_lr 0.001 - bs 128 - mean val auc: 0.9126551508903503
Result for _inner_e8deb_00007:
  auc: 0.9126551508903503
  date: 2021-03-19_12-06-21
  done: false
  experiment_id: e978a404a3ab43e496602fbb0c67a6bc
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35871
  time_since_restore: 2667.706332206726
  time_this_iter_s: 2667.706332206726
  time_total_s: 2667.706332206726
  timestamp: 1616151981
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00007
  
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 50/180 (1 PENDING, 26 RUNNING, 23 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    |                     |          128 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |                     |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00007 | RUNNING    | 145.101.32.82:35871 |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00010 | RUNNING    |                     |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    |                     |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00020 | RUNNING    |                     |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00025 | RUNNING    |                     |           32 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00049 | PENDING    |                     |          512 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |                     |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00016 | TERMINATED |                     |           64 |     0 | 1     |    0.001 |      1 |         1864.57  | 0.913413 |
| _inner_e8deb_00017 | TERMINATED |                     |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 30 more trials not shown (16 RUNNING, 13 TERMINATED)


Result for _inner_e8deb_00007:
  auc: 0.9126551508903503
  date: 2021-03-19_12-06-21
  done: true
  experiment_id: e978a404a3ab43e496602fbb0c67a6bc
  experiment_tag: 7_batch_size=128,eta=0.0,lr=0.01,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35871
  time_since_restore: 2667.706332206726
  time_this_iter_s: 2667.706332206726
  time_total_s: 2667.706332206726
  timestamp: 1616151981
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00007
  
[2m[36m(pid=35802)[0m time to fit was 668.640841960907
[2m[36m(pid=35802)[0m GPU available: False, used: False
[2m[36m(pid=35802)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35802)[0m 
[2m[36m(pid=35802)[0m   | Name      | Type              | Params
[2m[36m(pid=35802)[0m ------------------------------------------------
[2m[36m(pid=35802)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35802)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35802)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35802)[0m ------------------------------------------------
[2m[36m(pid=35802)[0m 8.7 K     Trainable params
[2m[36m(pid=35802)[0m 0         Non-trainable params
[2m[36m(pid=35802)[0m 8.7 K     Total params
[2m[36m(pid=35782)[0m Starting run with seed 0 - lr 1 - sec_lr 0.01 - bs 512
[2m[36m(pid=35782)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35782)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35782)[0m GPU available: False, used: False
[2m[36m(pid=35782)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35782)[0m 
[2m[36m(pid=35782)[0m   | Name      | Type              | Params
[2m[36m(pid=35782)[0m ------------------------------------------------
[2m[36m(pid=35782)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35782)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35782)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35782)[0m ------------------------------------------------
[2m[36m(pid=35782)[0m 8.7 K     Trainable params
[2m[36m(pid=35782)[0m 0         Non-trainable params
[2m[36m(pid=35782)[0m 8.7 K     Total params
[2m[36m(pid=35782)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35782)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35782)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35782)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35782)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35782)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35820)[0m time to fit was 477.98872590065
[2m[36m(pid=35820)[0m GPU available: False, used: False
[2m[36m(pid=35820)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35820)[0m 
[2m[36m(pid=35820)[0m   | Name      | Type              | Params
[2m[36m(pid=35820)[0m ------------------------------------------------
[2m[36m(pid=35820)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35820)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35820)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35820)[0m ------------------------------------------------
[2m[36m(pid=35820)[0m 8.7 K     Trainable params
[2m[36m(pid=35820)[0m 0         Non-trainable params
[2m[36m(pid=35820)[0m 8.7 K     Total params
[2m[36m(pid=35781)[0m time to fit was 233.79305219650269
[2m[36m(pid=35781)[0m GPU available: False, used: False
[2m[36m(pid=35781)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35781)[0m 
[2m[36m(pid=35781)[0m   | Name      | Type              | Params
[2m[36m(pid=35781)[0m ------------------------------------------------
[2m[36m(pid=35781)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35781)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35781)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35781)[0m ------------------------------------------------
[2m[36m(pid=35781)[0m 8.7 K     Trainable params
[2m[36m(pid=35781)[0m 0         Non-trainable params
[2m[36m(pid=35781)[0m 8.7 K     Total params
[2m[36m(pid=35792)[0m time to fit was 323.4217600822449
[2m[36m(pid=35792)[0m GPU available: False, used: False
[2m[36m(pid=35792)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35792)[0m 
[2m[36m(pid=35792)[0m   | Name      | Type              | Params
[2m[36m(pid=35792)[0m ------------------------------------------------
[2m[36m(pid=35792)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35792)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35792)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35792)[0m ------------------------------------------------
[2m[36m(pid=35792)[0m 8.7 K     Trainable params
[2m[36m(pid=35792)[0m 0         Non-trainable params
[2m[36m(pid=35792)[0m 8.7 K     Total params
[2m[36m(pid=35777)[0m time to fit was 211.01939153671265
[2m[36m(pid=35777)[0m GPU available: False, used: False
[2m[36m(pid=35777)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35777)[0m 
[2m[36m(pid=35777)[0m   | Name      | Type              | Params
[2m[36m(pid=35777)[0m ------------------------------------------------
[2m[36m(pid=35777)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35777)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35777)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35777)[0m ------------------------------------------------
[2m[36m(pid=35777)[0m 8.7 K     Trainable params
[2m[36m(pid=35777)[0m 0         Non-trainable params
[2m[36m(pid=35777)[0m 8.7 K     Total params
[2m[36m(pid=35852)[0m time to fit was 813.0428574085236
[2m[36m(pid=35852)[0m GPU available: False, used: False
[2m[36m(pid=35852)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35852)[0m 
[2m[36m(pid=35852)[0m   | Name      | Type              | Params
[2m[36m(pid=35852)[0m ------------------------------------------------
[2m[36m(pid=35852)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35852)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35852)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35852)[0m ------------------------------------------------
[2m[36m(pid=35852)[0m 8.7 K     Trainable params
[2m[36m(pid=35852)[0m 0         Non-trainable params
[2m[36m(pid=35852)[0m 8.7 K     Total params
[2m[36m(pid=35782)[0m time to fit was 93.784428358078
[2m[36m(pid=35782)[0m GPU available: False, used: False
[2m[36m(pid=35782)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35782)[0m 
[2m[36m(pid=35782)[0m   | Name      | Type              | Params
[2m[36m(pid=35782)[0m ------------------------------------------------
[2m[36m(pid=35782)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35782)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35782)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35782)[0m ------------------------------------------------
[2m[36m(pid=35782)[0m 8.7 K     Trainable params
[2m[36m(pid=35782)[0m 0         Non-trainable params
[2m[36m(pid=35782)[0m 8.7 K     Total params
[2m[36m(pid=35784)[0m time to fit was 129.6172833442688
[2m[36m(pid=35784)[0m GPU available: False, used: False
[2m[36m(pid=35784)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35784)[0m 
[2m[36m(pid=35784)[0m   | Name      | Type              | Params
[2m[36m(pid=35784)[0m ------------------------------------------------
[2m[36m(pid=35784)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35784)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35784)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35784)[0m ------------------------------------------------
[2m[36m(pid=35784)[0m 8.7 K     Trainable params
[2m[36m(pid=35784)[0m 0         Non-trainable params
[2m[36m(pid=35784)[0m 8.7 K     Total params
[2m[36m(pid=35821)[0m time to fit was 1449.7222287654877
[2m[36m(pid=35821)[0m GPU available: False, used: False
[2m[36m(pid=35821)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35785)[0m time to fit was 287.7176659107208
[2m[36m(pid=35821)[0m 
[2m[36m(pid=35821)[0m   | Name      | Type              | Params
[2m[36m(pid=35821)[0m ------------------------------------------------
[2m[36m(pid=35821)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35821)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35821)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35821)[0m ------------------------------------------------
[2m[36m(pid=35821)[0m 8.7 K     Trainable params
[2m[36m(pid=35821)[0m 0         Non-trainable params
[2m[36m(pid=35821)[0m 8.7 K     Total params
[2m[36m(pid=35785)[0m GPU available: False, used: False
[2m[36m(pid=35785)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35785)[0m 
[2m[36m(pid=35785)[0m   | Name      | Type              | Params
[2m[36m(pid=35785)[0m ------------------------------------------------
[2m[36m(pid=35785)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35785)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35785)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35785)[0m ------------------------------------------------
[2m[36m(pid=35785)[0m 8.7 K     Trainable params
[2m[36m(pid=35785)[0m 0         Non-trainable params
[2m[36m(pid=35785)[0m 8.7 K     Total params
[2m[36m(pid=35823)[0m time to fit was 546.1776189804077
[2m[36m(pid=35823)[0m GPU available: False, used: False
[2m[36m(pid=35823)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35823)[0m 
[2m[36m(pid=35823)[0m   | Name      | Type              | Params
[2m[36m(pid=35823)[0m ------------------------------------------------
[2m[36m(pid=35823)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35823)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35823)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35823)[0m ------------------------------------------------
[2m[36m(pid=35823)[0m 8.7 K     Trainable params
[2m[36m(pid=35823)[0m 0         Non-trainable params
[2m[36m(pid=35823)[0m 8.7 K     Total params
[2m[36m(pid=35855)[0m time to fit was 540.3258128166199
Result for _inner_e8deb_00002:
  auc: 0.9034114837646484
  date: 2021-03-19_12-08-52
  done: false
  experiment_id: cdb662ea01824154827313eec701fa16
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35855
  time_since_restore: 2820.8790769577026
  time_this_iter_s: 2820.8790769577026
  time_total_s: 2820.8790769577026
  timestamp: 1616152132
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00002
  
[2m[36m(pid=35855)[0m Finished run with seed 0 - lr 0.001 - sec_lr 0.001 - bs 128 - mean val auc: 0.9034114837646484
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 51/180 (1 PENDING, 26 RUNNING, 24 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00002 | RUNNING    | 145.101.32.82:35855 |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |                     |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |                     |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    |                     |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00020 | RUNNING    |                     |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00025 | RUNNING    |                     |           32 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00050 | PENDING    |                     |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |                     |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00016 | TERMINATED |                     |           64 |     0 | 1     |    0.001 |      1 |         1864.57  | 0.913413 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 31 more trials not shown (16 RUNNING, 14 TERMINATED)


Result for _inner_e8deb_00002:
  auc: 0.9034114837646484
  date: 2021-03-19_12-08-52
  done: true
  experiment_id: cdb662ea01824154827313eec701fa16
  experiment_tag: 2_batch_size=128,eta=0.0,lr=0.001,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35855
  time_since_restore: 2820.8790769577026
  time_this_iter_s: 2820.8790769577026
  time_total_s: 2820.8790769577026
  timestamp: 1616152132
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00002
  
[2m[36m(pid=35783)[0m Starting run with seed 0 - lr 2 - sec_lr 0.01 - bs 32
[2m[36m(pid=35783)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35783)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35783)[0m GPU available: False, used: False
[2m[36m(pid=35783)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35783)[0m 
[2m[36m(pid=35783)[0m   | Name      | Type              | Params
[2m[36m(pid=35783)[0m ------------------------------------------------
[2m[36m(pid=35783)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35783)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35783)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35783)[0m ------------------------------------------------
[2m[36m(pid=35783)[0m 8.7 K     Trainable params
[2m[36m(pid=35783)[0m 0         Non-trainable params
[2m[36m(pid=35783)[0m 8.7 K     Total params
[2m[36m(pid=35783)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35783)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35783)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35783)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35783)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35783)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35810)[0m time to fit was 328.468053817749
Result for _inner_e8deb_00033:
  auc: 0.9000673055648803
  date: 2021-03-19_12-09-35
  done: false
  experiment_id: d08c00eb55334320b37b47b90b6437dc
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35810
  time_since_restore: 1655.0630402565002
  time_this_iter_s: 1655.0630402565002
  time_total_s: 1655.0630402565002
  timestamp: 1616152175
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00033
  
[2m[36m(pid=35810)[0m Finished run with seed 0 - lr 0.001 - sec_lr 0.01 - bs 256 - mean val auc: 0.9000673055648803
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 52/180 (1 PENDING, 26 RUNNING, 25 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    |       |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00020 | RUNNING    |       |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00025 | RUNNING    |       |           32 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00051 | PENDING    |       |           64 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 32 more trials not shown (16 RUNNING, 15 TERMINATED)


Result for _inner_e8deb_00033:
  auc: 0.9000673055648803
  date: 2021-03-19_12-09-35
  done: true
  experiment_id: d08c00eb55334320b37b47b90b6437dc
  experiment_tag: 33_batch_size=256,eta=0.0,lr=0.001,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35810
  time_since_restore: 1655.0630402565002
  time_this_iter_s: 1655.0630402565002
  time_total_s: 1655.0630402565002
  timestamp: 1616152175
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00033
  
[2m[36m(pid=35782)[0m time to fit was 93.79741954803467
[2m[36m(pid=35782)[0m GPU available: False, used: False
[2m[36m(pid=35782)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35782)[0m 
[2m[36m(pid=35782)[0m   | Name      | Type              | Params
[2m[36m(pid=35782)[0m ------------------------------------------------
[2m[36m(pid=35782)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35782)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35782)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35782)[0m ------------------------------------------------
[2m[36m(pid=35782)[0m 8.7 K     Trainable params
[2m[36m(pid=35782)[0m 0         Non-trainable params
[2m[36m(pid=35782)[0m 8.7 K     Total params
[2m[36m(pid=35780)[0m time to fit was 223.33907532691956
Result for _inner_e8deb_00039:
  auc: 0.9120933532714843
  date: 2021-03-19_12-09-42
  done: false
  experiment_id: cc6af1e428fc4000a73ef94e9295d973
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35780
  time_since_restore: 1129.056640625
  time_this_iter_s: 1129.056640625
  time_total_s: 1129.056640625
  timestamp: 1616152182
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00039
  
[2m[36m(pid=35780)[0m Finished run with seed 0 - lr 0.01 - sec_lr 0.01 - bs 512 - mean val auc: 0.9120933532714843
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 53/180 (1 PENDING, 26 RUNNING, 26 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    |       |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00020 | RUNNING    |       |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00025 | RUNNING    |       |           32 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00052 | PENDING    |       |          128 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 33 more trials not shown (16 RUNNING, 16 TERMINATED)


Result for _inner_e8deb_00039:
  auc: 0.9120933532714843
  date: 2021-03-19_12-09-42
  done: true
  experiment_id: cc6af1e428fc4000a73ef94e9295d973
  experiment_tag: 39_batch_size=512,eta=0.0,lr=0.01,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35780
  time_since_restore: 1129.056640625
  time_this_iter_s: 1129.056640625
  time_total_s: 1129.056640625
  timestamp: 1616152182
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00039
  
[2m[36m(pid=35776)[0m Starting run with seed 0 - lr 2 - sec_lr 0.01 - bs 64
[2m[36m(pid=35776)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35776)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35776)[0m GPU available: False, used: False
[2m[36m(pid=35776)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35776)[0m 
[2m[36m(pid=35776)[0m   | Name      | Type              | Params
[2m[36m(pid=35776)[0m ------------------------------------------------
[2m[36m(pid=35776)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35776)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35776)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35776)[0m ------------------------------------------------
[2m[36m(pid=35776)[0m 8.7 K     Trainable params
[2m[36m(pid=35776)[0m 0         Non-trainable params
[2m[36m(pid=35776)[0m 8.7 K     Total params
[2m[36m(pid=35776)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35776)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35776)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35776)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35776)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35776)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35787)[0m time to fit was 617.8274345397949
[2m[36m(pid=35787)[0m GPU available: False, used: False
[2m[36m(pid=35787)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35787)[0m 
[2m[36m(pid=35787)[0m   | Name      | Type              | Params
[2m[36m(pid=35787)[0m ------------------------------------------------
[2m[36m(pid=35787)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35787)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35787)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35787)[0m ------------------------------------------------
[2m[36m(pid=35787)[0m 8.7 K     Trainable params
[2m[36m(pid=35787)[0m 0         Non-trainable params
[2m[36m(pid=35787)[0m 8.7 K     Total params
[2m[36m(pid=40498)[0m Starting run with seed 0 - lr 2 - sec_lr 0.01 - bs 128
[2m[36m(pid=40498)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40498)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=40498)[0m GPU available: False, used: False
[2m[36m(pid=40498)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40498)[0m 
[2m[36m(pid=40498)[0m   | Name      | Type              | Params
[2m[36m(pid=40498)[0m ------------------------------------------------
[2m[36m(pid=40498)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40498)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40498)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40498)[0m ------------------------------------------------
[2m[36m(pid=40498)[0m 8.7 K     Trainable params
[2m[36m(pid=40498)[0m 0         Non-trainable params
[2m[36m(pid=40498)[0m 8.7 K     Total params
[2m[36m(pid=40498)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=40498)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=40498)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=40498)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=40498)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=40498)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35784)[0m time to fit was 112.49371147155762
[2m[36m(pid=35784)[0m GPU available: False, used: False
[2m[36m(pid=35784)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35784)[0m 
[2m[36m(pid=35784)[0m   | Name      | Type              | Params
[2m[36m(pid=35784)[0m ------------------------------------------------
[2m[36m(pid=35784)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35784)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35784)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35784)[0m ------------------------------------------------
[2m[36m(pid=35784)[0m 8.7 K     Trainable params
[2m[36m(pid=35784)[0m 0         Non-trainable params
[2m[36m(pid=35784)[0m 8.7 K     Total params
[2m[36m(pid=35781)[0m time to fit was 190.15051651000977
Result for _inner_e8deb_00042:
  auc: 0.9134185433387756
  date: 2021-03-19_12-10-13
  done: false
  experiment_id: bc8c3669834c47798fddce4cb398c459
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35781
  time_since_restore: 1030.702952861786
  time_this_iter_s: 1030.702952861786
  time_total_s: 1030.702952861786
  timestamp: 1616152213
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00042
  
[2m[36m(pid=35781)[0m Finished run with seed 0 - lr 0.1 - sec_lr 0.01 - bs 128 - mean val auc: 0.9134185433387756
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 54/180 (1 PENDING, 26 RUNNING, 27 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    |       |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00020 | RUNNING    |       |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00025 | RUNNING    |       |           32 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00053 | PENDING    |       |          256 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 34 more trials not shown (16 RUNNING, 17 TERMINATED)


Result for _inner_e8deb_00042:
  auc: 0.9134185433387756
  date: 2021-03-19_12-10-13
  done: true
  experiment_id: bc8c3669834c47798fddce4cb398c459
  experiment_tag: 42_batch_size=128,eta=0.0,lr=0.1,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35781
  time_since_restore: 1030.702952861786
  time_this_iter_s: 1030.702952861786
  time_total_s: 1030.702952861786
  timestamp: 1616152213
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00042
  
[2m[36m(pid=40518)[0m Starting run with seed 0 - lr 2 - sec_lr 0.01 - bs 256
[2m[36m(pid=40518)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40518)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=40518)[0m GPU available: False, used: False
[2m[36m(pid=40518)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40518)[0m 
[2m[36m(pid=40518)[0m   | Name      | Type              | Params
[2m[36m(pid=40518)[0m ------------------------------------------------
[2m[36m(pid=40518)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40518)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40518)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40518)[0m ------------------------------------------------
[2m[36m(pid=40518)[0m 8.7 K     Trainable params
[2m[36m(pid=40518)[0m 0         Non-trainable params
[2m[36m(pid=40518)[0m 8.7 K     Total params
[2m[36m(pid=40518)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=40518)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=40518)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=40518)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=40518)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=40518)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35848)[0m time to fit was 1815.1674857139587
[2m[36m(pid=35848)[0m GPU available: False, used: False
[2m[36m(pid=35848)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35848)[0m 
[2m[36m(pid=35848)[0m   | Name      | Type              | Params
[2m[36m(pid=35848)[0m ------------------------------------------------
[2m[36m(pid=35848)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35848)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35848)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35848)[0m ------------------------------------------------
[2m[36m(pid=35848)[0m 8.7 K     Trainable params
[2m[36m(pid=35848)[0m 0         Non-trainable params
[2m[36m(pid=35848)[0m 8.7 K     Total params
[2m[36m(pid=35803)[0m time to fit was 395.92142963409424
[2m[36m(pid=35803)[0m GPU available: False, used: False
[2m[36m(pid=35803)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35803)[0m 
[2m[36m(pid=35803)[0m   | Name      | Type              | Params
[2m[36m(pid=35803)[0m ------------------------------------------------
[2m[36m(pid=35803)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35803)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35803)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35803)[0m ------------------------------------------------
[2m[36m(pid=35803)[0m 8.7 K     Trainable params
[2m[36m(pid=35803)[0m 0         Non-trainable params
[2m[36m(pid=35803)[0m 8.7 K     Total params
[2m[36m(pid=35777)[0m time to fit was 209.9543435573578
[2m[36m(pid=35777)[0m GPU available: False, used: False
[2m[36m(pid=35777)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35777)[0m 
[2m[36m(pid=35777)[0m   | Name      | Type              | Params
[2m[36m(pid=35777)[0m ------------------------------------------------
[2m[36m(pid=35777)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35777)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35777)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35777)[0m ------------------------------------------------
[2m[36m(pid=35777)[0m 8.7 K     Trainable params
[2m[36m(pid=35777)[0m 0         Non-trainable params
[2m[36m(pid=35777)[0m 8.7 K     Total params
[2m[36m(pid=35790)[0m time to fit was 440.9070210456848
[2m[36m(pid=35790)[0m GPU available: False, used: False
[2m[36m(pid=35790)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35790)[0m 
[2m[36m(pid=35790)[0m   | Name      | Type              | Params
[2m[36m(pid=35790)[0m ------------------------------------------------
[2m[36m(pid=35790)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35790)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35790)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35790)[0m ------------------------------------------------
[2m[36m(pid=35790)[0m 8.7 K     Trainable params
[2m[36m(pid=35790)[0m 0         Non-trainable params
[2m[36m(pid=35790)[0m 8.7 K     Total params
[2m[36m(pid=35818)[0m time to fit was 977.8609447479248
[2m[36m(pid=35818)[0m GPU available: False, used: False
[2m[36m(pid=35818)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35818)[0m 
[2m[36m(pid=35818)[0m   | Name      | Type              | Params
[2m[36m(pid=35818)[0m ------------------------------------------------
[2m[36m(pid=35818)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35818)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35818)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35818)[0m ------------------------------------------------
[2m[36m(pid=35818)[0m 8.7 K     Trainable params
[2m[36m(pid=35818)[0m 0         Non-trainable params
[2m[36m(pid=35818)[0m 8.7 K     Total params
[2m[36m(pid=35782)[0m time to fit was 99.49191498756409
[2m[36m(pid=35782)[0m GPU available: False, used: False
[2m[36m(pid=35782)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35782)[0m 
[2m[36m(pid=35782)[0m   | Name      | Type              | Params
[2m[36m(pid=35782)[0m ------------------------------------------------
[2m[36m(pid=35782)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35782)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35782)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35782)[0m ------------------------------------------------
[2m[36m(pid=35782)[0m 8.7 K     Trainable params
[2m[36m(pid=35782)[0m 0         Non-trainable params
[2m[36m(pid=35782)[0m 8.7 K     Total params
[2m[36m(pid=35786)[0m time to fit was 613.9337220191956
[2m[36m(pid=35786)[0m GPU available: False, used: False
[2m[36m(pid=35786)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35786)[0m 
[2m[36m(pid=35786)[0m   | Name      | Type              | Params
[2m[36m(pid=35786)[0m ------------------------------------------------
[2m[36m(pid=35786)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35786)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35786)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35786)[0m ------------------------------------------------
[2m[36m(pid=35786)[0m 8.7 K     Trainable params
[2m[36m(pid=35786)[0m 0         Non-trainable params
[2m[36m(pid=35786)[0m 8.7 K     Total params
[2m[36m(pid=35857)[0m time to fit was 964.5559976100922
[2m[36m(pid=35857)[0m GPU available: False, used: False
[2m[36m(pid=35857)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35857)[0m 
[2m[36m(pid=35857)[0m   | Name      | Type              | Params
[2m[36m(pid=35857)[0m ------------------------------------------------
[2m[36m(pid=35857)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35857)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35857)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35857)[0m ------------------------------------------------
[2m[36m(pid=35857)[0m 8.7 K     Trainable params
[2m[36m(pid=35857)[0m 0         Non-trainable params
[2m[36m(pid=35857)[0m 8.7 K     Total params
[2m[36m(pid=40498)[0m time to fit was 133.6602177619934
[2m[36m(pid=40498)[0m GPU available: False, used: False
[2m[36m(pid=40498)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40498)[0m 
[2m[36m(pid=40498)[0m   | Name      | Type              | Params
[2m[36m(pid=40498)[0m ------------------------------------------------
[2m[36m(pid=40498)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40498)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40498)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40498)[0m ------------------------------------------------
[2m[36m(pid=40498)[0m 8.7 K     Trainable params
[2m[36m(pid=40498)[0m 0         Non-trainable params
[2m[36m(pid=40498)[0m 8.7 K     Total params
[2m[36m(pid=35784)[0m time to fit was 136.17306637763977
[2m[36m(pid=35784)[0m GPU available: False, used: False
[2m[36m(pid=35784)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35784)[0m 
[2m[36m(pid=35784)[0m   | Name      | Type              | Params
[2m[36m(pid=35784)[0m ------------------------------------------------
[2m[36m(pid=35784)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35784)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35784)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35784)[0m ------------------------------------------------
[2m[36m(pid=35784)[0m 8.7 K     Trainable params
[2m[36m(pid=35784)[0m 0         Non-trainable params
[2m[36m(pid=35784)[0m 8.7 K     Total params
[2m[36m(pid=35792)[0m time to fit was 318.6072714328766
Result for _inner_e8deb_00038:
  auc: 0.9121300101280212
  date: 2021-03-19_12-12-59
  done: false
  experiment_id: 6560759a1e2642088069ca3bc101bb8f
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35792
  time_since_restore: 1633.9336502552032
  time_this_iter_s: 1633.9336502552032
  time_total_s: 1633.9336502552032
  timestamp: 1616152379
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00038
  
[2m[36m(pid=35792)[0m Finished run with seed 0 - lr 0.01 - sec_lr 0.01 - bs 256 - mean val auc: 0.9121300101280212
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 55/180 (1 PENDING, 26 RUNNING, 28 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    |       |           32 |     0 | 1     |    0.001 |        |                  |          |
| _inner_e8deb_00020 | RUNNING    |       |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00025 | RUNNING    |       |           32 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00054 | PENDING    |       |          512 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 35 more trials not shown (16 RUNNING, 18 TERMINATED)


Result for _inner_e8deb_00038:
  auc: 0.9121300101280212
  date: 2021-03-19_12-12-59
  done: true
  experiment_id: 6560759a1e2642088069ca3bc101bb8f
  experiment_tag: 38_batch_size=256,eta=0.0,lr=0.01,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35792
  time_since_restore: 1633.9336502552032
  time_this_iter_s: 1633.9336502552032
  time_total_s: 1633.9336502552032
  timestamp: 1616152379
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00038
  
2021-03-19 12:13:01,417	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffffd0c1f57d01000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=35782)[0m time to fit was 112.75053477287292
[2m[36m(pid=35782)[0m GPU available: False, used: False
[2m[36m(pid=35782)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35782)[0m 
[2m[36m(pid=35782)[0m   | Name      | Type              | Params
[2m[36m(pid=35782)[0m ------------------------------------------------
[2m[36m(pid=35782)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35782)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35782)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35782)[0m ------------------------------------------------
[2m[36m(pid=35782)[0m 8.7 K     Trainable params
[2m[36m(pid=35782)[0m 0         Non-trainable params
[2m[36m(pid=35782)[0m 8.7 K     Total params
[2m[36m(pid=46501)[0m Starting run with seed 0 - lr 2 - sec_lr 0.01 - bs 512
[2m[36m(pid=46501)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46501)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=46501)[0m GPU available: False, used: False
[2m[36m(pid=46501)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=46501)[0m 
[2m[36m(pid=46501)[0m   | Name      | Type              | Params
[2m[36m(pid=46501)[0m ------------------------------------------------
[2m[36m(pid=46501)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=46501)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=46501)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=46501)[0m ------------------------------------------------
[2m[36m(pid=46501)[0m 8.7 K     Trainable params
[2m[36m(pid=46501)[0m 0         Non-trainable params
[2m[36m(pid=46501)[0m 8.7 K     Total params
[2m[36m(pid=46501)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=46501)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=46501)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=46501)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=46501)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=46501)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=40518)[0m time to fit was 177.08695650100708
[2m[36m(pid=40518)[0m GPU available: False, used: False
[2m[36m(pid=40518)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40518)[0m 
[2m[36m(pid=40518)[0m   | Name      | Type              | Params
[2m[36m(pid=40518)[0m ------------------------------------------------
[2m[36m(pid=40518)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40518)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40518)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40518)[0m ------------------------------------------------
[2m[36m(pid=40518)[0m 8.7 K     Trainable params
[2m[36m(pid=40518)[0m 0         Non-trainable params
[2m[36m(pid=40518)[0m 8.7 K     Total params
[2m[36m(pid=35889)[0m time to fit was 543.1650660037994
Result for _inner_e8deb_00015:
  auc: 0.9128451228141785
  date: 2021-03-19_12-13-51
  done: false
  experiment_id: d3b3dcdcf29c4fbaa72187ba4bd799de
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35889
  time_since_restore: 3119.1529228687286
  time_this_iter_s: 3119.1529228687286
  time_total_s: 3119.1529228687286
  timestamp: 1616152431
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00015
  
[2m[36m(pid=35889)[0m Finished run with seed 0 - lr 1 - sec_lr 0.001 - bs 32 - mean val auc: 0.9128451228141785
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 56/180 (1 PENDING, 26 RUNNING, 29 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |                     |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |                     |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00015 | RUNNING    | 145.101.32.82:35889 |           32 |     0 | 1     |    0.001 |      1 |         3119.15  | 0.912845 |
| _inner_e8deb_00020 | RUNNING    |                     |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00025 | RUNNING    |                     |           32 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |                     |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00055 | PENDING    |                     |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |                     |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 36 more trials not shown (16 RUNNING, 19 TERMINATED)


Result for _inner_e8deb_00015:
  auc: 0.9128451228141785
  date: 2021-03-19_12-13-51
  done: true
  experiment_id: d3b3dcdcf29c4fbaa72187ba4bd799de
  experiment_tag: 15_batch_size=32,eta=0.0,lr=1,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35889
  time_since_restore: 3119.1529228687286
  time_this_iter_s: 3119.1529228687286
  time_total_s: 3119.1529228687286
  timestamp: 1616152431
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00015
  
[2m[36m(pid=46501)[0m time to fit was 42.84670948982239
[2m[36m(pid=46501)[0m GPU available: False, used: False
[2m[36m(pid=46501)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=46501)[0m 
[2m[36m(pid=46501)[0m   | Name      | Type              | Params
[2m[36m(pid=46501)[0m ------------------------------------------------
[2m[36m(pid=46501)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=46501)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=46501)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=46501)[0m ------------------------------------------------
[2m[36m(pid=46501)[0m 8.7 K     Trainable params
[2m[36m(pid=46501)[0m 0         Non-trainable params
[2m[36m(pid=46501)[0m 8.7 K     Total params
[2m[36m(pid=35777)[0m time to fit was 167.16345119476318
[2m[36m(pid=35777)[0m GPU available: False, used: False
[2m[36m(pid=35777)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35777)[0m 
[2m[36m(pid=35777)[0m   | Name      | Type              | Params
[2m[36m(pid=35777)[0m ------------------------------------------------
[2m[36m(pid=35777)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35777)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35777)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35777)[0m ------------------------------------------------
[2m[36m(pid=35777)[0m 8.7 K     Trainable params
[2m[36m(pid=35777)[0m 0         Non-trainable params
[2m[36m(pid=35777)[0m 8.7 K     Total params
[2m[36m(pid=48139)[0m Starting run with seed 0 - lr 5 - sec_lr 0.01 - bs 32
[2m[36m(pid=48139)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=48139)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=48139)[0m GPU available: False, used: False
[2m[36m(pid=48139)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48139)[0m 
[2m[36m(pid=48139)[0m   | Name      | Type              | Params
[2m[36m(pid=48139)[0m ------------------------------------------------
[2m[36m(pid=48139)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48139)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48139)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48139)[0m ------------------------------------------------
[2m[36m(pid=48139)[0m 8.7 K     Trainable params
[2m[36m(pid=48139)[0m 0         Non-trainable params
[2m[36m(pid=48139)[0m 8.7 K     Total params
[2m[36m(pid=48139)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=48139)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48139)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=48139)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48139)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=48139)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35782)[0m time to fit was 79.22561383247375
Result for _inner_e8deb_00049:
  auc: 0.913597583770752
  date: 2021-03-19_12-14-29
  done: false
  experiment_id: 26e3cf22024b4994a58b3a56017ebf6d
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35782
  time_since_restore: 480.3927505016327
  time_this_iter_s: 480.3927505016327
  time_total_s: 480.3927505016327
  timestamp: 1616152469
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00049
  
[2m[36m(pid=35782)[0m Finished run with seed 0 - lr 1 - sec_lr 0.01 - bs 512 - mean val auc: 0.913597583770752
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 57/180 (1 PENDING, 26 RUNNING, 30 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00020 | RUNNING    |       |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00025 | RUNNING    |       |           32 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00032 | RUNNING    |       |          128 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00056 | PENDING    |       |           64 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 37 more trials not shown (16 RUNNING, 20 TERMINATED)


Result for _inner_e8deb_00049:
  auc: 0.913597583770752
  date: 2021-03-19_12-14-29
  done: true
  experiment_id: 26e3cf22024b4994a58b3a56017ebf6d
  experiment_tag: 49_batch_size=512,eta=0.0,lr=1,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35782
  time_since_restore: 480.3927505016327
  time_this_iter_s: 480.3927505016327
  time_total_s: 480.3927505016327
  timestamp: 1616152469
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00049
  
2021-03-19 12:14:31,651	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff1fa374ee01000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=49332)[0m Starting run with seed 0 - lr 5 - sec_lr 0.01 - bs 64
[2m[36m(pid=49332)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=49332)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=49332)[0m GPU available: False, used: False
[2m[36m(pid=49332)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49332)[0m 
[2m[36m(pid=49332)[0m   | Name      | Type              | Params
[2m[36m(pid=49332)[0m ------------------------------------------------
[2m[36m(pid=49332)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49332)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49332)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49332)[0m ------------------------------------------------
[2m[36m(pid=49332)[0m 8.7 K     Trainable params
[2m[36m(pid=49332)[0m 0         Non-trainable params
[2m[36m(pid=49332)[0m 8.7 K     Total params
[2m[36m(pid=49332)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=49332)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49332)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=49332)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49332)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=49332)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35784)[0m time to fit was 155.3986690044403
[2m[36m(pid=35784)[0m GPU available: False, used: False
[2m[36m(pid=35784)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35784)[0m 
[2m[36m(pid=35784)[0m   | Name      | Type              | Params
[2m[36m(pid=35784)[0m ------------------------------------------------
[2m[36m(pid=35784)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35784)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35784)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35784)[0m ------------------------------------------------
[2m[36m(pid=35784)[0m 8.7 K     Trainable params
[2m[36m(pid=35784)[0m 0         Non-trainable params
[2m[36m(pid=35784)[0m 8.7 K     Total params
[2m[36m(pid=35878)[0m time to fit was 955.2403650283813
[2m[36m(pid=35878)[0m GPU available: False, used: False
[2m[36m(pid=35878)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35878)[0m 
[2m[36m(pid=35878)[0m   | Name      | Type              | Params
[2m[36m(pid=35878)[0m ------------------------------------------------
[2m[36m(pid=35878)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35878)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35878)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35878)[0m ------------------------------------------------
[2m[36m(pid=35878)[0m 8.7 K     Trainable params
[2m[36m(pid=35878)[0m 0         Non-trainable params
[2m[36m(pid=35878)[0m 8.7 K     Total params
[2m[36m(pid=35776)[0m time to fit was 335.1842122077942
[2m[36m(pid=35776)[0m GPU available: False, used: False
[2m[36m(pid=35776)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35776)[0m 
[2m[36m(pid=35776)[0m   | Name      | Type              | Params
[2m[36m(pid=35776)[0m ------------------------------------------------
[2m[36m(pid=35776)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35776)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35776)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35776)[0m ------------------------------------------------
[2m[36m(pid=35776)[0m 8.7 K     Trainable params
[2m[36m(pid=35776)[0m 0         Non-trainable params
[2m[36m(pid=35776)[0m 8.7 K     Total params
[2m[36m(pid=35820)[0m time to fit was 520.4938235282898
[2m[36m(pid=35783)[0m time to fit was 390.0982623100281
[2m[36m(pid=35820)[0m GPU available: False, used: False
[2m[36m(pid=35820)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35820)[0m 
[2m[36m(pid=35820)[0m   | Name      | Type              | Params
[2m[36m(pid=35820)[0m ------------------------------------------------
[2m[36m(pid=35820)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35820)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35820)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35820)[0m ------------------------------------------------
[2m[36m(pid=35820)[0m 8.7 K     Trainable params
[2m[36m(pid=35820)[0m 0         Non-trainable params
[2m[36m(pid=35820)[0m 8.7 K     Total params
[2m[36m(pid=35783)[0m GPU available: False, used: False
[2m[36m(pid=35783)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35783)[0m 
[2m[36m(pid=35783)[0m   | Name      | Type              | Params
[2m[36m(pid=35783)[0m ------------------------------------------------
[2m[36m(pid=35783)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35783)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35783)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35783)[0m ------------------------------------------------
[2m[36m(pid=35783)[0m 8.7 K     Trainable params
[2m[36m(pid=35783)[0m 0         Non-trainable params
[2m[36m(pid=35783)[0m 8.7 K     Total params
[2m[36m(pid=46501)[0m time to fit was 115.20977354049683
[2m[36m(pid=46501)[0m GPU available: False, used: False
[2m[36m(pid=46501)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=46501)[0m 
[2m[36m(pid=46501)[0m   | Name      | Type              | Params
[2m[36m(pid=46501)[0m ------------------------------------------------
[2m[36m(pid=46501)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=46501)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=46501)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=46501)[0m ------------------------------------------------
[2m[36m(pid=46501)[0m 8.7 K     Trainable params
[2m[36m(pid=46501)[0m 0         Non-trainable params
[2m[36m(pid=46501)[0m 8.7 K     Total params
[2m[36m(pid=35784)[0m time to fit was 79.32090592384338
Result for _inner_e8deb_00048:
  auc: 0.9135586023330688
  date: 2021-03-19_12-16-08
  done: false
  experiment_id: 518a1000157d415da7f1fd691ffbe1cb
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35784
  time_since_restore: 614.2809045314789
  time_this_iter_s: 614.2809045314789
  time_total_s: 614.2809045314789
  timestamp: 1616152568
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00048
  
[2m[36m(pid=35784)[0m Finished run with seed 0 - lr 1 - sec_lr 0.01 - bs 256 - mean val auc: 0.9135586023330688
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 58/180 (1 PENDING, 26 RUNNING, 31 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    |       |           32 |     0 | 0.1   |    0.001 |        |                  |          |
| _inner_e8deb_00020 | RUNNING    |       |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00025 | RUNNING    |       |           32 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00032 | RUNNING    |       |          128 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00057 | PENDING    |       |          128 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 38 more trials not shown (16 RUNNING, 21 TERMINATED)


Result for _inner_e8deb_00048:
  auc: 0.9135586023330688
  date: 2021-03-19_12-16-08
  done: true
  experiment_id: 518a1000157d415da7f1fd691ffbe1cb
  experiment_tag: 48_batch_size=256,eta=0.0,lr=1,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35784
  time_since_restore: 614.2809045314789
  time_this_iter_s: 614.2809045314789
  time_total_s: 614.2809045314789
  timestamp: 1616152568
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00048
  
[2m[36m(pid=52234)[0m Starting run with seed 0 - lr 5 - sec_lr 0.01 - bs 128
[2m[36m(pid=52234)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=52234)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=52234)[0m GPU available: False, used: False
[2m[36m(pid=52234)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=52234)[0m 
[2m[36m(pid=52234)[0m   | Name      | Type              | Params
[2m[36m(pid=52234)[0m ------------------------------------------------
[2m[36m(pid=52234)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=52234)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=52234)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=52234)[0m ------------------------------------------------
[2m[36m(pid=52234)[0m 8.7 K     Trainable params
[2m[36m(pid=52234)[0m 0         Non-trainable params
[2m[36m(pid=52234)[0m 8.7 K     Total params
[2m[36m(pid=52234)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=52234)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=52234)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=52234)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=52234)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=52234)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35785)[0m time to fit was 490.8304510116577
[2m[36m(pid=35785)[0m GPU available: False, used: False
[2m[36m(pid=35785)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35785)[0m 
[2m[36m(pid=35785)[0m   | Name      | Type              | Params
[2m[36m(pid=35785)[0m ------------------------------------------------
[2m[36m(pid=35785)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35785)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35785)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35785)[0m ------------------------------------------------
[2m[36m(pid=35785)[0m 8.7 K     Trainable params
[2m[36m(pid=35785)[0m 0         Non-trainable params
[2m[36m(pid=35785)[0m 8.7 K     Total params
[2m[36m(pid=40518)[0m time to fit was 188.12928366661072
[2m[36m(pid=40518)[0m GPU available: False, used: False
[2m[36m(pid=40518)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40518)[0m 
[2m[36m(pid=40518)[0m   | Name      | Type              | Params
[2m[36m(pid=40518)[0m ------------------------------------------------
[2m[36m(pid=40518)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40518)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40518)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40518)[0m ------------------------------------------------
[2m[36m(pid=40518)[0m 8.7 K     Trainable params
[2m[36m(pid=40518)[0m 0         Non-trainable params
[2m[36m(pid=40518)[0m 8.7 K     Total params
[2m[36m(pid=40498)[0m time to fit was 269.8418490886688
[2m[36m(pid=40498)[0m GPU available: False, used: False
[2m[36m(pid=40498)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40498)[0m 
[2m[36m(pid=40498)[0m   | Name      | Type              | Params
[2m[36m(pid=40498)[0m ------------------------------------------------
[2m[36m(pid=40498)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40498)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40498)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40498)[0m ------------------------------------------------
[2m[36m(pid=40498)[0m 8.7 K     Trainable params
[2m[36m(pid=40498)[0m 0         Non-trainable params
[2m[36m(pid=40498)[0m 8.7 K     Total params
[2m[36m(pid=35852)[0m time to fit was 540.4522123336792
Result for _inner_e8deb_00010:
  auc: 0.9134906768798828
  date: 2021-03-19_12-16-52
  done: false
  experiment_id: 86ca0f36da9748c1a5c66054bb7580e2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35852
  time_since_restore: 3300.476579427719
  time_this_iter_s: 3300.476579427719
  time_total_s: 3300.476579427719
  timestamp: 1616152612
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00010
  
[2m[36m(pid=35852)[0m Finished run with seed 0 - lr 0.1 - sec_lr 0.001 - bs 32 - mean val auc: 0.9134906768798828
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 59/180 (1 PENDING, 26 RUNNING, 32 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |                     |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00010 | RUNNING    | 145.101.32.82:35852 |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00020 | RUNNING    |                     |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00025 | RUNNING    |                     |           32 |     0 | 5     |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |                     |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00032 | RUNNING    |                     |          128 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00058 | PENDING    |                     |          256 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |                     |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 39 more trials not shown (16 RUNNING, 22 TERMINATED)


Result for _inner_e8deb_00010:
  auc: 0.9134906768798828
  date: 2021-03-19_12-16-52
  done: true
  experiment_id: 86ca0f36da9748c1a5c66054bb7580e2
  experiment_tag: 10_batch_size=32,eta=0.0,lr=0.1,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35852
  time_since_restore: 3300.476579427719
  time_this_iter_s: 3300.476579427719
  time_total_s: 3300.476579427719
  timestamp: 1616152612
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00010
  
[2m[36m(pid=663)[0m Starting run with seed 0 - lr 5 - sec_lr 0.01 - bs 256
[2m[36m(pid=663)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=663)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=663)[0m GPU available: False, used: False
[2m[36m(pid=663)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=663)[0m 
[2m[36m(pid=663)[0m   | Name      | Type              | Params
[2m[36m(pid=663)[0m ------------------------------------------------
[2m[36m(pid=663)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=663)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=663)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=663)[0m ------------------------------------------------
[2m[36m(pid=663)[0m 8.7 K     Trainable params
[2m[36m(pid=663)[0m 0         Non-trainable params
[2m[36m(pid=663)[0m 8.7 K     Total params
[2m[36m(pid=663)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=663)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=663)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=663)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=663)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=663)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35790)[0m time to fit was 359.6611154079437
[2m[36m(pid=35790)[0m GPU available: False, used: False
[2m[36m(pid=35790)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35790)[0m 
[2m[36m(pid=35790)[0m   | Name      | Type              | Params
[2m[36m(pid=35790)[0m ------------------------------------------------
[2m[36m(pid=35790)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35790)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35790)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35790)[0m ------------------------------------------------
[2m[36m(pid=35790)[0m 8.7 K     Trainable params
[2m[36m(pid=35790)[0m 0         Non-trainable params
[2m[36m(pid=35790)[0m 8.7 K     Total params
[2m[36m(pid=35777)[0m time to fit was 195.49328184127808
[2m[36m(pid=35777)[0m GPU available: False, used: False
[2m[36m(pid=35777)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35777)[0m 
[2m[36m(pid=35777)[0m   | Name      | Type              | Params
[2m[36m(pid=35777)[0m ------------------------------------------------
[2m[36m(pid=35777)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35777)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35777)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35777)[0m ------------------------------------------------
[2m[36m(pid=35777)[0m 8.7 K     Trainable params
[2m[36m(pid=35777)[0m 0         Non-trainable params
[2m[36m(pid=35777)[0m 8.7 K     Total params
[2m[36m(pid=35823)[0m time to fit was 530.0247223377228
[2m[36m(pid=35823)[0m GPU available: False, used: False
[2m[36m(pid=35823)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35823)[0m 
[2m[36m(pid=35823)[0m   | Name      | Type              | Params
[2m[36m(pid=35823)[0m ------------------------------------------------
[2m[36m(pid=35823)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35823)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35823)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35823)[0m ------------------------------------------------
[2m[36m(pid=35823)[0m 8.7 K     Trainable params
[2m[36m(pid=35823)[0m 0         Non-trainable params
[2m[36m(pid=35823)[0m 8.7 K     Total params
[2m[36m(pid=46501)[0m time to fit was 101.60978412628174
[2m[36m(pid=46501)[0m GPU available: False, used: False
[2m[36m(pid=46501)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=46501)[0m 
[2m[36m(pid=46501)[0m   | Name      | Type              | Params
[2m[36m(pid=46501)[0m ------------------------------------------------
[2m[36m(pid=46501)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=46501)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=46501)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=46501)[0m ------------------------------------------------
[2m[36m(pid=46501)[0m 8.7 K     Trainable params
[2m[36m(pid=46501)[0m 0         Non-trainable params
[2m[36m(pid=46501)[0m 8.7 K     Total params
[2m[36m(pid=663)[0m time to fit was 65.0926365852356
[2m[36m(pid=663)[0m GPU available: False, used: False
[2m[36m(pid=663)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=663)[0m 
[2m[36m(pid=663)[0m   | Name      | Type              | Params
[2m[36m(pid=663)[0m ------------------------------------------------
[2m[36m(pid=663)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=663)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=663)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=663)[0m ------------------------------------------------
[2m[36m(pid=663)[0m 8.7 K     Trainable params
[2m[36m(pid=663)[0m 0         Non-trainable params
[2m[36m(pid=663)[0m 8.7 K     Total params
[2m[36m(pid=52234)[0m time to fit was 122.71741104125977
[2m[36m(pid=52234)[0m GPU available: False, used: False
[2m[36m(pid=52234)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=52234)[0m 
[2m[36m(pid=52234)[0m   | Name      | Type              | Params
[2m[36m(pid=52234)[0m ------------------------------------------------
[2m[36m(pid=52234)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=52234)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=52234)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=52234)[0m ------------------------------------------------
[2m[36m(pid=52234)[0m 8.7 K     Trainable params
[2m[36m(pid=52234)[0m 0         Non-trainable params
[2m[36m(pid=52234)[0m 8.7 K     Total params
[2m[36m(pid=35806)[0m time to fit was 839.7413020133972
[2m[36m(pid=35806)[0m GPU available: False, used: False
[2m[36m(pid=35806)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35806)[0m 
[2m[36m(pid=35806)[0m   | Name      | Type              | Params
[2m[36m(pid=35806)[0m ------------------------------------------------
[2m[36m(pid=35806)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35806)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35806)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35806)[0m ------------------------------------------------
[2m[36m(pid=35806)[0m 8.7 K     Trainable params
[2m[36m(pid=35806)[0m 0         Non-trainable params
[2m[36m(pid=35806)[0m 8.7 K     Total params
[2m[36m(pid=35821)[0m time to fit was 641.3880527019501
Result for _inner_e8deb_00025:
  auc: 0.6175939321517945
  date: 2021-03-19_12-18-54
  done: false
  experiment_id: d14210d64f5b46989cd9612b93804ff0
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35821
  time_since_restore: 3421.402410507202
  time_this_iter_s: 3421.402410507202
  time_total_s: 3421.402410507202
  timestamp: 1616152734
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00025
  
[2m[36m(pid=35821)[0m Finished run with seed 0 - lr 5 - sec_lr 0.001 - bs 32 - mean val auc: 0.6175939321517945
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 60/180 (1 PENDING, 26 RUNNING, 33 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |                     |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00020 | RUNNING    |                     |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00025 | RUNNING    | 145.101.32.82:35821 |           32 |     0 | 5     |    0.001 |      1 |         3421.4   | 0.617594 |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |                     |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00032 | RUNNING    |                     |          128 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |                     |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00059 | PENDING    |                     |          512 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 40 more trials not shown (16 RUNNING, 23 TERMINATED)


Result for _inner_e8deb_00025:
  auc: 0.6175939321517945
  date: 2021-03-19_12-18-54
  done: true
  experiment_id: d14210d64f5b46989cd9612b93804ff0
  experiment_tag: 25_batch_size=32,eta=0.0,lr=5,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35821
  time_since_restore: 3421.402410507202
  time_this_iter_s: 3421.402410507202
  time_total_s: 3421.402410507202
  timestamp: 1616152734
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00025
  
[2m[36m(pid=4513)[0m Starting run with seed 0 - lr 5 - sec_lr 0.01 - bs 512
[2m[36m(pid=4513)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=4513)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=4513)[0m GPU available: False, used: False
[2m[36m(pid=4513)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=4513)[0m 
[2m[36m(pid=4513)[0m   | Name      | Type              | Params
[2m[36m(pid=4513)[0m ------------------------------------------------
[2m[36m(pid=4513)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=4513)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=4513)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=4513)[0m ------------------------------------------------
[2m[36m(pid=4513)[0m 8.7 K     Trainable params
[2m[36m(pid=4513)[0m 0         Non-trainable params
[2m[36m(pid=4513)[0m 8.7 K     Total params
[2m[36m(pid=4513)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=4513)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=4513)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=4513)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=4513)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=4513)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=663)[0m time to fit was 66.04001379013062
[2m[36m(pid=663)[0m GPU available: False, used: False
[2m[36m(pid=663)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=663)[0m 
[2m[36m(pid=663)[0m   | Name      | Type              | Params
[2m[36m(pid=663)[0m ------------------------------------------------
[2m[36m(pid=663)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=663)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=663)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=663)[0m ------------------------------------------------
[2m[36m(pid=663)[0m 8.7 K     Trainable params
[2m[36m(pid=663)[0m 0         Non-trainable params
[2m[36m(pid=663)[0m 8.7 K     Total params
[2m[36m(pid=35787)[0m time to fit was 566.3344485759735
[2m[36m(pid=35787)[0m GPU available: False, used: False
[2m[36m(pid=35787)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35787)[0m 
[2m[36m(pid=35787)[0m   | Name      | Type              | Params
[2m[36m(pid=35787)[0m ------------------------------------------------
[2m[36m(pid=35787)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35787)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35787)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35787)[0m ------------------------------------------------
[2m[36m(pid=35787)[0m 8.7 K     Trainable params
[2m[36m(pid=35787)[0m 0         Non-trainable params
[2m[36m(pid=35787)[0m 8.7 K     Total params
[2m[36m(pid=35777)[0m time to fit was 156.8931164741516
Result for _inner_e8deb_00047:
  auc: 0.9136356592178345
  date: 2021-03-19_12-19-51
  done: false
  experiment_id: 6104b7781d7d4cbba53c4ad318dc77dc
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35777
  time_since_restore: 941.7061870098114
  time_this_iter_s: 941.7061870098114
  time_total_s: 941.7061870098114
  timestamp: 1616152791
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00047
  
[2m[36m(pid=35777)[0m Finished run with seed 0 - lr 1 - sec_lr 0.01 - bs 128 - mean val auc: 0.9136356592178345
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 61/180 (1 PENDING, 26 RUNNING, 34 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00020 | RUNNING    |       |           32 |     0 | 2     |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00032 | RUNNING    |       |          128 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00036 | RUNNING    |       |           64 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00060 | PENDING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 41 more trials not shown (16 RUNNING, 24 TERMINATED)


Result for _inner_e8deb_00047:
  auc: 0.9136356592178345
  date: 2021-03-19_12-19-51
  done: true
  experiment_id: 6104b7781d7d4cbba53c4ad318dc77dc
  experiment_tag: 47_batch_size=128,eta=0.0,lr=1,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35777
  time_since_restore: 941.7061870098114
  time_this_iter_s: 941.7061870098114
  time_total_s: 941.7061870098114
  timestamp: 1616152791
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00047
  
2021-03-19 12:19:52,493	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff1e69661801000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=6688)[0m Starting run with seed 0 - lr 0.001 - sec_lr 0.1 - bs 32
[2m[36m(pid=6688)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=6688)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=6688)[0m GPU available: False, used: False
[2m[36m(pid=6688)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40518)[0m time to fit was 213.80488848686218
[2m[36m(pid=6688)[0m 
[2m[36m(pid=6688)[0m   | Name      | Type              | Params
[2m[36m(pid=6688)[0m ------------------------------------------------
[2m[36m(pid=6688)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=6688)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=6688)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=6688)[0m ------------------------------------------------
[2m[36m(pid=6688)[0m 8.7 K     Trainable params
[2m[36m(pid=6688)[0m 0         Non-trainable params
[2m[36m(pid=6688)[0m 8.7 K     Total params
[2m[36m(pid=6688)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=6688)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=40518)[0m GPU available: False, used: False
[2m[36m(pid=40518)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40518)[0m 
[2m[36m(pid=40518)[0m   | Name      | Type              | Params
[2m[36m(pid=40518)[0m ------------------------------------------------
[2m[36m(pid=40518)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40518)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40518)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40518)[0m ------------------------------------------------
[2m[36m(pid=40518)[0m 8.7 K     Trainable params
[2m[36m(pid=40518)[0m 0         Non-trainable params
[2m[36m(pid=40518)[0m 8.7 K     Total params
[2m[36m(pid=6688)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=6688)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=6688)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=6688)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=46501)[0m time to fit was 166.83294916152954
[2m[36m(pid=46501)[0m GPU available: False, used: False
[2m[36m(pid=46501)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=46501)[0m 
[2m[36m(pid=46501)[0m   | Name      | Type              | Params
[2m[36m(pid=46501)[0m ------------------------------------------------
[2m[36m(pid=46501)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=46501)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=46501)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=46501)[0m ------------------------------------------------
[2m[36m(pid=46501)[0m 8.7 K     Trainable params
[2m[36m(pid=46501)[0m 0         Non-trainable params
[2m[36m(pid=46501)[0m 8.7 K     Total params
[2m[36m(pid=663)[0m time to fit was 66.95887804031372
[2m[36m(pid=663)[0m GPU available: False, used: False
[2m[36m(pid=663)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=663)[0m 
[2m[36m(pid=663)[0m   | Name      | Type              | Params
[2m[36m(pid=663)[0m ------------------------------------------------
[2m[36m(pid=663)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=663)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=663)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=663)[0m ------------------------------------------------
[2m[36m(pid=663)[0m 8.7 K     Trainable params
[2m[36m(pid=663)[0m 0         Non-trainable params
[2m[36m(pid=663)[0m 8.7 K     Total params
[2m[36m(pid=4513)[0m time to fit was 76.36296963691711
[2m[36m(pid=4513)[0m GPU available: False, used: False
[2m[36m(pid=4513)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=4513)[0m 
[2m[36m(pid=4513)[0m   | Name      | Type              | Params
[2m[36m(pid=4513)[0m ------------------------------------------------
[2m[36m(pid=4513)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=4513)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=4513)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=4513)[0m ------------------------------------------------
[2m[36m(pid=4513)[0m 8.7 K     Trainable params
[2m[36m(pid=4513)[0m 0         Non-trainable params
[2m[36m(pid=4513)[0m 8.7 K     Total params
[2m[36m(pid=35803)[0m time to fit was 572.5372362136841
[2m[36m(pid=49332)[0m time to fit was 353.40777349472046
Result for _inner_e8deb_00020:
  auc: 0.8263137936592102
  date: 2021-03-19_12-20-34
  done: false
  experiment_id: d7bf4c4ec10948369384fa84fbe40b26
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35803
  time_since_restore: 3520.6793265342712
  time_this_iter_s: 3520.6793265342712
  time_total_s: 3520.6793265342712
  timestamp: 1616152834
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00020
  
[2m[36m(pid=35803)[0m Finished run with seed 0 - lr 2 - sec_lr 0.001 - bs 32 - mean val auc: 0.8263137936592102
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 62/180 (1 PENDING, 26 RUNNING, 35 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |                     |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00020 | RUNNING    | 145.101.32.82:35803 |           32 |     0 | 2     |    0.001 |      1 |         3520.68  | 0.826314 |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |                     |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00032 | RUNNING    |                     |          128 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |                     |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00036 | RUNNING    |                     |           64 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00061 | PENDING    |                     |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 42 more trials not shown (16 RUNNING, 25 TERMINATED)


Result for _inner_e8deb_00020:
  auc: 0.8263137936592102
  date: 2021-03-19_12-20-34
  done: true
  experiment_id: d7bf4c4ec10948369384fa84fbe40b26
  experiment_tag: 20_batch_size=32,eta=0.0,lr=2,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35803
  time_since_restore: 3520.6793265342712
  time_this_iter_s: 3520.6793265342712
  time_total_s: 3520.6793265342712
  timestamp: 1616152834
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00020
  
[2m[36m(pid=49332)[0m GPU available: False, used: False
[2m[36m(pid=49332)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49332)[0m 
[2m[36m(pid=49332)[0m   | Name      | Type              | Params
[2m[36m(pid=49332)[0m ------------------------------------------------
[2m[36m(pid=49332)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49332)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49332)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49332)[0m ------------------------------------------------
[2m[36m(pid=49332)[0m 8.7 K     Trainable params
[2m[36m(pid=49332)[0m 0         Non-trainable params
[2m[36m(pid=49332)[0m 8.7 K     Total params
[2m[36m(pid=8106)[0m Starting run with seed 0 - lr 0.001 - sec_lr 0.1 - bs 64
[2m[36m(pid=8106)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=8106)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=8106)[0m GPU available: False, used: False
[2m[36m(pid=8106)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8106)[0m 
[2m[36m(pid=8106)[0m   | Name      | Type              | Params
[2m[36m(pid=8106)[0m ------------------------------------------------
[2m[36m(pid=8106)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8106)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8106)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8106)[0m ------------------------------------------------
[2m[36m(pid=8106)[0m 8.7 K     Trainable params
[2m[36m(pid=8106)[0m 0         Non-trainable params
[2m[36m(pid=8106)[0m 8.7 K     Total params
[2m[36m(pid=8106)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8106)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8106)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8106)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8106)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=8106)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35776)[0m time to fit was 341.8099081516266
[2m[36m(pid=35776)[0m GPU available: False, used: False
[2m[36m(pid=35776)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35776)[0m 
[2m[36m(pid=35776)[0m   | Name      | Type              | Params
[2m[36m(pid=35776)[0m ------------------------------------------------
[2m[36m(pid=35776)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35776)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35776)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35776)[0m ------------------------------------------------
[2m[36m(pid=35776)[0m 8.7 K     Trainable params
[2m[36m(pid=35776)[0m 0         Non-trainable params
[2m[36m(pid=35776)[0m 8.7 K     Total params
[2m[36m(pid=40498)[0m time to fit was 273.10440492630005
[2m[36m(pid=40498)[0m GPU available: False, used: False
[2m[36m(pid=40498)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40498)[0m 
[2m[36m(pid=40498)[0m   | Name      | Type              | Params
[2m[36m(pid=40498)[0m ------------------------------------------------
[2m[36m(pid=40498)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40498)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40498)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40498)[0m ------------------------------------------------
[2m[36m(pid=40498)[0m 8.7 K     Trainable params
[2m[36m(pid=40498)[0m 0         Non-trainable params
[2m[36m(pid=40498)[0m 8.7 K     Total params
[2m[36m(pid=4513)[0m time to fit was 51.036131858825684
[2m[36m(pid=4513)[0m GPU available: False, used: False
[2m[36m(pid=4513)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=4513)[0m 
[2m[36m(pid=4513)[0m   | Name      | Type              | Params
[2m[36m(pid=4513)[0m ------------------------------------------------
[2m[36m(pid=4513)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=4513)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=4513)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=4513)[0m ------------------------------------------------
[2m[36m(pid=4513)[0m 8.7 K     Trainable params
[2m[36m(pid=4513)[0m 0         Non-trainable params
[2m[36m(pid=4513)[0m 8.7 K     Total params
[2m[36m(pid=46501)[0m time to fit was 76.51389026641846
Result for _inner_e8deb_00054:
  auc: 0.8373398423194885
  date: 2021-03-19_12-21-35
  done: false
  experiment_id: d1d0c9fd7992485d9f3d9e2e0fa1c63d
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 46501
  time_since_restore: 504.25887513160706
  time_this_iter_s: 504.25887513160706
  time_total_s: 504.25887513160706
  timestamp: 1616152895
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00054
  
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 63/180 (1 PENDING, 26 RUNNING, 36 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00032 | RUNNING    |       |          128 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00036 | RUNNING    |       |           64 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00037 | RUNNING    |       |          128 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00062 | PENDING    |       |          128 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 43 more trials not shown (16 RUNNING, 26 TERMINATED)


Result for _inner_e8deb_00054:
  auc: 0.8373398423194885
  date: 2021-03-19_12-21-35
  done: true
  experiment_id: d1d0c9fd7992485d9f3d9e2e0fa1c63d
  experiment_tag: 54_batch_size=512,eta=0.0,lr=2,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 46501
  time_since_restore: 504.25887513160706
  time_this_iter_s: 504.25887513160706
  time_total_s: 504.25887513160706
  timestamp: 1616152895
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00054
  
[2m[36m(pid=46501)[0m Finished run with seed 0 - lr 2 - sec_lr 0.01 - bs 512 - mean val auc: 0.8373398423194885
[2m[36m(pid=10108)[0m Starting run with seed 0 - lr 0.001 - sec_lr 0.1 - bs 128
[2m[36m(pid=10108)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10108)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=10108)[0m GPU available: False, used: False
[2m[36m(pid=10108)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10108)[0m 
[2m[36m(pid=10108)[0m   | Name      | Type              | Params
[2m[36m(pid=10108)[0m ------------------------------------------------
[2m[36m(pid=10108)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10108)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10108)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10108)[0m ------------------------------------------------
[2m[36m(pid=10108)[0m 8.7 K     Trainable params
[2m[36m(pid=10108)[0m 0         Non-trainable params
[2m[36m(pid=10108)[0m 8.7 K     Total params
[2m[36m(pid=10108)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=10108)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=10108)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=10108)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=10108)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=10108)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=4513)[0m time to fit was 43.29416084289551
[2m[36m(pid=4513)[0m GPU available: False, used: False
[2m[36m(pid=4513)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=4513)[0m 
[2m[36m(pid=4513)[0m   | Name      | Type              | Params
[2m[36m(pid=4513)[0m ------------------------------------------------
[2m[36m(pid=4513)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=4513)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=4513)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=4513)[0m ------------------------------------------------
[2m[36m(pid=4513)[0m 8.7 K     Trainable params
[2m[36m(pid=4513)[0m 0         Non-trainable params
[2m[36m(pid=4513)[0m 8.7 K     Total params
[2m[36m(pid=35790)[0m time to fit was 302.41418504714966
Result for _inner_e8deb_00041:
  auc: 0.913311767578125
  date: 2021-03-19_12-22-14
  done: false
  experiment_id: 3eacdd61eeb040e682e8e2c5f774b5d0
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35790
  time_since_restore: 1767.2689695358276
  time_this_iter_s: 1767.2689695358276
  time_total_s: 1767.2689695358276
  timestamp: 1616152934
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00041
  
[2m[36m(pid=35790)[0m Finished run with seed 0 - lr 0.1 - sec_lr 0.01 - bs 64 - mean val auc: 0.913311767578125
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 64/180 (1 PENDING, 26 RUNNING, 37 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00032 | RUNNING    |       |          128 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00036 | RUNNING    |       |           64 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00037 | RUNNING    |       |          128 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00063 | PENDING    |       |          256 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 44 more trials not shown (16 RUNNING, 27 TERMINATED)


Result for _inner_e8deb_00041:
  auc: 0.913311767578125
  date: 2021-03-19_12-22-14
  done: true
  experiment_id: 3eacdd61eeb040e682e8e2c5f774b5d0
  experiment_tag: 41_batch_size=64,eta=0.0,lr=0.1,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35790
  time_since_restore: 1767.2689695358276
  time_this_iter_s: 1767.2689695358276
  time_total_s: 1767.2689695358276
  timestamp: 1616152934
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00041
  
[2m[36m(pid=663)[0m time to fit was 123.4732928276062
[2m[36m(pid=11277)[0m Starting run with seed 0 - lr 0.001 - sec_lr 0.1 - bs 256
[2m[36m(pid=11277)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=11277)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=11277)[0m GPU available: False, used: False
[2m[36m(pid=11277)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=11277)[0m 
[2m[36m(pid=11277)[0m   | Name      | Type              | Params
[2m[36m(pid=11277)[0m ------------------------------------------------
[2m[36m(pid=11277)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=11277)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=11277)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=11277)[0m ------------------------------------------------
[2m[36m(pid=11277)[0m 8.7 K     Trainable params
[2m[36m(pid=11277)[0m 0         Non-trainable params
[2m[36m(pid=11277)[0m 8.7 K     Total params
[2m[36m(pid=11277)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=11277)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=663)[0m GPU available: False, used: False
[2m[36m(pid=663)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=663)[0m 
[2m[36m(pid=663)[0m   | Name      | Type              | Params
[2m[36m(pid=663)[0m ------------------------------------------------
[2m[36m(pid=663)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=663)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=663)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=663)[0m ------------------------------------------------
[2m[36m(pid=663)[0m 8.7 K     Trainable params
[2m[36m(pid=663)[0m 0         Non-trainable params
[2m[36m(pid=663)[0m 8.7 K     Total params
[2m[36m(pid=11277)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=11277)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=11277)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=11277)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=4513)[0m time to fit was 43.16131114959717
[2m[36m(pid=4513)[0m GPU available: False, used: False
[2m[36m(pid=4513)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=4513)[0m 
[2m[36m(pid=4513)[0m   | Name      | Type              | Params
[2m[36m(pid=4513)[0m ------------------------------------------------
[2m[36m(pid=4513)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=4513)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=4513)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=4513)[0m ------------------------------------------------
[2m[36m(pid=4513)[0m 8.7 K     Trainable params
[2m[36m(pid=4513)[0m 0         Non-trainable params
[2m[36m(pid=4513)[0m 8.7 K     Total params
[2m[36m(pid=40518)[0m time to fit was 160.15333700180054
[2m[36m(pid=40518)[0m GPU available: False, used: False
[2m[36m(pid=40518)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40518)[0m 
[2m[36m(pid=40518)[0m   | Name      | Type              | Params
[2m[36m(pid=40518)[0m ------------------------------------------------
[2m[36m(pid=40518)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40518)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40518)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40518)[0m ------------------------------------------------
[2m[36m(pid=40518)[0m 8.7 K     Trainable params
[2m[36m(pid=40518)[0m 0         Non-trainable params
[2m[36m(pid=40518)[0m 8.7 K     Total params
[2m[36m(pid=35785)[0m time to fit was 417.6688632965088
[2m[36m(pid=35785)[0m GPU available: False, used: False
[2m[36m(pid=35785)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35785)[0m 
[2m[36m(pid=35785)[0m   | Name      | Type              | Params
[2m[36m(pid=35785)[0m ------------------------------------------------
[2m[36m(pid=35785)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35785)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35785)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35785)[0m ------------------------------------------------
[2m[36m(pid=35785)[0m 8.7 K     Trainable params
[2m[36m(pid=35785)[0m 0         Non-trainable params
[2m[36m(pid=35785)[0m 8.7 K     Total params
[2m[36m(pid=4513)[0m time to fit was 42.7290563583374
Result for _inner_e8deb_00059:
  auc: 0.6044993996620178
  date: 2021-03-19_12-23-24
  done: false
  experiment_id: 294d85a7a2b74a2dbeda477dbc4cb60b
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 4513
  time_since_restore: 257.81182408332825
  time_this_iter_s: 257.81182408332825
  time_total_s: 257.81182408332825
  timestamp: 1616153004
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00059
  
[2m[36m(pid=4513)[0m Finished run with seed 0 - lr 5 - sec_lr 0.01 - bs 512 - mean val auc: 0.6044993996620178
== Status ==
Memory usage on this node: 9.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 65/180 (1 PENDING, 26 RUNNING, 38 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00032 | RUNNING    |       |          128 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00036 | RUNNING    |       |           64 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00037 | RUNNING    |       |          128 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00064 | PENDING    |       |          512 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 45 more trials not shown (16 RUNNING, 28 TERMINATED)


Result for _inner_e8deb_00059:
  auc: 0.6044993996620178
  date: 2021-03-19_12-23-24
  done: true
  experiment_id: 294d85a7a2b74a2dbeda477dbc4cb60b
  experiment_tag: 59_batch_size=512,eta=0.0,lr=5,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 4513
  time_since_restore: 257.81182408332825
  time_this_iter_s: 257.81182408332825
  time_total_s: 257.81182408332825
  timestamp: 1616153004
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00059
  
2021-03-19 12:23:25,291	WARNING util.py:142 -- The `start_trial` operation took 0.768 s, which may be a performance bottleneck.
[2m[36m(pid=13479)[0m Starting run with seed 0 - lr 0.001 - sec_lr 0.1 - bs 512
[2m[36m(pid=13479)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=13479)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35820)[0m time to fit was 483.6684844493866
[2m[36m(pid=13479)[0m GPU available: False, used: False
[2m[36m(pid=13479)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=13479)[0m 
[2m[36m(pid=13479)[0m   | Name      | Type              | Params
[2m[36m(pid=13479)[0m ------------------------------------------------
[2m[36m(pid=13479)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=13479)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=13479)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=13479)[0m ------------------------------------------------
[2m[36m(pid=13479)[0m 8.7 K     Trainable params
[2m[36m(pid=13479)[0m 0         Non-trainable params
[2m[36m(pid=13479)[0m 8.7 K     Total params
[2m[36m(pid=13479)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=13479)[0m   warnings.warn(*args, **kwargs)
Result for _inner_e8deb_00037:
  auc: 0.9123754620552063
  date: 2021-03-19_12-23-36
  done: false
  experiment_id: a6e60fcb361542ad8c851592f23a48cf
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35820
  time_since_restore: 2308.395033597946
  time_this_iter_s: 2308.395033597946
  time_total_s: 2308.395033597946
  timestamp: 1616153016
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00037
  
[2m[36m(pid=35820)[0m Finished run with seed 0 - lr 0.01 - sec_lr 0.01 - bs 128 - mean val auc: 0.9123754620552063
== Status ==
Memory usage on this node: 9.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 66/180 (1 PENDING, 26 RUNNING, 39 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |                     |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |                     |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00032 | RUNNING    |                     |          128 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |                     |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00036 | RUNNING    |                     |           64 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00037 | RUNNING    | 145.101.32.82:35820 |          128 |     0 | 0.01  |    0.01  |      1 |         2308.4   | 0.912375 |
| _inner_e8deb_00065 | PENDING    |                     |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 46 more trials not shown (16 RUNNING, 29 TERMINATED)


Result for _inner_e8deb_00037:
  auc: 0.9123754620552063
  date: 2021-03-19_12-23-36
  done: true
  experiment_id: a6e60fcb361542ad8c851592f23a48cf
  experiment_tag: 37_batch_size=128,eta=0.0,lr=0.01,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35820
  time_since_restore: 2308.395033597946
  time_this_iter_s: 2308.395033597946
  time_total_s: 2308.395033597946
  timestamp: 1616153016
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00037
  
[2m[36m(pid=13479)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=13479)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=13479)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=13479)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=13798)[0m Starting run with seed 0 - lr 0.01 - sec_lr 0.1 - bs 32
[2m[36m(pid=13798)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=13798)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=13798)[0m GPU available: False, used: False
[2m[36m(pid=13798)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=13798)[0m 
[2m[36m(pid=13798)[0m   | Name      | Type              | Params
[2m[36m(pid=13798)[0m ------------------------------------------------
[2m[36m(pid=13798)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=13798)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=13798)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=13798)[0m ------------------------------------------------
[2m[36m(pid=13798)[0m 8.7 K     Trainable params
[2m[36m(pid=13798)[0m 0         Non-trainable params
[2m[36m(pid=13798)[0m 8.7 K     Total params
[2m[36m(pid=13798)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=13798)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=13798)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=13798)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=13798)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=13798)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35861)[0m time to fit was 1803.1952483654022
[2m[36m(pid=35861)[0m GPU available: False, used: False
[2m[36m(pid=35861)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35861)[0m 
[2m[36m(pid=35861)[0m   | Name      | Type              | Params
[2m[36m(pid=35861)[0m ------------------------------------------------
[2m[36m(pid=35861)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35861)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35861)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35861)[0m ------------------------------------------------
[2m[36m(pid=35861)[0m 8.7 K     Trainable params
[2m[36m(pid=35861)[0m 0         Non-trainable params
[2m[36m(pid=35861)[0m 8.7 K     Total params
[2m[36m(pid=35786)[0m time to fit was 750.5878159999847
[2m[36m(pid=35786)[0m GPU available: False, used: False
[2m[36m(pid=35786)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35786)[0m 
[2m[36m(pid=35786)[0m   | Name      | Type              | Params
[2m[36m(pid=35786)[0m ------------------------------------------------
[2m[36m(pid=35786)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35786)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35786)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35786)[0m ------------------------------------------------
[2m[36m(pid=35786)[0m 8.7 K     Trainable params
[2m[36m(pid=35786)[0m 0         Non-trainable params
[2m[36m(pid=35786)[0m 8.7 K     Total params
[2m[36m(pid=40498)[0m time to fit was 221.04247641563416
[2m[36m(pid=40498)[0m GPU available: False, used: False
[2m[36m(pid=40498)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40498)[0m 
[2m[36m(pid=40498)[0m   | Name      | Type              | Params
[2m[36m(pid=40498)[0m ------------------------------------------------
[2m[36m(pid=40498)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40498)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40498)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40498)[0m ------------------------------------------------
[2m[36m(pid=40498)[0m 8.7 K     Trainable params
[2m[36m(pid=40498)[0m 0         Non-trainable params
[2m[36m(pid=40498)[0m 8.7 K     Total params
[2m[36m(pid=40518)[0m time to fit was 166.05660343170166
Result for _inner_e8deb_00053:
  auc: 0.9116452097892761
  date: 2021-03-19_12-25-28
  done: false
  experiment_id: f5848cf59f994600958ca71822a809bb
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 40518
  time_since_restore: 906.3834419250488
  time_this_iter_s: 906.3834419250488
  time_total_s: 906.3834419250488
  timestamp: 1616153128
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00053
  
[2m[36m(pid=40518)[0m Finished run with seed 0 - lr 2 - sec_lr 0.01 - bs 256 - mean val auc: 0.9116452097892761
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 67/180 (1 PENDING, 26 RUNNING, 40 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |       |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00032 | RUNNING    |       |          128 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00036 | RUNNING    |       |           64 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00040 | RUNNING    |       |           32 |     0 | 0.1   |    0.01  |        |                  |          |
| _inner_e8deb_00066 | PENDING    |       |           64 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 47 more trials not shown (16 RUNNING, 30 TERMINATED)


Result for _inner_e8deb_00053:
  auc: 0.9116452097892761
  date: 2021-03-19_12-25-28
  done: true
  experiment_id: f5848cf59f994600958ca71822a809bb
  experiment_tag: 53_batch_size=256,eta=0.0,lr=2,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 40518
  time_since_restore: 906.3834419250488
  time_this_iter_s: 906.3834419250488
  time_total_s: 906.3834419250488
  timestamp: 1616153128
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00053
  
[2m[36m(pid=17269)[0m Starting run with seed 0 - lr 0.01 - sec_lr 0.1 - bs 64
[2m[36m(pid=17269)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=17269)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=17269)[0m GPU available: False, used: False
[2m[36m(pid=17269)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=17269)[0m 
[2m[36m(pid=17269)[0m   | Name      | Type              | Params
[2m[36m(pid=17269)[0m ------------------------------------------------
[2m[36m(pid=17269)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=17269)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=17269)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=17269)[0m ------------------------------------------------
[2m[36m(pid=17269)[0m 8.7 K     Trainable params
[2m[36m(pid=17269)[0m 0         Non-trainable params
[2m[36m(pid=17269)[0m 8.7 K     Total params
[2m[36m(pid=17269)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=17269)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=17269)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=17269)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=17269)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=17269)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49332)[0m time to fit was 320.7299289703369
[2m[36m(pid=49332)[0m GPU available: False, used: False
[2m[36m(pid=49332)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49332)[0m 
[2m[36m(pid=49332)[0m   | Name      | Type              | Params
[2m[36m(pid=49332)[0m ------------------------------------------------
[2m[36m(pid=49332)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49332)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49332)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49332)[0m ------------------------------------------------
[2m[36m(pid=49332)[0m 8.7 K     Trainable params
[2m[36m(pid=49332)[0m 0         Non-trainable params
[2m[36m(pid=49332)[0m 8.7 K     Total params
[2m[36m(pid=35823)[0m time to fit was 535.3975570201874
Result for _inner_e8deb_00032:
  auc: 0.9046417474746704
  date: 2021-03-19_12-26-11
  done: false
  experiment_id: 827ebb45de9d450aa19cb8b55558db21
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35823
  time_since_restore: 2716.5184273719788
  time_this_iter_s: 2716.5184273719788
  time_total_s: 2716.5184273719788
  timestamp: 1616153171
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00032
  
[2m[36m(pid=35823)[0m Finished run with seed 0 - lr 0.001 - sec_lr 0.01 - bs 128 - mean val auc: 0.9046417474746704
== Status ==
Memory usage on this node: 9.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 68/180 (1 PENDING, 26 RUNNING, 41 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    |                     |           64 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |                     |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00032 | RUNNING    | 145.101.32.82:35823 |          128 |     0 | 0.001 |    0.01  |      1 |         2716.52  | 0.904642 |
| _inner_e8deb_00035 | RUNNING    |                     |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00036 | RUNNING    |                     |           64 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00040 | RUNNING    |                     |           32 |     0 | 0.1   |    0.01  |        |                  |          |
| _inner_e8deb_00067 | PENDING    |                     |          128 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 48 more trials not shown (16 RUNNING, 31 TERMINATED)


Result for _inner_e8deb_00032:
  auc: 0.9046417474746704
  date: 2021-03-19_12-26-11
  done: true
  experiment_id: 827ebb45de9d450aa19cb8b55558db21
  experiment_tag: 32_batch_size=128,eta=0.0,lr=0.001,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35823
  time_since_restore: 2716.5184273719788
  time_this_iter_s: 2716.5184273719788
  time_total_s: 2716.5184273719788
  timestamp: 1616153171
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00032
  
2021-03-19 12:26:13,585	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff010cbd3701000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=35878)[0m time to fit was 680.5042746067047
Result for _inner_e8deb_00006:
  auc: 0.9126617193222046
  date: 2021-03-19_12-26-17
  done: false
  experiment_id: 176fa0c55bab43b0a915d2c8124f0b92
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35878
  time_since_restore: 3864.7560999393463
  time_this_iter_s: 3864.7560999393463
  time_total_s: 3864.7560999393463
  timestamp: 1616153177
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00006
  
== Status ==
Memory usage on this node: 9.6/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 69/180 (1 PENDING, 26 RUNNING, 42 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00006 | RUNNING    | 145.101.32.82:35878 |           64 |     0 | 0.01  |    0.001 |      1 |         3864.76  | 0.912662 |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |                     |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |                     |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00036 | RUNNING    |                     |           64 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00040 | RUNNING    |                     |           32 |     0 | 0.1   |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |                     |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00068 | PENDING    |                     |          256 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 49 more trials not shown (16 RUNNING, 32 TERMINATED)


Result for _inner_e8deb_00006:
  auc: 0.9126617193222046
  date: 2021-03-19_12-26-17
  done: true
  experiment_id: 176fa0c55bab43b0a915d2c8124f0b92
  experiment_tag: 6_batch_size=64,eta=0.0,lr=0.01,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35878
  time_since_restore: 3864.7560999393463
  time_this_iter_s: 3864.7560999393463
  time_total_s: 3864.7560999393463
  timestamp: 1616153177
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00006
  
[2m[36m(pid=35878)[0m Finished run with seed 0 - lr 0.01 - sec_lr 0.001 - bs 64 - mean val auc: 0.9126617193222046
[2m[36m(pid=18484)[0m Starting run with seed 0 - lr 0.01 - sec_lr 0.1 - bs 128
[2m[36m(pid=18484)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=18484)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=18484)[0m GPU available: False, used: False
[2m[36m(pid=18484)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=18484)[0m 
[2m[36m(pid=18484)[0m   | Name      | Type              | Params
[2m[36m(pid=18484)[0m ------------------------------------------------
[2m[36m(pid=18484)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=18484)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=18484)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=18484)[0m ------------------------------------------------
[2m[36m(pid=18484)[0m 8.7 K     Trainable params
[2m[36m(pid=18484)[0m 0         Non-trainable params
[2m[36m(pid=18484)[0m 8.7 K     Total params
[2m[36m(pid=18484)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=18484)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=18484)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=18484)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=18484)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=18484)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=18718)[0m Starting run with seed 0 - lr 0.01 - sec_lr 0.1 - bs 256
[2m[36m(pid=18718)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=18718)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=18718)[0m GPU available: False, used: False
[2m[36m(pid=18718)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=18718)[0m 
[2m[36m(pid=18718)[0m   | Name      | Type              | Params
[2m[36m(pid=18718)[0m ------------------------------------------------
[2m[36m(pid=18718)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=18718)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=18718)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=18718)[0m ------------------------------------------------
[2m[36m(pid=18718)[0m 8.7 K     Trainable params
[2m[36m(pid=18718)[0m 0         Non-trainable params
[2m[36m(pid=18718)[0m 8.7 K     Total params
[2m[36m(pid=18718)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=18718)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=18718)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=18718)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=18718)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=18718)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35776)[0m time to fit was 363.0161876678467
[2m[36m(pid=35776)[0m GPU available: False, used: False
[2m[36m(pid=35776)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35776)[0m 
[2m[36m(pid=35776)[0m   | Name      | Type              | Params
[2m[36m(pid=35776)[0m ------------------------------------------------
[2m[36m(pid=35776)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35776)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35776)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35776)[0m ------------------------------------------------
[2m[36m(pid=35776)[0m 8.7 K     Trainable params
[2m[36m(pid=35776)[0m 0         Non-trainable params
[2m[36m(pid=35776)[0m 8.7 K     Total params
[2m[36m(pid=35818)[0m time to fit was 954.7414667606354
[2m[36m(pid=35818)[0m GPU available: False, used: False
[2m[36m(pid=35818)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35818)[0m 
[2m[36m(pid=35818)[0m   | Name      | Type              | Params
[2m[36m(pid=35818)[0m ------------------------------------------------
[2m[36m(pid=35818)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35818)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35818)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35818)[0m ------------------------------------------------
[2m[36m(pid=35818)[0m 8.7 K     Trainable params
[2m[36m(pid=35818)[0m 0         Non-trainable params
[2m[36m(pid=35818)[0m 8.7 K     Total params
[2m[36m(pid=52234)[0m time to fit was 524.8094351291656
[2m[36m(pid=52234)[0m GPU available: False, used: False
[2m[36m(pid=52234)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=52234)[0m 
[2m[36m(pid=52234)[0m   | Name      | Type              | Params
[2m[36m(pid=52234)[0m ------------------------------------------------
[2m[36m(pid=52234)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=52234)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=52234)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=52234)[0m ------------------------------------------------
[2m[36m(pid=52234)[0m 8.7 K     Trainable params
[2m[36m(pid=52234)[0m 0         Non-trainable params
[2m[36m(pid=52234)[0m 8.7 K     Total params
[2m[36m(pid=13479)[0m time to fit was 219.9630537033081
[2m[36m(pid=13479)[0m GPU available: False, used: False
[2m[36m(pid=13479)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35806)[0m time to fit was 521.0383059978485
[2m[36m(pid=13479)[0m 
[2m[36m(pid=13479)[0m   | Name      | Type              | Params
[2m[36m(pid=13479)[0m ------------------------------------------------
[2m[36m(pid=13479)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=13479)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=13479)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=13479)[0m ------------------------------------------------
[2m[36m(pid=13479)[0m 8.7 K     Trainable params
[2m[36m(pid=13479)[0m 0         Non-trainable params
[2m[36m(pid=13479)[0m 8.7 K     Total params
[2m[36m(pid=35806)[0m GPU available: False, used: False
[2m[36m(pid=35806)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35806)[0m 
[2m[36m(pid=35806)[0m   | Name      | Type              | Params
[2m[36m(pid=35806)[0m ------------------------------------------------
[2m[36m(pid=35806)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35806)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35806)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35806)[0m ------------------------------------------------
[2m[36m(pid=35806)[0m 8.7 K     Trainable params
[2m[36m(pid=35806)[0m 0         Non-trainable params
[2m[36m(pid=35806)[0m 8.7 K     Total params
[2m[36m(pid=35857)[0m time to fit was 942.345972776413
[2m[36m(pid=35857)[0m GPU available: False, used: False
[2m[36m(pid=35857)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35857)[0m 
[2m[36m(pid=35857)[0m   | Name      | Type              | Params
[2m[36m(pid=35857)[0m ------------------------------------------------
[2m[36m(pid=35857)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35857)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35857)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35857)[0m ------------------------------------------------
[2m[36m(pid=35857)[0m 8.7 K     Trainable params
[2m[36m(pid=35857)[0m 0         Non-trainable params
[2m[36m(pid=35857)[0m 8.7 K     Total params
[2m[36m(pid=11277)[0m time to fit was 316.99580907821655
[2m[36m(pid=11277)[0m GPU available: False, used: False
[2m[36m(pid=11277)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=11277)[0m 
[2m[36m(pid=11277)[0m   | Name      | Type              | Params
[2m[36m(pid=11277)[0m ------------------------------------------------
[2m[36m(pid=11277)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=11277)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=11277)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=11277)[0m ------------------------------------------------
[2m[36m(pid=11277)[0m 8.7 K     Trainable params
[2m[36m(pid=11277)[0m 0         Non-trainable params
[2m[36m(pid=11277)[0m 8.7 K     Total params
[2m[36m(pid=663)[0m time to fit was 320.65219259262085
Result for _inner_e8deb_00058:
  auc: 0.6966688990592956
  date: 2021-03-19_12-27-46
  done: false
  experiment_id: 3743bf17686d4924948737913df505da
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 663
  time_since_restore: 643.4695336818695
  time_this_iter_s: 643.4695336818695
  time_total_s: 643.4695336818695
  timestamp: 1616153266
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00058
  
[2m[36m(pid=663)[0m Finished run with seed 0 - lr 5 - sec_lr 0.01 - bs 256 - mean val auc: 0.6966688990592956
== Status ==
Memory usage on this node: 9.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 70/180 (1 PENDING, 26 RUNNING, 43 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00036 | RUNNING    |       |           64 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00040 | RUNNING    |       |           32 |     0 | 0.1   |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |       |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00046 | RUNNING    |       |           64 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00069 | PENDING    |       |          512 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |          1197.15 | 0.9138   |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 50 more trials not shown (16 RUNNING, 33 TERMINATED)


Result for _inner_e8deb_00058:
  auc: 0.6966688990592956
  date: 2021-03-19_12-27-46
  done: true
  experiment_id: 3743bf17686d4924948737913df505da
  experiment_tag: 58_batch_size=256,eta=0.0,lr=5,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 663
  time_since_restore: 643.4695336818695
  time_this_iter_s: 643.4695336818695
  time_total_s: 643.4695336818695
  timestamp: 1616153266
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00058
  
[2m[36m(pid=21447)[0m Starting run with seed 0 - lr 0.01 - sec_lr 0.1 - bs 512
[2m[36m(pid=21447)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21447)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=21447)[0m GPU available: False, used: False
[2m[36m(pid=21447)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=21447)[0m 
[2m[36m(pid=21447)[0m   | Name      | Type              | Params
[2m[36m(pid=21447)[0m ------------------------------------------------
[2m[36m(pid=21447)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=21447)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=21447)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=21447)[0m ------------------------------------------------
[2m[36m(pid=21447)[0m 8.7 K     Trainable params
[2m[36m(pid=21447)[0m 0         Non-trainable params
[2m[36m(pid=21447)[0m 8.7 K     Total params
[2m[36m(pid=21447)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=21447)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35783)[0m time to fit was 745.4607679843903
[2m[36m(pid=35783)[0m GPU available: False, used: False
[2m[36m(pid=35783)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35783)[0m 
[2m[36m(pid=35783)[0m   | Name      | Type              | Params
[2m[36m(pid=35783)[0m ------------------------------------------------
[2m[36m(pid=35783)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35783)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35783)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35783)[0m ------------------------------------------------
[2m[36m(pid=35783)[0m 8.7 K     Trainable params
[2m[36m(pid=35783)[0m 0         Non-trainable params
[2m[36m(pid=35783)[0m 8.7 K     Total params
[2m[36m(pid=21447)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=21447)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=21447)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=21447)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35785)[0m time to fit was 304.27139472961426
[2m[36m(pid=35785)[0m GPU available: False, used: False
[2m[36m(pid=35785)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35785)[0m 
[2m[36m(pid=35785)[0m   | Name      | Type              | Params
[2m[36m(pid=35785)[0m ------------------------------------------------
[2m[36m(pid=35785)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35785)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35785)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35785)[0m ------------------------------------------------
[2m[36m(pid=35785)[0m 8.7 K     Trainable params
[2m[36m(pid=35785)[0m 0         Non-trainable params
[2m[36m(pid=35785)[0m 8.7 K     Total params
[2m[36m(pid=40498)[0m time to fit was 241.75572419166565
Result for _inner_e8deb_00052:
  auc: 0.8288928151130677
  date: 2021-03-19_12-28-53
  done: false
  experiment_id: 2bc7060f1f3248a8b12d7898fceb6cd8
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 40498
  time_since_restore: 1140.7299966812134
  time_this_iter_s: 1140.7299966812134
  time_total_s: 1140.7299966812134
  timestamp: 1616153333
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00052
  
[2m[36m(pid=40498)[0m Finished run with seed 0 - lr 2 - sec_lr 0.01 - bs 128 - mean val auc: 0.8288928151130677
== Status ==
Memory usage on this node: 9.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 71/180 (1 PENDING, 26 RUNNING, 44 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00036 | RUNNING    |       |           64 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00040 | RUNNING    |       |           32 |     0 | 0.1   |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |       |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00046 | RUNNING    |       |           64 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00070 | PENDING    |       |           32 |     0 | 0.1   |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |          1197.15 | 0.9138   |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 51 more trials not shown (16 RUNNING, 34 TERMINATED)


Result for _inner_e8deb_00052:
  auc: 0.8288928151130677
  date: 2021-03-19_12-28-53
  done: true
  experiment_id: 2bc7060f1f3248a8b12d7898fceb6cd8
  experiment_tag: 52_batch_size=128,eta=0.0,lr=2,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 40498
  time_since_restore: 1140.7299966812134
  time_this_iter_s: 1140.7299966812134
  time_total_s: 1140.7299966812134
  timestamp: 1616153333
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00052
  
2021-03-19 12:28:53,996	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff94ffcb6501000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=23585)[0m Starting run with seed 0 - lr 0.1 - sec_lr 0.1 - bs 32
[2m[36m(pid=23585)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=23585)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=23585)[0m GPU available: False, used: False
[2m[36m(pid=23585)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23585)[0m 
[2m[36m(pid=23585)[0m   | Name      | Type              | Params
[2m[36m(pid=23585)[0m ------------------------------------------------
[2m[36m(pid=23585)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23585)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23585)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23585)[0m ------------------------------------------------
[2m[36m(pid=23585)[0m 8.7 K     Trainable params
[2m[36m(pid=23585)[0m 0         Non-trainable params
[2m[36m(pid=23585)[0m 8.7 K     Total params
[2m[36m(pid=23585)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=23585)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23585)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=23585)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23585)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=23585)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=18718)[0m time to fit was 159.0800108909607
[2m[36m(pid=18718)[0m GPU available: False, used: False
[2m[36m(pid=18718)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=18718)[0m 
[2m[36m(pid=18718)[0m   | Name      | Type              | Params
[2m[36m(pid=18718)[0m ------------------------------------------------
[2m[36m(pid=18718)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=18718)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=18718)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=18718)[0m ------------------------------------------------
[2m[36m(pid=18718)[0m 8.7 K     Trainable params
[2m[36m(pid=18718)[0m 0         Non-trainable params
[2m[36m(pid=18718)[0m 8.7 K     Total params
[2m[36m(pid=35787)[0m time to fit was 602.9765207767487
[2m[36m(pid=35787)[0m GPU available: False, used: False
[2m[36m(pid=35787)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35787)[0m 
[2m[36m(pid=35787)[0m   | Name      | Type              | Params
[2m[36m(pid=35787)[0m ------------------------------------------------
[2m[36m(pid=35787)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35787)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35787)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35787)[0m ------------------------------------------------
[2m[36m(pid=35787)[0m 8.7 K     Trainable params
[2m[36m(pid=35787)[0m 0         Non-trainable params
[2m[36m(pid=35787)[0m 8.7 K     Total params
[2m[36m(pid=18484)[0m time to fit was 198.8882565498352
[2m[36m(pid=18484)[0m GPU available: False, used: False
[2m[36m(pid=18484)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=18484)[0m 
[2m[36m(pid=18484)[0m   | Name      | Type              | Params
[2m[36m(pid=18484)[0m ------------------------------------------------
[2m[36m(pid=18484)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=18484)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=18484)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=18484)[0m ------------------------------------------------
[2m[36m(pid=18484)[0m 8.7 K     Trainable params
[2m[36m(pid=18484)[0m 0         Non-trainable params
[2m[36m(pid=18484)[0m 8.7 K     Total params
[2m[36m(pid=35802)[0m time to fit was 1403.6486690044403
[2m[36m(pid=35802)[0m GPU available: False, used: False
[2m[36m(pid=35802)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35802)[0m 
[2m[36m(pid=35802)[0m   | Name      | Type              | Params
[2m[36m(pid=35802)[0m ------------------------------------------------
[2m[36m(pid=35802)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35802)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35802)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35802)[0m ------------------------------------------------
[2m[36m(pid=35802)[0m 8.7 K     Trainable params
[2m[36m(pid=35802)[0m 0         Non-trainable params
[2m[36m(pid=35802)[0m 8.7 K     Total params
[2m[36m(pid=21447)[0m time to fit was 151.43584561347961
[2m[36m(pid=21447)[0m GPU available: False, used: False
[2m[36m(pid=21447)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=21447)[0m 
[2m[36m(pid=21447)[0m   | Name      | Type              | Params
[2m[36m(pid=21447)[0m ------------------------------------------------
[2m[36m(pid=21447)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=21447)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=21447)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=21447)[0m ------------------------------------------------
[2m[36m(pid=21447)[0m 8.7 K     Trainable params
[2m[36m(pid=21447)[0m 0         Non-trainable params
[2m[36m(pid=21447)[0m 8.7 K     Total params
[2m[36m(pid=10108)[0m time to fit was 527.6268985271454
[2m[36m(pid=10108)[0m GPU available: False, used: False
[2m[36m(pid=10108)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10108)[0m 
[2m[36m(pid=10108)[0m   | Name      | Type              | Params
[2m[36m(pid=10108)[0m ------------------------------------------------
[2m[36m(pid=10108)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10108)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10108)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10108)[0m ------------------------------------------------
[2m[36m(pid=10108)[0m 8.7 K     Trainable params
[2m[36m(pid=10108)[0m 0         Non-trainable params
[2m[36m(pid=10108)[0m 8.7 K     Total params
[2m[36m(pid=13479)[0m time to fit was 221.6361198425293
[2m[36m(pid=13479)[0m GPU available: False, used: False
[2m[36m(pid=13479)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=13479)[0m 
[2m[36m(pid=13479)[0m   | Name      | Type              | Params
[2m[36m(pid=13479)[0m ------------------------------------------------
[2m[36m(pid=13479)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=13479)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=13479)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=13479)[0m ------------------------------------------------
[2m[36m(pid=13479)[0m 8.7 K     Trainable params
[2m[36m(pid=13479)[0m 0         Non-trainable params
[2m[36m(pid=13479)[0m 8.7 K     Total params
[2m[36m(pid=17269)[0m time to fit was 370.9997627735138
[2m[36m(pid=17269)[0m GPU available: False, used: False
[2m[36m(pid=17269)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=17269)[0m 
[2m[36m(pid=17269)[0m   | Name      | Type              | Params
[2m[36m(pid=17269)[0m ------------------------------------------------
[2m[36m(pid=17269)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=17269)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=17269)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=17269)[0m ------------------------------------------------
[2m[36m(pid=17269)[0m 8.7 K     Trainable params
[2m[36m(pid=17269)[0m 0         Non-trainable params
[2m[36m(pid=17269)[0m 8.7 K     Total params
[2m[36m(pid=52234)[0m time to fit was 304.5851094722748
[2m[36m(pid=52234)[0m GPU available: False, used: False
[2m[36m(pid=52234)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=52234)[0m 
[2m[36m(pid=52234)[0m   | Name      | Type              | Params
[2m[36m(pid=52234)[0m ------------------------------------------------
[2m[36m(pid=52234)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=52234)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=52234)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=52234)[0m ------------------------------------------------
[2m[36m(pid=52234)[0m 8.7 K     Trainable params
[2m[36m(pid=52234)[0m 0         Non-trainable params
[2m[36m(pid=52234)[0m 8.7 K     Total params
[2m[36m(pid=13798)[0m time to fit was 530.221658706665
[2m[36m(pid=13798)[0m GPU available: False, used: False
[2m[36m(pid=13798)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=13798)[0m 
[2m[36m(pid=13798)[0m   | Name      | Type              | Params
[2m[36m(pid=13798)[0m ------------------------------------------------
[2m[36m(pid=13798)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=13798)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=13798)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=13798)[0m ------------------------------------------------
[2m[36m(pid=13798)[0m 8.7 K     Trainable params
[2m[36m(pid=13798)[0m 0         Non-trainable params
[2m[36m(pid=13798)[0m 8.7 K     Total params
[2m[36m(pid=11277)[0m time to fit was 318.2613112926483
[2m[36m(pid=11277)[0m GPU available: False, used: False
[2m[36m(pid=11277)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=11277)[0m 
[2m[36m(pid=11277)[0m   | Name      | Type              | Params
[2m[36m(pid=11277)[0m ------------------------------------------------
[2m[36m(pid=11277)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=11277)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=11277)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=11277)[0m ------------------------------------------------
[2m[36m(pid=11277)[0m 8.7 K     Trainable params
[2m[36m(pid=11277)[0m 0         Non-trainable params
[2m[36m(pid=11277)[0m 8.7 K     Total params
[2m[36m(pid=35776)[0m time to fit was 360.8679711818695
[2m[36m(pid=35776)[0m GPU available: False, used: False
[2m[36m(pid=35776)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35776)[0m 
[2m[36m(pid=35776)[0m   | Name      | Type              | Params
[2m[36m(pid=35776)[0m ------------------------------------------------
[2m[36m(pid=35776)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35776)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35776)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35776)[0m ------------------------------------------------
[2m[36m(pid=35776)[0m 8.7 K     Trainable params
[2m[36m(pid=35776)[0m 0         Non-trainable params
[2m[36m(pid=35776)[0m 8.7 K     Total params
[2m[36m(pid=35785)[0m time to fit was 303.8989806175232
Result for _inner_e8deb_00046:
  auc: 0.9132384181022644
  date: 2021-03-19_12-33-31
  done: false
  experiment_id: 8ddc3f26cc1f4f8b89d266a3e0eeb7cc
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35785
  time_since_restore: 1805.7961649894714
  time_this_iter_s: 1805.7961649894714
  time_total_s: 1805.7961649894714
  timestamp: 1616153611
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00046
  
[2m[36m(pid=35785)[0m Finished run with seed 0 - lr 1 - sec_lr 0.01 - bs 64 - mean val auc: 0.9132384181022644
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 72/180 (1 PENDING, 26 RUNNING, 45 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |                     |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |                     |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00036 | RUNNING    |                     |           64 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00040 | RUNNING    |                     |           32 |     0 | 0.1   |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |                     |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00046 | RUNNING    | 145.101.32.82:35785 |           64 |     0 | 1     |    0.01  |      1 |          1805.8  | 0.913238 |
| _inner_e8deb_00071 | PENDING    |                     |           64 |     0 | 0.1   |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |          1197.15 | 0.9138   |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 52 more trials not shown (16 RUNNING, 35 TERMINATED)


Result for _inner_e8deb_00046:
  auc: 0.9132384181022644
  date: 2021-03-19_12-33-31
  done: true
  experiment_id: 8ddc3f26cc1f4f8b89d266a3e0eeb7cc
  experiment_tag: 46_batch_size=64,eta=0.0,lr=1,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35785
  time_since_restore: 1805.7961649894714
  time_this_iter_s: 1805.7961649894714
  time_total_s: 1805.7961649894714
  timestamp: 1616153611
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00046
  
[2m[36m(pid=18718)[0m time to fit was 275.39355659484863
[2m[36m(pid=32133)[0m Starting run with seed 0 - lr 0.1 - sec_lr 0.1 - bs 64
[2m[36m(pid=32133)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=32133)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=32133)[0m GPU available: False, used: False
[2m[36m(pid=32133)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=18718)[0m GPU available: False, used: False
[2m[36m(pid=18718)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=32133)[0m 
[2m[36m(pid=32133)[0m   | Name      | Type              | Params
[2m[36m(pid=32133)[0m ------------------------------------------------
[2m[36m(pid=32133)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=32133)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=32133)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=32133)[0m ------------------------------------------------
[2m[36m(pid=32133)[0m 8.7 K     Trainable params
[2m[36m(pid=32133)[0m 0         Non-trainable params
[2m[36m(pid=32133)[0m 8.7 K     Total params
[2m[36m(pid=18718)[0m 
[2m[36m(pid=18718)[0m   | Name      | Type              | Params
[2m[36m(pid=18718)[0m ------------------------------------------------
[2m[36m(pid=18718)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=18718)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=18718)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=18718)[0m ------------------------------------------------
[2m[36m(pid=18718)[0m 8.7 K     Trainable params
[2m[36m(pid=18718)[0m 0         Non-trainable params
[2m[36m(pid=18718)[0m 8.7 K     Total params
[2m[36m(pid=32133)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=32133)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=32133)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=32133)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=32133)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=32133)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=21447)[0m time to fit was 219.46882963180542
[2m[36m(pid=21447)[0m GPU available: False, used: False
[2m[36m(pid=21447)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=21447)[0m 
[2m[36m(pid=21447)[0m   | Name      | Type              | Params
[2m[36m(pid=21447)[0m ------------------------------------------------
[2m[36m(pid=21447)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=21447)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=21447)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=21447)[0m ------------------------------------------------
[2m[36m(pid=21447)[0m 8.7 K     Trainable params
[2m[36m(pid=21447)[0m 0         Non-trainable params
[2m[36m(pid=21447)[0m 8.7 K     Total params
[2m[36m(pid=13479)[0m time to fit was 218.54058718681335
[2m[36m(pid=13479)[0m GPU available: False, used: False
[2m[36m(pid=13479)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=13479)[0m 
[2m[36m(pid=13479)[0m   | Name      | Type              | Params
[2m[36m(pid=13479)[0m ------------------------------------------------
[2m[36m(pid=13479)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=13479)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=13479)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=13479)[0m ------------------------------------------------
[2m[36m(pid=13479)[0m 8.7 K     Trainable params
[2m[36m(pid=13479)[0m 0         Non-trainable params
[2m[36m(pid=13479)[0m 8.7 K     Total params
[2m[36m(pid=18484)[0m time to fit was 304.5266456604004
[2m[36m(pid=18484)[0m GPU available: False, used: False
[2m[36m(pid=18484)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=18484)[0m 
[2m[36m(pid=18484)[0m   | Name      | Type              | Params
[2m[36m(pid=18484)[0m ------------------------------------------------
[2m[36m(pid=18484)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=18484)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=18484)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=18484)[0m ------------------------------------------------
[2m[36m(pid=18484)[0m 8.7 K     Trainable params
[2m[36m(pid=18484)[0m 0         Non-trainable params
[2m[36m(pid=18484)[0m 8.7 K     Total params
[2m[36m(pid=35838)[0m time to fit was 1770.6698246002197
[2m[36m(pid=35838)[0m GPU available: False, used: False
[2m[36m(pid=35838)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35838)[0m 
[2m[36m(pid=35838)[0m   | Name      | Type              | Params
[2m[36m(pid=35838)[0m ------------------------------------------------
[2m[36m(pid=35838)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35838)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35838)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35838)[0m ------------------------------------------------
[2m[36m(pid=35838)[0m 8.7 K     Trainable params
[2m[36m(pid=35838)[0m 0         Non-trainable params
[2m[36m(pid=35838)[0m 8.7 K     Total params
[2m[36m(pid=49332)[0m time to fit was 587.8647093772888
[2m[36m(pid=49332)[0m GPU available: False, used: False
[2m[36m(pid=49332)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49332)[0m 
[2m[36m(pid=49332)[0m   | Name      | Type              | Params
[2m[36m(pid=49332)[0m ------------------------------------------------
[2m[36m(pid=49332)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49332)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49332)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49332)[0m ------------------------------------------------
[2m[36m(pid=49332)[0m 8.7 K     Trainable params
[2m[36m(pid=49332)[0m 0         Non-trainable params
[2m[36m(pid=49332)[0m 8.7 K     Total params
[2m[36m(pid=35806)[0m time to fit was 547.0959827899933
Result for _inner_e8deb_00036:
  auc: 0.9121419429779053
  date: 2021-03-19_12-36-23
  done: false
  experiment_id: aadde1fe2ef54b5684a34e7dbf6ada5e
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35806
  time_since_restore: 3160.355309009552
  time_this_iter_s: 3160.355309009552
  time_total_s: 3160.355309009552
  timestamp: 1616153783
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00036
  
[2m[36m(pid=35806)[0m Finished run with seed 0 - lr 0.01 - sec_lr 0.01 - bs 64 - mean val auc: 0.9121419429779053
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 73/180 (1 PENDING, 26 RUNNING, 46 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |                     |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |                     |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00036 | RUNNING    | 145.101.32.82:35806 |           64 |     0 | 0.01  |    0.01  |      1 |          3160.36 | 0.912142 |
| _inner_e8deb_00040 | RUNNING    |                     |           32 |     0 | 0.1   |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |                     |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |                     |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00072 | PENDING    |                     |          128 |     0 | 0.1   |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |          1197.15 | 0.9138   |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 53 more trials not shown (16 RUNNING, 36 TERMINATED)


Result for _inner_e8deb_00036:
  auc: 0.9121419429779053
  date: 2021-03-19_12-36-23
  done: true
  experiment_id: aadde1fe2ef54b5684a34e7dbf6ada5e
  experiment_tag: 36_batch_size=64,eta=0.0,lr=0.01,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35806
  time_since_restore: 3160.355309009552
  time_this_iter_s: 3160.355309009552
  time_total_s: 3160.355309009552
  timestamp: 1616153783
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00036
  
2021-03-19 12:36:25,165	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff6bbe78af01000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=8106)[0m time to fit was 944.244467496872
[2m[36m(pid=8106)[0m GPU available: False, used: False
[2m[36m(pid=8106)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8106)[0m 
[2m[36m(pid=8106)[0m   | Name      | Type              | Params
[2m[36m(pid=8106)[0m ------------------------------------------------
[2m[36m(pid=8106)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8106)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8106)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8106)[0m ------------------------------------------------
[2m[36m(pid=8106)[0m 8.7 K     Trainable params
[2m[36m(pid=8106)[0m 0         Non-trainable params
[2m[36m(pid=8106)[0m 8.7 K     Total params
[2m[36m(pid=35787)[0m time to fit was 432.0371940135956
Result for _inner_e8deb_00040:
  auc: 0.913101875782013
  date: 2021-03-19_12-36-32
  done: false
  experiment_id: a214d31e76c44da09bc356531f0b7fdc
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35787
  time_since_restore: 2732.200400829315
  time_this_iter_s: 2732.200400829315
  time_total_s: 2732.200400829315
  timestamp: 1616153792
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00040
  
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 74/180 (1 PENDING, 26 RUNNING, 47 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |                     |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |                     |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00040 | RUNNING    | 145.101.32.82:35787 |           32 |     0 | 0.1   |    0.01  |      1 |          2732.2  | 0.913102 |
| _inner_e8deb_00045 | RUNNING    |                     |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |                     |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00051 | RUNNING    |                     |           64 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00073 | PENDING    |                     |          256 |     0 | 0.1   |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |          1197.15 | 0.9138   |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 54 more trials not shown (16 RUNNING, 37 TERMINATED)


Result for _inner_e8deb_00040:
  auc: 0.913101875782013
  date: 2021-03-19_12-36-32
  done: true
  experiment_id: a214d31e76c44da09bc356531f0b7fdc
  experiment_tag: 40_batch_size=32,eta=0.0,lr=0.1,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35787
  time_since_restore: 2732.200400829315
  time_this_iter_s: 2732.200400829315
  time_total_s: 2732.200400829315
  timestamp: 1616153792
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00040
  
[2m[36m(pid=35787)[0m Finished run with seed 0 - lr 0.1 - sec_lr 0.01 - bs 32 - mean val auc: 0.913101875782013
[2m[36m(pid=38100)[0m Starting run with seed 0 - lr 0.1 - sec_lr 0.1 - bs 128
[2m[36m(pid=38100)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38100)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=38100)[0m GPU available: False, used: False
[2m[36m(pid=38100)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=38100)[0m 
[2m[36m(pid=38100)[0m   | Name      | Type              | Params
[2m[36m(pid=38100)[0m ------------------------------------------------
[2m[36m(pid=38100)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=38100)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=38100)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=38100)[0m ------------------------------------------------
[2m[36m(pid=38100)[0m 8.7 K     Trainable params
[2m[36m(pid=38100)[0m 0         Non-trainable params
[2m[36m(pid=38100)[0m 8.7 K     Total params
[2m[36m(pid=38100)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=38100)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=38100)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=38100)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=38100)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=38100)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=38368)[0m Starting run with seed 0 - lr 0.1 - sec_lr 0.1 - bs 256
[2m[36m(pid=38368)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38368)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=38368)[0m GPU available: False, used: False
[2m[36m(pid=38368)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=38368)[0m 
[2m[36m(pid=38368)[0m   | Name      | Type              | Params
[2m[36m(pid=38368)[0m ------------------------------------------------
[2m[36m(pid=38368)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=38368)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=38368)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=38368)[0m ------------------------------------------------
[2m[36m(pid=38368)[0m 8.7 K     Trainable params
[2m[36m(pid=38368)[0m 0         Non-trainable params
[2m[36m(pid=38368)[0m 8.7 K     Total params
[2m[36m(pid=38368)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=38368)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=38368)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=38368)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=38368)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=38368)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35848)[0m time to fit was 1569.746030330658
[2m[36m(pid=35848)[0m GPU available: False, used: False
[2m[36m(pid=35848)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35848)[0m 
[2m[36m(pid=35848)[0m   | Name      | Type              | Params
[2m[36m(pid=35848)[0m ------------------------------------------------
[2m[36m(pid=35848)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35848)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35848)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35848)[0m ------------------------------------------------
[2m[36m(pid=35848)[0m 8.7 K     Trainable params
[2m[36m(pid=35848)[0m 0         Non-trainable params
[2m[36m(pid=35848)[0m 8.7 K     Total params
[2m[36m(pid=32133)[0m time to fit was 238.52004528045654
[2m[36m(pid=32133)[0m GPU available: False, used: False
[2m[36m(pid=32133)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=32133)[0m 
[2m[36m(pid=32133)[0m   | Name      | Type              | Params
[2m[36m(pid=32133)[0m ------------------------------------------------
[2m[36m(pid=32133)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=32133)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=32133)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=32133)[0m ------------------------------------------------
[2m[36m(pid=32133)[0m 8.7 K     Trainable params
[2m[36m(pid=32133)[0m 0         Non-trainable params
[2m[36m(pid=32133)[0m 8.7 K     Total params
[2m[36m(pid=21447)[0m time to fit was 221.65944623947144
[2m[36m(pid=21447)[0m GPU available: False, used: False
[2m[36m(pid=21447)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=21447)[0m 
[2m[36m(pid=21447)[0m   | Name      | Type              | Params
[2m[36m(pid=21447)[0m ------------------------------------------------
[2m[36m(pid=21447)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=21447)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=21447)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=21447)[0m ------------------------------------------------
[2m[36m(pid=21447)[0m 8.7 K     Trainable params
[2m[36m(pid=21447)[0m 0         Non-trainable params
[2m[36m(pid=21447)[0m 8.7 K     Total params
[2m[36m(pid=18718)[0m time to fit was 259.8203856945038
[2m[36m(pid=18718)[0m GPU available: False, used: False
[2m[36m(pid=18718)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=18718)[0m 
[2m[36m(pid=18718)[0m   | Name      | Type              | Params
[2m[36m(pid=18718)[0m ------------------------------------------------
[2m[36m(pid=18718)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=18718)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=18718)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=18718)[0m ------------------------------------------------
[2m[36m(pid=18718)[0m 8.7 K     Trainable params
[2m[36m(pid=18718)[0m 0         Non-trainable params
[2m[36m(pid=18718)[0m 8.7 K     Total params
[2m[36m(pid=13479)[0m time to fit was 223.1345489025116
[2m[36m(pid=13479)[0m GPU available: False, used: False
[2m[36m(pid=13479)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=13479)[0m 
[2m[36m(pid=13479)[0m   | Name      | Type              | Params
[2m[36m(pid=13479)[0m ------------------------------------------------
[2m[36m(pid=13479)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=13479)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=13479)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=13479)[0m ------------------------------------------------
[2m[36m(pid=13479)[0m 8.7 K     Trainable params
[2m[36m(pid=13479)[0m 0         Non-trainable params
[2m[36m(pid=13479)[0m 8.7 K     Total params
[2m[36m(pid=11277)[0m time to fit was 320.22387886047363
[2m[36m(pid=11277)[0m GPU available: False, used: False
[2m[36m(pid=11277)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=11277)[0m 
[2m[36m(pid=11277)[0m   | Name      | Type              | Params
[2m[36m(pid=11277)[0m ------------------------------------------------
[2m[36m(pid=11277)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=11277)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=11277)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=11277)[0m ------------------------------------------------
[2m[36m(pid=11277)[0m 8.7 K     Trainable params
[2m[36m(pid=11277)[0m 0         Non-trainable params
[2m[36m(pid=11277)[0m 8.7 K     Total params
[2m[36m(pid=38368)[0m time to fit was 104.8395299911499
[2m[36m(pid=38368)[0m GPU available: False, used: False
[2m[36m(pid=38368)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=38368)[0m 
[2m[36m(pid=38368)[0m   | Name      | Type              | Params
[2m[36m(pid=38368)[0m ------------------------------------------------
[2m[36m(pid=38368)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=38368)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=38368)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=38368)[0m ------------------------------------------------
[2m[36m(pid=38368)[0m 8.7 K     Trainable params
[2m[36m(pid=38368)[0m 0         Non-trainable params
[2m[36m(pid=38368)[0m 8.7 K     Total params
[2m[36m(pid=23585)[0m time to fit was 564.4541041851044
[2m[36m(pid=23585)[0m GPU available: False, used: False
[2m[36m(pid=23585)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23585)[0m 
[2m[36m(pid=23585)[0m   | Name      | Type              | Params
[2m[36m(pid=23585)[0m ------------------------------------------------
[2m[36m(pid=23585)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23585)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23585)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23585)[0m ------------------------------------------------
[2m[36m(pid=23585)[0m 8.7 K     Trainable params
[2m[36m(pid=23585)[0m 0         Non-trainable params
[2m[36m(pid=23585)[0m 8.7 K     Total params
[2m[36m(pid=38100)[0m time to fit was 135.48473501205444
[2m[36m(pid=38100)[0m GPU available: False, used: False
[2m[36m(pid=38100)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=38100)[0m 
[2m[36m(pid=38100)[0m   | Name      | Type              | Params
[2m[36m(pid=38100)[0m ------------------------------------------------
[2m[36m(pid=38100)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=38100)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=38100)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=38100)[0m ------------------------------------------------
[2m[36m(pid=38100)[0m 8.7 K     Trainable params
[2m[36m(pid=38100)[0m 0         Non-trainable params
[2m[36m(pid=38100)[0m 8.7 K     Total params
[2m[36m(pid=52234)[0m time to fit was 408.75565099716187
[2m[36m(pid=52234)[0m GPU available: False, used: False
[2m[36m(pid=52234)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=52234)[0m 
[2m[36m(pid=52234)[0m   | Name      | Type              | Params
[2m[36m(pid=52234)[0m ------------------------------------------------
[2m[36m(pid=52234)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=52234)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=52234)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=52234)[0m ------------------------------------------------
[2m[36m(pid=52234)[0m 8.7 K     Trainable params
[2m[36m(pid=52234)[0m 0         Non-trainable params
[2m[36m(pid=52234)[0m 8.7 K     Total params
[2m[36m(pid=49332)[0m time to fit was 209.64440298080444
[2m[36m(pid=49332)[0m GPU available: False, used: False
[2m[36m(pid=49332)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49332)[0m 
[2m[36m(pid=49332)[0m   | Name      | Type              | Params
[2m[36m(pid=49332)[0m ------------------------------------------------
[2m[36m(pid=49332)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49332)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49332)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49332)[0m ------------------------------------------------
[2m[36m(pid=49332)[0m 8.7 K     Trainable params
[2m[36m(pid=49332)[0m 0         Non-trainable params
[2m[36m(pid=49332)[0m 8.7 K     Total params
[2m[36m(pid=10108)[0m time to fit was 530.2783586978912
[2m[36m(pid=10108)[0m GPU available: False, used: False
[2m[36m(pid=10108)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10108)[0m 
[2m[36m(pid=10108)[0m   | Name      | Type              | Params
[2m[36m(pid=10108)[0m ------------------------------------------------
[2m[36m(pid=10108)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10108)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10108)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10108)[0m ------------------------------------------------
[2m[36m(pid=10108)[0m 8.7 K     Trainable params
[2m[36m(pid=10108)[0m 0         Non-trainable params
[2m[36m(pid=10108)[0m 8.7 K     Total params
[2m[36m(pid=35786)[0m time to fit was 935.5304479598999
[2m[36m(pid=35786)[0m GPU available: False, used: False
[2m[36m(pid=35786)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35786)[0m 
[2m[36m(pid=35786)[0m   | Name      | Type              | Params
[2m[36m(pid=35786)[0m ------------------------------------------------
[2m[36m(pid=35786)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35786)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35786)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35786)[0m ------------------------------------------------
[2m[36m(pid=35786)[0m 8.7 K     Trainable params
[2m[36m(pid=35786)[0m 0         Non-trainable params
[2m[36m(pid=35786)[0m 8.7 K     Total params
[2m[36m(pid=38368)[0m time to fit was 96.33089542388916
[2m[36m(pid=38368)[0m GPU available: False, used: False
[2m[36m(pid=38368)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=38368)[0m 
[2m[36m(pid=38368)[0m   | Name      | Type              | Params
[2m[36m(pid=38368)[0m ------------------------------------------------
[2m[36m(pid=38368)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=38368)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=38368)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=38368)[0m ------------------------------------------------
[2m[36m(pid=38368)[0m 8.7 K     Trainable params
[2m[36m(pid=38368)[0m 0         Non-trainable params
[2m[36m(pid=38368)[0m 8.7 K     Total params
[2m[36m(pid=21447)[0m time to fit was 136.0911705493927
[2m[36m(pid=35776)[0m time to fit was 422.08901500701904
[2m[36m(pid=21447)[0m GPU available: False, used: False
[2m[36m(pid=21447)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35776)[0m Finished run with seed 0 - lr 2 - sec_lr 0.01 - bs 64 - mean val auc: 0.9094533085823059
[2m[36m(pid=21447)[0m 
[2m[36m(pid=21447)[0m   | Name      | Type              | Params
[2m[36m(pid=21447)[0m ------------------------------------------------
[2m[36m(pid=21447)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=21447)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=21447)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=21447)[0m ------------------------------------------------
[2m[36m(pid=21447)[0m 8.7 K     Trainable params
[2m[36m(pid=21447)[0m 0         Non-trainable params
[2m[36m(pid=21447)[0m 8.7 K     Total params
Result for _inner_e8deb_00051:
  auc: 0.9094533085823059
  date: 2021-03-19_12-40-07
  done: false
  experiment_id: 8878e4a320dc4d02a46db8a7a41a9891
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35776
  time_since_restore: 1824.0578587055206
  time_this_iter_s: 1824.0578587055206
  time_total_s: 1824.0578587055206
  timestamp: 1616154007
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00051
  
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 75/180 (1 PENDING, 26 RUNNING, 48 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |                     |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |                     |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |                     |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |                     |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |                     |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00051 | RUNNING    | 145.101.32.82:35776 |           64 |     0 | 2     |    0.01  |      1 |          1824.06 | 0.909453 |
| _inner_e8deb_00055 | RUNNING    |                     |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00074 | PENDING    |                     |          512 |     0 | 0.1   |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |          1197.15 | 0.9138   |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 55 more trials not shown (16 RUNNING, 38 TERMINATED)


Result for _inner_e8deb_00051:
  auc: 0.9094533085823059
  date: 2021-03-19_12-40-07
  done: true
  experiment_id: 8878e4a320dc4d02a46db8a7a41a9891
  experiment_tag: 51_batch_size=64,eta=0.0,lr=2,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35776
  time_since_restore: 1824.0578587055206
  time_this_iter_s: 1824.0578587055206
  time_total_s: 1824.0578587055206
  timestamp: 1616154007
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00051
  
[2m[36m(pid=45622)[0m Starting run with seed 0 - lr 0.1 - sec_lr 0.1 - bs 512
[2m[36m(pid=45622)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=45622)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=45622)[0m GPU available: False, used: False
[2m[36m(pid=45622)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=45622)[0m 
[2m[36m(pid=45622)[0m   | Name      | Type              | Params
[2m[36m(pid=45622)[0m ------------------------------------------------
[2m[36m(pid=45622)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=45622)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=45622)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=45622)[0m ------------------------------------------------
[2m[36m(pid=45622)[0m 8.7 K     Trainable params
[2m[36m(pid=45622)[0m 0         Non-trainable params
[2m[36m(pid=45622)[0m 8.7 K     Total params
[2m[36m(pid=45622)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=45622)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=45622)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=45622)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=45622)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=45622)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35783)[0m time to fit was 748.5450947284698
[2m[36m(pid=35783)[0m GPU available: False, used: False
[2m[36m(pid=35783)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35783)[0m 
[2m[36m(pid=35783)[0m   | Name      | Type              | Params
[2m[36m(pid=35783)[0m ------------------------------------------------
[2m[36m(pid=35783)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35783)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35783)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35783)[0m ------------------------------------------------
[2m[36m(pid=35783)[0m 8.7 K     Trainable params
[2m[36m(pid=35783)[0m 0         Non-trainable params
[2m[36m(pid=35783)[0m 8.7 K     Total params
[2m[36m(pid=17269)[0m time to fit was 543.8337352275848
[2m[36m(pid=17269)[0m GPU available: False, used: False
[2m[36m(pid=17269)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=17269)[0m 
[2m[36m(pid=17269)[0m   | Name      | Type              | Params
[2m[36m(pid=17269)[0m ------------------------------------------------
[2m[36m(pid=17269)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=17269)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=17269)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=17269)[0m ------------------------------------------------
[2m[36m(pid=17269)[0m 8.7 K     Trainable params
[2m[36m(pid=17269)[0m 0         Non-trainable params
[2m[36m(pid=17269)[0m 8.7 K     Total params
[2m[36m(pid=48139)[0m time to fit was 1640.4096250534058
[2m[36m(pid=48139)[0m GPU available: False, used: False
[2m[36m(pid=48139)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48139)[0m 
[2m[36m(pid=48139)[0m   | Name      | Type              | Params
[2m[36m(pid=48139)[0m ------------------------------------------------
[2m[36m(pid=48139)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48139)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48139)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48139)[0m ------------------------------------------------
[2m[36m(pid=48139)[0m 8.7 K     Trainable params
[2m[36m(pid=48139)[0m 0         Non-trainable params
[2m[36m(pid=48139)[0m 8.7 K     Total params
[2m[36m(pid=52234)[0m time to fit was 159.90125799179077
Result for _inner_e8deb_00057:
  auc: 0.8095038056373596
  date: 2021-03-19_12-41-42
  done: false
  experiment_id: 7daa43a90fa646ccbd2bdde4bc182a47
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 52234
  time_since_restore: 1522.0740580558777
  time_this_iter_s: 1522.0740580558777
  time_total_s: 1522.0740580558777
  timestamp: 1616154102
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00057
  
[2m[36m(pid=52234)[0m Finished run with seed 0 - lr 5 - sec_lr 0.01 - bs 128 - mean val auc: 0.8095038056373596
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 76/180 (1 PENDING, 26 RUNNING, 49 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |       |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |       |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00056 | RUNNING    |       |           64 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00075 | PENDING    |       |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |          1197.15 | 0.9138   |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 56 more trials not shown (16 RUNNING, 39 TERMINATED)


Result for _inner_e8deb_00057:
  auc: 0.8095038056373596
  date: 2021-03-19_12-41-42
  done: true
  experiment_id: 7daa43a90fa646ccbd2bdde4bc182a47
  experiment_tag: 57_batch_size=128,eta=0.0,lr=5,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 52234
  time_since_restore: 1522.0740580558777
  time_this_iter_s: 1522.0740580558777
  time_total_s: 1522.0740580558777
  timestamp: 1616154102
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00057
  
[2m[36m(pid=45622)[0m time to fit was 86.10210514068604
[2m[36m(pid=45622)[0m GPU available: False, used: False
[2m[36m(pid=45622)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=45622)[0m 
[2m[36m(pid=45622)[0m   | Name      | Type              | Params
[2m[36m(pid=45622)[0m ------------------------------------------------
[2m[36m(pid=45622)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=45622)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=45622)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=45622)[0m ------------------------------------------------
[2m[36m(pid=45622)[0m 8.7 K     Trainable params
[2m[36m(pid=45622)[0m 0         Non-trainable params
[2m[36m(pid=45622)[0m 8.7 K     Total params
[2m[36m(pid=48984)[0m Starting run with seed 0 - lr 1 - sec_lr 0.1 - bs 32
[2m[36m(pid=48984)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=48984)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=48984)[0m GPU available: False, used: False
[2m[36m(pid=48984)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48984)[0m 
[2m[36m(pid=48984)[0m   | Name      | Type              | Params
[2m[36m(pid=48984)[0m ------------------------------------------------
[2m[36m(pid=48984)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48984)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48984)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48984)[0m ------------------------------------------------
[2m[36m(pid=48984)[0m 8.7 K     Trainable params
[2m[36m(pid=48984)[0m 0         Non-trainable params
[2m[36m(pid=48984)[0m 8.7 K     Total params
[2m[36m(pid=48984)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=48984)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48984)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=48984)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48984)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=48984)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=38368)[0m time to fit was 112.93674087524414
[2m[36m(pid=38368)[0m GPU available: False, used: False
[2m[36m(pid=38368)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=38368)[0m 
[2m[36m(pid=38368)[0m   | Name      | Type              | Params
[2m[36m(pid=38368)[0m ------------------------------------------------
[2m[36m(pid=38368)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=38368)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=38368)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=38368)[0m ------------------------------------------------
[2m[36m(pid=38368)[0m 8.7 K     Trainable params
[2m[36m(pid=38368)[0m 0         Non-trainable params
[2m[36m(pid=38368)[0m 8.7 K     Total params
[2m[36m(pid=38100)[0m time to fit was 187.9149100780487
[2m[36m(pid=38100)[0m GPU available: False, used: False
[2m[36m(pid=38100)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=38100)[0m 
[2m[36m(pid=38100)[0m   | Name      | Type              | Params
[2m[36m(pid=38100)[0m ------------------------------------------------
[2m[36m(pid=38100)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=38100)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=38100)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=38100)[0m ------------------------------------------------
[2m[36m(pid=38100)[0m 8.7 K     Trainable params
[2m[36m(pid=38100)[0m 0         Non-trainable params
[2m[36m(pid=38100)[0m 8.7 K     Total params
[2m[36m(pid=13479)[0m time to fit was 226.01718831062317
[2m[36m(pid=13479)[0m Finished run with seed 0 - lr 0.001 - sec_lr 0.1 - bs 512 - mean val auc: 0.895311462879181
Result for _inner_e8deb_00064:
  auc: 0.895311462879181
  date: 2021-03-19_12-42-06
  done: false
  experiment_id: f6a8f5ee5acd43f994f404438e7aa7e7
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 13479
  time_since_restore: 1110.5360550880432
  time_this_iter_s: 1110.5360550880432
  time_total_s: 1110.5360550880432
  timestamp: 1616154126
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00064
  
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 77/180 (1 PENDING, 26 RUNNING, 50 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |       |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |       |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00056 | RUNNING    |       |           64 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00076 | PENDING    |       |           64 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |          1197.15 | 0.9138   |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 57 more trials not shown (16 RUNNING, 40 TERMINATED)


Result for _inner_e8deb_00064:
  auc: 0.895311462879181
  date: 2021-03-19_12-42-06
  done: true
  experiment_id: f6a8f5ee5acd43f994f404438e7aa7e7
  experiment_tag: 64_batch_size=512,eta=0.0,lr=0.001,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 13479
  time_since_restore: 1110.5360550880432
  time_this_iter_s: 1110.5360550880432
  time_total_s: 1110.5360550880432
  timestamp: 1616154126
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00064
  
[2m[36m(pid=49910)[0m Starting run with seed 0 - lr 1 - sec_lr 0.1 - bs 64
[2m[36m(pid=49910)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=49910)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=49910)[0m GPU available: False, used: False
[2m[36m(pid=49910)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49910)[0m 
[2m[36m(pid=49910)[0m   | Name      | Type              | Params
[2m[36m(pid=49910)[0m ------------------------------------------------
[2m[36m(pid=49910)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49910)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49910)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49910)[0m ------------------------------------------------
[2m[36m(pid=49910)[0m 8.7 K     Trainable params
[2m[36m(pid=49910)[0m 0         Non-trainable params
[2m[36m(pid=49910)[0m 8.7 K     Total params
[2m[36m(pid=49910)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=49910)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49910)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=49910)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49910)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=49910)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=21447)[0m time to fit was 134.53274965286255
Result for _inner_e8deb_00069:
  auc: 0.9103763461112976
  date: 2021-03-19_12-42-21
  done: false
  experiment_id: 84d74a4afbb944428f253223dc230bb0
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 21447
  time_since_restore: 864.4227960109711
  time_this_iter_s: 864.4227960109711
  time_total_s: 864.4227960109711
  timestamp: 1616154141
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00069
  
[2m[36m(pid=21447)[0m Finished run with seed 0 - lr 0.01 - sec_lr 0.1 - bs 512 - mean val auc: 0.9103763461112976
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 78/180 (1 PENDING, 26 RUNNING, 51 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    |       |           64 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |       |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |       |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00056 | RUNNING    |       |           64 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00077 | PENDING    |       |          128 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |          1197.15 | 0.9138   |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 58 more trials not shown (16 RUNNING, 41 TERMINATED)


Result for _inner_e8deb_00069:
  auc: 0.9103763461112976
  date: 2021-03-19_12-42-21
  done: true
  experiment_id: 84d74a4afbb944428f253223dc230bb0
  experiment_tag: 69_batch_size=512,eta=0.0,lr=0.01,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 21447
  time_since_restore: 864.4227960109711
  time_this_iter_s: 864.4227960109711
  time_total_s: 864.4227960109711
  timestamp: 1616154141
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00069
  
[2m[36m(pid=18718)[0m time to fit was 264.31325578689575
[2m[36m(pid=18718)[0m GPU available: False, used: False
[2m[36m(pid=18718)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=18718)[0m 
[2m[36m(pid=18718)[0m   | Name      | Type              | Params
[2m[36m(pid=18718)[0m ------------------------------------------------
[2m[36m(pid=18718)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=18718)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=18718)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=18718)[0m ------------------------------------------------
[2m[36m(pid=18718)[0m 8.7 K     Trainable params
[2m[36m(pid=18718)[0m 0         Non-trainable params
[2m[36m(pid=18718)[0m 8.7 K     Total params
[2m[36m(pid=18484)[0m time to fit was 464.56945037841797
[2m[36m(pid=18484)[0m GPU available: False, used: False
[2m[36m(pid=18484)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=18484)[0m 
[2m[36m(pid=18484)[0m   | Name      | Type              | Params
[2m[36m(pid=18484)[0m ------------------------------------------------
[2m[36m(pid=18484)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=18484)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=18484)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=18484)[0m ------------------------------------------------
[2m[36m(pid=18484)[0m 8.7 K     Trainable params
[2m[36m(pid=18484)[0m 0         Non-trainable params
[2m[36m(pid=18484)[0m 8.7 K     Total params
[2m[36m(pid=50407)[0m Starting run with seed 0 - lr 1 - sec_lr 0.1 - bs 128
[2m[36m(pid=50407)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50407)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=50407)[0m GPU available: False, used: False
[2m[36m(pid=50407)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=50407)[0m 
[2m[36m(pid=50407)[0m   | Name      | Type              | Params
[2m[36m(pid=50407)[0m ------------------------------------------------
[2m[36m(pid=50407)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=50407)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=50407)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=50407)[0m ------------------------------------------------
[2m[36m(pid=50407)[0m 8.7 K     Trainable params
[2m[36m(pid=50407)[0m 0         Non-trainable params
[2m[36m(pid=50407)[0m 8.7 K     Total params
[2m[36m(pid=50407)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=50407)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=50407)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=50407)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=50407)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=50407)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=32133)[0m time to fit was 297.89044070243835
[2m[36m(pid=32133)[0m GPU available: False, used: False
[2m[36m(pid=32133)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=32133)[0m 
[2m[36m(pid=32133)[0m   | Name      | Type              | Params
[2m[36m(pid=32133)[0m ------------------------------------------------
[2m[36m(pid=32133)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=32133)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=32133)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=32133)[0m ------------------------------------------------
[2m[36m(pid=32133)[0m 8.7 K     Trainable params
[2m[36m(pid=32133)[0m 0         Non-trainable params
[2m[36m(pid=32133)[0m 8.7 K     Total params
[2m[36m(pid=35818)[0m time to fit was 961.0443642139435
[2m[36m(pid=35818)[0m GPU available: False, used: False
[2m[36m(pid=35818)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35818)[0m 
[2m[36m(pid=35818)[0m   | Name      | Type              | Params
[2m[36m(pid=35818)[0m ------------------------------------------------
[2m[36m(pid=35818)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35818)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35818)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35818)[0m ------------------------------------------------
[2m[36m(pid=35818)[0m 8.7 K     Trainable params
[2m[36m(pid=35818)[0m 0         Non-trainable params
[2m[36m(pid=35818)[0m 8.7 K     Total params
[2m[36m(pid=35857)[0m time to fit was 954.457409620285
Result for _inner_e8deb_00001:
  auc: 0.9068581819534302
  date: 2021-03-19_12-43-12
  done: false
  experiment_id: b5b747258c674c78a064fe5af6a0b36f
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35857
  time_since_restore: 4880.386644601822
  time_this_iter_s: 4880.386644601822
  time_total_s: 4880.386644601822
  timestamp: 1616154192
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00001
  
[2m[36m(pid=35857)[0m Finished run with seed 0 - lr 0.001 - sec_lr 0.001 - bs 64 - mean val auc: 0.9068581819534302
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 79/180 (1 PENDING, 26 RUNNING, 52 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00001 | RUNNING    | 145.101.32.82:35857 |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |                     |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |                     |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |                     |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |                     |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |                     |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00056 | RUNNING    |                     |           64 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00078 | PENDING    |                     |          256 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |          1197.15 | 0.9138   |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 59 more trials not shown (16 RUNNING, 42 TERMINATED)


Result for _inner_e8deb_00001:
  auc: 0.9068581819534302
  date: 2021-03-19_12-43-12
  done: true
  experiment_id: b5b747258c674c78a064fe5af6a0b36f
  experiment_tag: 1_batch_size=64,eta=0.0,lr=0.001,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35857
  time_since_restore: 4880.386644601822
  time_this_iter_s: 4880.386644601822
  time_total_s: 4880.386644601822
  timestamp: 1616154192
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00001
  
[2m[36m(pid=45622)[0m time to fit was 87.69847059249878
[2m[36m(pid=45622)[0m GPU available: False, used: False
[2m[36m(pid=45622)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=45622)[0m 
[2m[36m(pid=45622)[0m   | Name      | Type              | Params
[2m[36m(pid=45622)[0m ------------------------------------------------
[2m[36m(pid=45622)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=45622)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=45622)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=45622)[0m ------------------------------------------------
[2m[36m(pid=45622)[0m 8.7 K     Trainable params
[2m[36m(pid=45622)[0m 0         Non-trainable params
[2m[36m(pid=45622)[0m 8.7 K     Total params
[2m[36m(pid=13798)[0m time to fit was 640.6148943901062
[2m[36m(pid=13798)[0m GPU available: False, used: False
[2m[36m(pid=13798)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=13798)[0m 
[2m[36m(pid=13798)[0m   | Name      | Type              | Params
[2m[36m(pid=13798)[0m ------------------------------------------------
[2m[36m(pid=13798)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=13798)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=13798)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=13798)[0m ------------------------------------------------
[2m[36m(pid=13798)[0m 8.7 K     Trainable params
[2m[36m(pid=13798)[0m 0         Non-trainable params
[2m[36m(pid=13798)[0m 8.7 K     Total params
[2m[36m(pid=51937)[0m Starting run with seed 0 - lr 1 - sec_lr 0.1 - bs 256
[2m[36m(pid=51937)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=51937)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=51937)[0m GPU available: False, used: False
[2m[36m(pid=51937)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=51937)[0m 
[2m[36m(pid=51937)[0m   | Name      | Type              | Params
[2m[36m(pid=51937)[0m ------------------------------------------------
[2m[36m(pid=51937)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=51937)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=51937)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=51937)[0m ------------------------------------------------
[2m[36m(pid=51937)[0m 8.7 K     Trainable params
[2m[36m(pid=51937)[0m 0         Non-trainable params
[2m[36m(pid=51937)[0m 8.7 K     Total params
[2m[36m(pid=51937)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=51937)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=51937)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=51937)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=51937)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=51937)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=38368)[0m time to fit was 97.15600633621216
[2m[36m(pid=38368)[0m GPU available: False, used: False
[2m[36m(pid=38368)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=38368)[0m 
[2m[36m(pid=38368)[0m   | Name      | Type              | Params
[2m[36m(pid=38368)[0m ------------------------------------------------
[2m[36m(pid=38368)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=38368)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=38368)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=38368)[0m ------------------------------------------------
[2m[36m(pid=38368)[0m 8.7 K     Trainable params
[2m[36m(pid=38368)[0m 0         Non-trainable params
[2m[36m(pid=38368)[0m 8.7 K     Total params
[2m[36m(pid=11277)[0m time to fit was 320.90097093582153
[2m[36m(pid=11277)[0m GPU available: False, used: False
[2m[36m(pid=11277)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=11277)[0m 
[2m[36m(pid=11277)[0m   | Name      | Type              | Params
[2m[36m(pid=11277)[0m ------------------------------------------------
[2m[36m(pid=11277)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=11277)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=11277)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=11277)[0m ------------------------------------------------
[2m[36m(pid=11277)[0m 8.7 K     Trainable params
[2m[36m(pid=11277)[0m 0         Non-trainable params
[2m[36m(pid=11277)[0m 8.7 K     Total params
[2m[36m(pid=38100)[0m time to fit was 135.17706179618835
[2m[36m(pid=38100)[0m GPU available: False, used: False
[2m[36m(pid=38100)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=38100)[0m 
[2m[36m(pid=38100)[0m   | Name      | Type              | Params
[2m[36m(pid=38100)[0m ------------------------------------------------
[2m[36m(pid=38100)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=38100)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=38100)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=38100)[0m ------------------------------------------------
[2m[36m(pid=38100)[0m 8.7 K     Trainable params
[2m[36m(pid=38100)[0m 0         Non-trainable params
[2m[36m(pid=38100)[0m 8.7 K     Total params
[2m[36m(pid=51937)[0m time to fit was 90.29093146324158
[2m[36m(pid=51937)[0m GPU available: False, used: False
[2m[36m(pid=51937)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=51937)[0m 
[2m[36m(pid=51937)[0m   | Name      | Type              | Params
[2m[36m(pid=51937)[0m ------------------------------------------------
[2m[36m(pid=51937)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=51937)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=51937)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=51937)[0m ------------------------------------------------
[2m[36m(pid=51937)[0m 8.7 K     Trainable params
[2m[36m(pid=51937)[0m 0         Non-trainable params
[2m[36m(pid=51937)[0m 8.7 K     Total params
[2m[36m(pid=38368)[0m time to fit was 91.94438982009888
[2m[36m(pid=38368)[0m Finished run with seed 0 - lr 0.1 - sec_lr 0.1 - bs 256 - mean val auc: 0.9119445323944092
Result for _inner_e8deb_00073:
  auc: 0.9119445323944092
  date: 2021-03-19_12-45-07
  done: false
  experiment_id: 9a5929bacd584d98bc17f781a852514a
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 38368
  time_since_restore: 504.50676012039185
  time_this_iter_s: 504.50676012039185
  time_total_s: 504.50676012039185
  timestamp: 1616154307
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00073
  
[2m[36m(pid=38368)[0m 
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 80/180 (1 PENDING, 26 RUNNING, 53 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |       |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |       |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00056 | RUNNING    |       |           64 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00079 | PENDING    |       |          512 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 60 more trials not shown (16 RUNNING, 43 TERMINATED)


Result for _inner_e8deb_00073:
  auc: 0.9119445323944092
  date: 2021-03-19_12-45-07
  done: true
  experiment_id: 9a5929bacd584d98bc17f781a852514a
  experiment_tag: 73_batch_size=256,eta=0.0,lr=0.1,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 38368
  time_since_restore: 504.50676012039185
  time_this_iter_s: 504.50676012039185
  time_total_s: 504.50676012039185
  timestamp: 1616154307
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00073
  
[2m[36m(pid=45622)[0m time to fit was 117.90702962875366
[2m[36m(pid=45622)[0m GPU available: False, used: False
[2m[36m(pid=45622)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=45622)[0m 
[2m[36m(pid=45622)[0m   | Name      | Type              | Params
[2m[36m(pid=45622)[0m ------------------------------------------------
[2m[36m(pid=45622)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=45622)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=45622)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=45622)[0m ------------------------------------------------
[2m[36m(pid=45622)[0m 8.7 K     Trainable params
[2m[36m(pid=45622)[0m 0         Non-trainable params
[2m[36m(pid=45622)[0m 8.7 K     Total params
[2m[36m(pid=2972)[0m Starting run with seed 0 - lr 1 - sec_lr 0.1 - bs 512
[2m[36m(pid=2972)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=2972)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=2972)[0m GPU available: False, used: False
[2m[36m(pid=2972)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=2972)[0m 
[2m[36m(pid=2972)[0m   | Name      | Type              | Params
[2m[36m(pid=2972)[0m ------------------------------------------------
[2m[36m(pid=2972)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=2972)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=2972)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=2972)[0m ------------------------------------------------
[2m[36m(pid=2972)[0m 8.7 K     Trainable params
[2m[36m(pid=2972)[0m 0         Non-trainable params
[2m[36m(pid=2972)[0m 8.7 K     Total params
[2m[36m(pid=2972)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=2972)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=2972)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=2972)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=2972)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=2972)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=50407)[0m time to fit was 178.7393000125885
[2m[36m(pid=50407)[0m GPU available: False, used: False
[2m[36m(pid=50407)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=50407)[0m 
[2m[36m(pid=50407)[0m   | Name      | Type              | Params
[2m[36m(pid=50407)[0m ------------------------------------------------
[2m[36m(pid=50407)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=50407)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=50407)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=50407)[0m ------------------------------------------------
[2m[36m(pid=50407)[0m 8.7 K     Trainable params
[2m[36m(pid=50407)[0m 0         Non-trainable params
[2m[36m(pid=50407)[0m 8.7 K     Total params
[2m[36m(pid=35802)[0m time to fit was 944.4994723796844
[2m[36m(pid=35802)[0m GPU available: False, used: False
[2m[36m(pid=35802)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35802)[0m 
[2m[36m(pid=35802)[0m   | Name      | Type              | Params
[2m[36m(pid=35802)[0m ------------------------------------------------
[2m[36m(pid=35802)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35802)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35802)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35802)[0m ------------------------------------------------
[2m[36m(pid=35802)[0m 8.7 K     Trainable params
[2m[36m(pid=35802)[0m 0         Non-trainable params
[2m[36m(pid=35802)[0m 8.7 K     Total params
[2m[36m(pid=18484)[0m time to fit was 188.26656651496887
[2m[36m(pid=18484)[0m GPU available: False, used: False
[2m[36m(pid=18484)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=18484)[0m 
[2m[36m(pid=18484)[0m   | Name      | Type              | Params
[2m[36m(pid=18484)[0m ------------------------------------------------
[2m[36m(pid=18484)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=18484)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=18484)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=18484)[0m ------------------------------------------------
[2m[36m(pid=18484)[0m 8.7 K     Trainable params
[2m[36m(pid=18484)[0m 0         Non-trainable params
[2m[36m(pid=18484)[0m 8.7 K     Total params
[2m[36m(pid=49910)[0m time to fit was 227.28345584869385
[2m[36m(pid=49910)[0m GPU available: False, used: False
[2m[36m(pid=49910)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49910)[0m 
[2m[36m(pid=49910)[0m   | Name      | Type              | Params
[2m[36m(pid=49910)[0m ------------------------------------------------
[2m[36m(pid=49910)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49910)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49910)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49910)[0m ------------------------------------------------
[2m[36m(pid=49910)[0m 8.7 K     Trainable params
[2m[36m(pid=49910)[0m 0         Non-trainable params
[2m[36m(pid=49910)[0m 8.7 K     Total params
[2m[36m(pid=49332)[0m time to fit was 416.2057218551636
Result for _inner_e8deb_00056:
  auc: 0.7157704830169678
  date: 2021-03-19_12-46-09
  done: false
  experiment_id: 360063fc9f1e4537b0328703d0fc4b72
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 49332
  time_since_restore: 1889.1385660171509
  time_this_iter_s: 1889.1385660171509
  time_total_s: 1889.1385660171509
  timestamp: 1616154369
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00056
  
[2m[36m(pid=49332)[0m Finished run with seed 0 - lr 5 - sec_lr 0.01 - bs 64 - mean val auc: 0.7157704830169678
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 81/180 (1 PENDING, 26 RUNNING, 54 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |                     |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |                     |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |                     |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |                     |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |                     |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00056 | RUNNING    | 145.101.32.82:49332 |           64 |     0 | 5     |    0.01  |      1 |          1889.14 | 0.71577  |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00080 | PENDING    |                     |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 61 more trials not shown (16 RUNNING, 44 TERMINATED)


Result for _inner_e8deb_00056:
  auc: 0.7157704830169678
  date: 2021-03-19_12-46-09
  done: true
  experiment_id: 360063fc9f1e4537b0328703d0fc4b72
  experiment_tag: 56_batch_size=64,eta=0.0,lr=5,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 49332
  time_since_restore: 1889.1385660171509
  time_this_iter_s: 1889.1385660171509
  time_total_s: 1889.1385660171509
  timestamp: 1616154369
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00056
  
[2m[36m(pid=5013)[0m Starting run with seed 0 - lr 2 - sec_lr 0.1 - bs 32
[2m[36m(pid=5013)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=5013)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=5013)[0m GPU available: False, used: False
[2m[36m(pid=5013)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=5013)[0m 
[2m[36m(pid=5013)[0m   | Name      | Type              | Params
[2m[36m(pid=5013)[0m ------------------------------------------------
[2m[36m(pid=5013)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=5013)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=5013)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=5013)[0m ------------------------------------------------
[2m[36m(pid=5013)[0m 8.7 K     Trainable params
[2m[36m(pid=5013)[0m 0         Non-trainable params
[2m[36m(pid=5013)[0m 8.7 K     Total params
[2m[36m(pid=5013)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=5013)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=5013)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=5013)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=5013)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=5013)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=45622)[0m time to fit was 75.7483446598053
[2m[36m(pid=45622)[0m GPU available: False, used: False
[2m[36m(pid=45622)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=45622)[0m 
[2m[36m(pid=45622)[0m   | Name      | Type              | Params
[2m[36m(pid=45622)[0m ------------------------------------------------
[2m[36m(pid=45622)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=45622)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=45622)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=45622)[0m ------------------------------------------------
[2m[36m(pid=45622)[0m 8.7 K     Trainable params
[2m[36m(pid=45622)[0m 0         Non-trainable params
[2m[36m(pid=45622)[0m 8.7 K     Total params
[2m[36m(pid=2972)[0m time to fit was 80.2111132144928
[2m[36m(pid=2972)[0m GPU available: False, used: False
[2m[36m(pid=2972)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=2972)[0m 
[2m[36m(pid=2972)[0m   | Name      | Type              | Params
[2m[36m(pid=2972)[0m ------------------------------------------------
[2m[36m(pid=2972)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=2972)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=2972)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=2972)[0m ------------------------------------------------
[2m[36m(pid=2972)[0m 8.7 K     Trainable params
[2m[36m(pid=2972)[0m 0         Non-trainable params
[2m[36m(pid=2972)[0m 8.7 K     Total params
[2m[36m(pid=18718)[0m time to fit was 266.7378535270691
Result for _inner_e8deb_00068:
  auc: 0.9103809356689453
  date: 2021-03-19_12-46-54
  done: false
  experiment_id: a164ab5ab6d4423aa1f43913e52feca6
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 18718
  time_since_restore: 1226.6149706840515
  time_this_iter_s: 1226.6149706840515
  time_total_s: 1226.6149706840515
  timestamp: 1616154414
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00068
  
[2m[36m(pid=18718)[0m Finished run with seed 0 - lr 0.01 - sec_lr 0.1 - bs 256 - mean val auc: 0.9103809356689453
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 82/180 (1 PENDING, 26 RUNNING, 55 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |       |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |       |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00081 | PENDING    |       |           64 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 62 more trials not shown (16 RUNNING, 45 TERMINATED)


Result for _inner_e8deb_00068:
  auc: 0.9103809356689453
  date: 2021-03-19_12-46-54
  done: true
  experiment_id: a164ab5ab6d4423aa1f43913e52feca6
  experiment_tag: 68_batch_size=256,eta=0.0,lr=0.01,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 18718
  time_since_restore: 1226.6149706840515
  time_this_iter_s: 1226.6149706840515
  time_total_s: 1226.6149706840515
  timestamp: 1616154414
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00068
  
2021-03-19 12:46:56,907	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff54a879a001000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=38100)[0m time to fit was 169.01045632362366
[2m[36m(pid=38100)[0m GPU available: False, used: False
[2m[36m(pid=38100)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=38100)[0m 
[2m[36m(pid=38100)[0m   | Name      | Type              | Params
[2m[36m(pid=38100)[0m ------------------------------------------------
[2m[36m(pid=38100)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=38100)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=38100)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=38100)[0m ------------------------------------------------
[2m[36m(pid=38100)[0m 8.7 K     Trainable params
[2m[36m(pid=38100)[0m 0         Non-trainable params
[2m[36m(pid=38100)[0m 8.7 K     Total params
[2m[36m(pid=6537)[0m Starting run with seed 0 - lr 2 - sec_lr 0.1 - bs 64
[2m[36m(pid=6537)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=6537)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=6537)[0m GPU available: False, used: False
[2m[36m(pid=6537)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=6537)[0m 
[2m[36m(pid=6537)[0m   | Name      | Type              | Params
[2m[36m(pid=6537)[0m ------------------------------------------------
[2m[36m(pid=6537)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=6537)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=6537)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=6537)[0m ------------------------------------------------
[2m[36m(pid=6537)[0m 8.7 K     Trainable params
[2m[36m(pid=6537)[0m 0         Non-trainable params
[2m[36m(pid=6537)[0m 8.7 K     Total params
[2m[36m(pid=6537)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=6537)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=51937)[0m time to fit was 130.92279052734375
[2m[36m(pid=51937)[0m GPU available: False, used: False
[2m[36m(pid=51937)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=51937)[0m 
[2m[36m(pid=51937)[0m   | Name      | Type              | Params
[2m[36m(pid=51937)[0m ------------------------------------------------
[2m[36m(pid=51937)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=51937)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=51937)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=51937)[0m ------------------------------------------------
[2m[36m(pid=51937)[0m 8.7 K     Trainable params
[2m[36m(pid=51937)[0m 0         Non-trainable params
[2m[36m(pid=51937)[0m 8.7 K     Total params
[2m[36m(pid=6537)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=6537)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=6537)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=6537)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=45622)[0m time to fit was 80.62878680229187
Result for _inner_e8deb_00074:
  auc: 0.9121780514717102
  date: 2021-03-19_12-47-48
  done: false
  experiment_id: cf4bc23469f244749a82cccb98f2f570
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 45622
  time_since_restore: 449.3274278640747
  time_this_iter_s: 449.3274278640747
  time_total_s: 449.3274278640747
  timestamp: 1616154468
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00074
  
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 83/180 (1 PENDING, 26 RUNNING, 56 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |       |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |       |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00082 | PENDING    |       |          128 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 63 more trials not shown (16 RUNNING, 46 TERMINATED)


Result for _inner_e8deb_00074:
  auc: 0.9121780514717102
  date: 2021-03-19_12-47-48
  done: true
  experiment_id: cf4bc23469f244749a82cccb98f2f570
  experiment_tag: 74_batch_size=512,eta=0.0,lr=0.1,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 45622
  time_since_restore: 449.3274278640747
  time_this_iter_s: 449.3274278640747
  time_total_s: 449.3274278640747
  timestamp: 1616154468
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00074
  
[2m[36m(pid=45622)[0m Finished run with seed 0 - lr 0.1 - sec_lr 0.1 - bs 512 - mean val auc: 0.9121780514717102
[2m[36m(pid=50407)[0m time to fit was 144.7561056613922
[2m[36m(pid=50407)[0m GPU available: False, used: False
[2m[36m(pid=50407)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=50407)[0m 
[2m[36m(pid=50407)[0m   | Name      | Type              | Params
[2m[36m(pid=50407)[0m ------------------------------------------------
[2m[36m(pid=50407)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=50407)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=50407)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=50407)[0m ------------------------------------------------
[2m[36m(pid=50407)[0m 8.7 K     Trainable params
[2m[36m(pid=50407)[0m 0         Non-trainable params
[2m[36m(pid=50407)[0m 8.7 K     Total params
[2m[36m(pid=2972)[0m time to fit was 78.64704322814941
[2m[36m(pid=2972)[0m GPU available: False, used: False
[2m[36m(pid=2972)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=2972)[0m 
[2m[36m(pid=2972)[0m   | Name      | Type              | Params
[2m[36m(pid=2972)[0m ------------------------------------------------
[2m[36m(pid=2972)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=2972)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=2972)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=2972)[0m ------------------------------------------------
[2m[36m(pid=2972)[0m 8.7 K     Trainable params
[2m[36m(pid=2972)[0m 0         Non-trainable params
[2m[36m(pid=2972)[0m 8.7 K     Total params
[2m[36m(pid=8194)[0m Starting run with seed 0 - lr 2 - sec_lr 0.1 - bs 128
[2m[36m(pid=8194)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=8194)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=8194)[0m GPU available: False, used: False
[2m[36m(pid=8194)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8194)[0m 
[2m[36m(pid=8194)[0m   | Name      | Type              | Params
[2m[36m(pid=8194)[0m ------------------------------------------------
[2m[36m(pid=8194)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8194)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8194)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8194)[0m ------------------------------------------------
[2m[36m(pid=8194)[0m 8.7 K     Trainable params
[2m[36m(pid=8194)[0m 0         Non-trainable params
[2m[36m(pid=8194)[0m 8.7 K     Total params
[2m[36m(pid=8194)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8194)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8194)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8194)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48139)[0m time to fit was 397.147652387619
[2m[36m(pid=8194)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=8194)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48139)[0m GPU available: False, used: False
[2m[36m(pid=48139)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48139)[0m 
[2m[36m(pid=48139)[0m   | Name      | Type              | Params
[2m[36m(pid=48139)[0m ------------------------------------------------
[2m[36m(pid=48139)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48139)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48139)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48139)[0m ------------------------------------------------
[2m[36m(pid=48139)[0m 8.7 K     Trainable params
[2m[36m(pid=48139)[0m 0         Non-trainable params
[2m[36m(pid=48139)[0m 8.7 K     Total params
[2m[36m(pid=10108)[0m time to fit was 539.3283100128174
[2m[36m(pid=10108)[0m GPU available: False, used: False
[2m[36m(pid=10108)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10108)[0m 
[2m[36m(pid=10108)[0m   | Name      | Type              | Params
[2m[36m(pid=10108)[0m ------------------------------------------------
[2m[36m(pid=10108)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10108)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10108)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10108)[0m ------------------------------------------------
[2m[36m(pid=10108)[0m 8.7 K     Trainable params
[2m[36m(pid=10108)[0m 0         Non-trainable params
[2m[36m(pid=10108)[0m 8.7 K     Total params
[2m[36m(pid=32133)[0m time to fit was 349.3549015522003
[2m[36m(pid=32133)[0m GPU available: False, used: False
[2m[36m(pid=32133)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=32133)[0m 
[2m[36m(pid=32133)[0m   | Name      | Type              | Params
[2m[36m(pid=32133)[0m ------------------------------------------------
[2m[36m(pid=32133)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=32133)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=32133)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=32133)[0m ------------------------------------------------
[2m[36m(pid=32133)[0m 8.7 K     Trainable params
[2m[36m(pid=32133)[0m 0         Non-trainable params
[2m[36m(pid=32133)[0m 8.7 K     Total params
[2m[36m(pid=23585)[0m time to fit was 610.7959220409393
[2m[36m(pid=23585)[0m GPU available: False, used: False
[2m[36m(pid=23585)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23585)[0m 
[2m[36m(pid=23585)[0m   | Name      | Type              | Params
[2m[36m(pid=23585)[0m ------------------------------------------------
[2m[36m(pid=23585)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23585)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23585)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23585)[0m ------------------------------------------------
[2m[36m(pid=23585)[0m 8.7 K     Trainable params
[2m[36m(pid=23585)[0m 0         Non-trainable params
[2m[36m(pid=23585)[0m 8.7 K     Total params
[2m[36m(pid=18484)[0m time to fit was 190.90323519706726
[2m[36m(pid=18484)[0m Finished run with seed 0 - lr 0.01 - sec_lr 0.1 - bs 128 - mean val auc: 0.9105523228645325
Result for _inner_e8deb_00067:
  auc: 0.9105523228645325
  date: 2021-03-19_12-48-50
  done: false
  experiment_id: 53ae9a21c3a1485295248bc02c61d7b1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 18484
  time_since_restore: 1348.5914278030396
  time_this_iter_s: 1348.5914278030396
  time_total_s: 1348.5914278030396
  timestamp: 1616154530
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00067
  
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 84/180 (1 PENDING, 26 RUNNING, 57 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |       |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |       |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00083 | PENDING    |       |          256 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 64 more trials not shown (16 RUNNING, 47 TERMINATED)


Result for _inner_e8deb_00067:
  auc: 0.9105523228645325
  date: 2021-03-19_12-48-50
  done: true
  experiment_id: 53ae9a21c3a1485295248bc02c61d7b1
  experiment_tag: 67_batch_size=128,eta=0.0,lr=0.01,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 18484
  time_since_restore: 1348.5914278030396
  time_this_iter_s: 1348.5914278030396
  time_total_s: 1348.5914278030396
  timestamp: 1616154530
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00067
  
[2m[36m(pid=51937)[0m time to fit was 109.27990531921387
[2m[36m(pid=51937)[0m GPU available: False, used: False
[2m[36m(pid=51937)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=51937)[0m 
[2m[36m(pid=51937)[0m   | Name      | Type              | Params
[2m[36m(pid=51937)[0m ------------------------------------------------
[2m[36m(pid=51937)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=51937)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=51937)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=51937)[0m ------------------------------------------------
[2m[36m(pid=51937)[0m 8.7 K     Trainable params
[2m[36m(pid=51937)[0m 0         Non-trainable params
[2m[36m(pid=51937)[0m 8.7 K     Total params
[2m[36m(pid=10131)[0m Starting run with seed 0 - lr 2 - sec_lr 0.1 - bs 256
[2m[36m(pid=10131)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10131)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=10131)[0m GPU available: False, used: False
[2m[36m(pid=10131)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10131)[0m 
[2m[36m(pid=10131)[0m   | Name      | Type              | Params
[2m[36m(pid=10131)[0m ------------------------------------------------
[2m[36m(pid=10131)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10131)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10131)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10131)[0m ------------------------------------------------
[2m[36m(pid=10131)[0m 8.7 K     Trainable params
[2m[36m(pid=10131)[0m 0         Non-trainable params
[2m[36m(pid=10131)[0m 8.7 K     Total params
[2m[36m(pid=10131)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=10131)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=10131)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=10131)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=10131)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=10131)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=11277)[0m time to fit was 320.8195562362671
Result for _inner_e8deb_00063:
  auc: 0.9035791516304016
  date: 2021-03-19_12-49-03
  done: false
  experiment_id: 19fb7fce2c1b440b90c9a886acf2afeb
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 11277
  time_since_restore: 1598.4434983730316
  time_this_iter_s: 1598.4434983730316
  time_total_s: 1598.4434983730316
  timestamp: 1616154543
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00063
  
[2m[36m(pid=11277)[0m Finished run with seed 0 - lr 0.001 - sec_lr 0.1 - bs 256 - mean val auc: 0.9035791516304016
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 85/180 (1 PENDING, 26 RUNNING, 58 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |       |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |       |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00084 | PENDING    |       |          512 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 65 more trials not shown (16 RUNNING, 48 TERMINATED)


Result for _inner_e8deb_00063:
  auc: 0.9035791516304016
  date: 2021-03-19_12-49-03
  done: true
  experiment_id: 19fb7fce2c1b440b90c9a886acf2afeb
  experiment_tag: 63_batch_size=256,eta=0.0,lr=0.001,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 11277
  time_since_restore: 1598.4434983730316
  time_this_iter_s: 1598.4434983730316
  time_total_s: 1598.4434983730316
  timestamp: 1616154543
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00063
  
[2m[36m(pid=10483)[0m Starting run with seed 0 - lr 2 - sec_lr 0.1 - bs 512
[2m[36m(pid=10483)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10483)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=10483)[0m GPU available: False, used: False
[2m[36m(pid=10483)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10483)[0m 
[2m[36m(pid=10483)[0m   | Name      | Type              | Params
[2m[36m(pid=10483)[0m ------------------------------------------------
[2m[36m(pid=10483)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10483)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10483)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10483)[0m ------------------------------------------------
[2m[36m(pid=10483)[0m 8.7 K     Trainable params
[2m[36m(pid=10483)[0m 0         Non-trainable params
[2m[36m(pid=10483)[0m 8.7 K     Total params
[2m[36m(pid=10483)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=10483)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=10483)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=10483)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=10483)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=10483)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=2972)[0m time to fit was 81.6033136844635
[2m[36m(pid=2972)[0m GPU available: False, used: False
[2m[36m(pid=2972)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=2972)[0m 
[2m[36m(pid=2972)[0m   | Name      | Type              | Params
[2m[36m(pid=2972)[0m ------------------------------------------------
[2m[36m(pid=2972)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=2972)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=2972)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=2972)[0m ------------------------------------------------
[2m[36m(pid=2972)[0m 8.7 K     Trainable params
[2m[36m(pid=2972)[0m 0         Non-trainable params
[2m[36m(pid=2972)[0m 8.7 K     Total params
[2m[36m(pid=38100)[0m time to fit was 143.63749599456787
Result for _inner_e8deb_00072:
  auc: 0.9121121048927308
  date: 2021-03-19_12-49-28
  done: false
  experiment_id: d35038201b104c048dbdc859ecd21722
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 38100
  time_since_restore: 772.5123858451843
  time_this_iter_s: 772.5123858451843
  time_total_s: 772.5123858451843
  timestamp: 1616154568
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00072
  
[2m[36m(pid=38100)[0m Finished run with seed 0 - lr 0.1 - sec_lr 0.1 - bs 128 - mean val auc: 0.9121121048927308
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 86/180 (1 PENDING, 26 RUNNING, 59 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |       |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |       |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | PENDING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 66 more trials not shown (16 RUNNING, 49 TERMINATED)


Result for _inner_e8deb_00072:
  auc: 0.9121121048927308
  date: 2021-03-19_12-49-28
  done: true
  experiment_id: d35038201b104c048dbdc859ecd21722
  experiment_tag: 72_batch_size=128,eta=0.0,lr=0.1,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 38100
  time_since_restore: 772.5123858451843
  time_this_iter_s: 772.5123858451843
  time_total_s: 772.5123858451843
  timestamp: 1616154568
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00072
  
[2m[36m(pid=6688)[0m time to fit was 1769.3061275482178
[2m[36m(pid=6688)[0m GPU available: False, used: False
[2m[36m(pid=6688)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=6688)[0m 
[2m[36m(pid=6688)[0m   | Name      | Type              | Params
[2m[36m(pid=6688)[0m ------------------------------------------------
[2m[36m(pid=6688)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=6688)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=6688)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=6688)[0m ------------------------------------------------
[2m[36m(pid=6688)[0m 8.7 K     Trainable params
[2m[36m(pid=6688)[0m 0         Non-trainable params
[2m[36m(pid=6688)[0m 8.7 K     Total params
[2m[36m(pid=11272)[0m Starting run with seed 0 - lr 5 - sec_lr 0.1 - bs 32
[2m[36m(pid=11272)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=11272)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=11272)[0m GPU available: False, used: False
[2m[36m(pid=11272)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=11272)[0m 
[2m[36m(pid=11272)[0m   | Name      | Type              | Params
[2m[36m(pid=11272)[0m ------------------------------------------------
[2m[36m(pid=11272)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=11272)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=11272)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=11272)[0m ------------------------------------------------
[2m[36m(pid=11272)[0m 8.7 K     Trainable params
[2m[36m(pid=11272)[0m 0         Non-trainable params
[2m[36m(pid=11272)[0m 8.7 K     Total params
[2m[36m(pid=11272)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=11272)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=11272)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=11272)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=11272)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=11272)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35786)[0m time to fit was 615.3654522895813
[2m[36m(pid=35786)[0m GPU available: False, used: False
[2m[36m(pid=35786)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35786)[0m 
[2m[36m(pid=35786)[0m   | Name      | Type              | Params
[2m[36m(pid=35786)[0m ------------------------------------------------
[2m[36m(pid=35786)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35786)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35786)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35786)[0m ------------------------------------------------
[2m[36m(pid=35786)[0m 8.7 K     Trainable params
[2m[36m(pid=35786)[0m 0         Non-trainable params
[2m[36m(pid=35786)[0m 8.7 K     Total params
[2m[36m(pid=35783)[0m time to fit was 574.6884694099426
[2m[36m(pid=35783)[0m GPU available: False, used: False
[2m[36m(pid=35783)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35783)[0m 
[2m[36m(pid=35783)[0m   | Name      | Type              | Params
[2m[36m(pid=35783)[0m ------------------------------------------------
[2m[36m(pid=35783)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35783)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35783)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35783)[0m ------------------------------------------------
[2m[36m(pid=35783)[0m 8.7 K     Trainable params
[2m[36m(pid=35783)[0m 0         Non-trainable params
[2m[36m(pid=35783)[0m 8.7 K     Total params
[2m[36m(pid=48984)[0m time to fit was 492.3603117465973
[2m[36m(pid=48984)[0m GPU available: False, used: False
[2m[36m(pid=48984)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48984)[0m 
[2m[36m(pid=48984)[0m   | Name      | Type              | Params
[2m[36m(pid=48984)[0m ------------------------------------------------
[2m[36m(pid=48984)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48984)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48984)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48984)[0m ------------------------------------------------
[2m[36m(pid=48984)[0m 8.7 K     Trainable params
[2m[36m(pid=48984)[0m 0         Non-trainable params
[2m[36m(pid=48984)[0m 8.7 K     Total params
[2m[36m(pid=17269)[0m time to fit was 569.8307294845581
[2m[36m(pid=17269)[0m GPU available: False, used: False
[2m[36m(pid=17269)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=17269)[0m 
[2m[36m(pid=17269)[0m   | Name      | Type              | Params
[2m[36m(pid=17269)[0m ------------------------------------------------
[2m[36m(pid=17269)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=17269)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=17269)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=17269)[0m ------------------------------------------------
[2m[36m(pid=17269)[0m 8.7 K     Trainable params
[2m[36m(pid=17269)[0m 0         Non-trainable params
[2m[36m(pid=17269)[0m 8.7 K     Total params
[2m[36m(pid=51937)[0m time to fit was 104.7845528125763
[2m[36m(pid=51937)[0m GPU available: False, used: False
[2m[36m(pid=51937)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=51937)[0m 
[2m[36m(pid=51937)[0m   | Name      | Type              | Params
[2m[36m(pid=51937)[0m ------------------------------------------------
[2m[36m(pid=51937)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=51937)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=51937)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=51937)[0m ------------------------------------------------
[2m[36m(pid=51937)[0m 8.7 K     Trainable params
[2m[36m(pid=51937)[0m 0         Non-trainable params
[2m[36m(pid=51937)[0m 8.7 K     Total params
[2m[36m(pid=2972)[0m time to fit was 80.30054044723511
[2m[36m(pid=2972)[0m GPU available: False, used: False
[2m[36m(pid=2972)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=2972)[0m 
[2m[36m(pid=2972)[0m   | Name      | Type              | Params
[2m[36m(pid=2972)[0m ------------------------------------------------
[2m[36m(pid=2972)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=2972)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=2972)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=2972)[0m ------------------------------------------------
[2m[36m(pid=2972)[0m 8.7 K     Trainable params
[2m[36m(pid=2972)[0m 0         Non-trainable params
[2m[36m(pid=2972)[0m 8.7 K     Total params
[2m[36m(pid=10483)[0m time to fit was 99.485182762146
[2m[36m(pid=10483)[0m GPU available: False, used: False
[2m[36m(pid=10483)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10483)[0m 
[2m[36m(pid=10483)[0m   | Name      | Type              | Params
[2m[36m(pid=10483)[0m ------------------------------------------------
[2m[36m(pid=10483)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10483)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10483)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10483)[0m ------------------------------------------------
[2m[36m(pid=10483)[0m 8.7 K     Trainable params
[2m[36m(pid=10483)[0m 0         Non-trainable params
[2m[36m(pid=10483)[0m 8.7 K     Total params
[2m[36m(pid=50407)[0m time to fit was 187.77335810661316
[2m[36m(pid=50407)[0m GPU available: False, used: False
[2m[36m(pid=50407)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=50407)[0m 
[2m[36m(pid=50407)[0m   | Name      | Type              | Params
[2m[36m(pid=50407)[0m ------------------------------------------------
[2m[36m(pid=50407)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=50407)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=50407)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=50407)[0m ------------------------------------------------
[2m[36m(pid=50407)[0m 8.7 K     Trainable params
[2m[36m(pid=50407)[0m 0         Non-trainable params
[2m[36m(pid=50407)[0m 8.7 K     Total params
[2m[36m(pid=8194)[0m time to fit was 226.9182255268097
[2m[36m(pid=8194)[0m GPU available: False, used: False
[2m[36m(pid=8194)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8194)[0m 
[2m[36m(pid=8194)[0m   | Name      | Type              | Params
[2m[36m(pid=8194)[0m ------------------------------------------------
[2m[36m(pid=8194)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8194)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8194)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8194)[0m ------------------------------------------------
[2m[36m(pid=8194)[0m 8.7 K     Trainable params
[2m[36m(pid=8194)[0m 0         Non-trainable params
[2m[36m(pid=8194)[0m 8.7 K     Total params
[2m[36m(pid=2972)[0m time to fit was 88.46301031112671
Result for _inner_e8deb_00079:
  auc: 0.9136805295944214
  date: 2021-03-19_12-52-09
  done: false
  experiment_id: 09f1fb651b484288af04af0d2325e050
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 2972
  time_since_restore: 410.41789865493774
  time_this_iter_s: 410.41789865493774
  time_total_s: 410.41789865493774
  timestamp: 1616154729
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00079
  
[2m[36m(pid=2972)[0m Finished run with seed 0 - lr 1 - sec_lr 0.1 - bs 512 - mean val auc: 0.9136805295944214
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 87/180 (1 PENDING, 26 RUNNING, 60 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |       |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |       |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00086 | PENDING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 67 more trials not shown (16 RUNNING, 50 TERMINATED)


Result for _inner_e8deb_00079:
  auc: 0.9136805295944214
  date: 2021-03-19_12-52-09
  done: true
  experiment_id: 09f1fb651b484288af04af0d2325e050
  experiment_tag: 79_batch_size=512,eta=0.0,lr=1,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 2972
  time_since_restore: 410.41789865493774
  time_this_iter_s: 410.41789865493774
  time_total_s: 410.41789865493774
  timestamp: 1616154729
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00079
  
[2m[36m(pid=51937)[0m time to fit was 92.37139534950256
Result for _inner_e8deb_00078:
  auc: 0.9127303123474121
  date: 2021-03-19_12-52-12
  done: false
  experiment_id: 140113711af14a8ebce6dcfacbdc2201
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 51937
  time_since_restore: 528.9235529899597
  time_this_iter_s: 528.9235529899597
  time_total_s: 528.9235529899597
  timestamp: 1616154732
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00078
  
Result for _inner_e8deb_00078:
  auc: 0.9127303123474121
  date: 2021-03-19_12-52-12
  done: true
  experiment_id: 140113711af14a8ebce6dcfacbdc2201
  experiment_tag: 78_batch_size=256,eta=0.0,lr=1,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 51937
  time_since_restore: 528.9235529899597
  time_this_iter_s: 528.9235529899597
  time_total_s: 528.9235529899597
  timestamp: 1616154732
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00078
  
[2m[36m(pid=51937)[0m Finished run with seed 0 - lr 1 - sec_lr 0.1 - bs 256 - mean val auc: 0.9127303123474121
[2m[36m(pid=8106)[0m time to fit was 948.8227922916412
[2m[36m(pid=8106)[0m GPU available: False, used: False
[2m[36m(pid=8106)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8106)[0m 
[2m[36m(pid=8106)[0m   | Name      | Type              | Params
[2m[36m(pid=8106)[0m ------------------------------------------------
[2m[36m(pid=8106)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8106)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8106)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8106)[0m ------------------------------------------------
[2m[36m(pid=8106)[0m 8.7 K     Trainable params
[2m[36m(pid=8106)[0m 0         Non-trainable params
[2m[36m(pid=8106)[0m 8.7 K     Total params
[2m[36m(pid=16527)[0m Starting run with seed 0 - lr 5 - sec_lr 0.1 - bs 64
[2m[36m(pid=16527)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=16527)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=16527)[0m GPU available: False, used: False
[2m[36m(pid=16527)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=16527)[0m 
[2m[36m(pid=16527)[0m   | Name      | Type              | Params
[2m[36m(pid=16527)[0m ------------------------------------------------
[2m[36m(pid=16527)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=16527)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=16527)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=16527)[0m ------------------------------------------------
[2m[36m(pid=16527)[0m 8.7 K     Trainable params
[2m[36m(pid=16527)[0m 0         Non-trainable params
[2m[36m(pid=16527)[0m 8.7 K     Total params
[2m[36m(pid=16527)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=16527)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=16527)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=16527)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=16527)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=16527)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49910)[0m time to fit was 377.803751707077
[2m[36m(pid=49910)[0m GPU available: False, used: False
[2m[36m(pid=49910)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=16623)[0m Starting run with seed 0 - lr 5 - sec_lr 0.1 - bs 128
[2m[36m(pid=49910)[0m 
[2m[36m(pid=49910)[0m   | Name      | Type              | Params
[2m[36m(pid=49910)[0m ------------------------------------------------
[2m[36m(pid=49910)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49910)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49910)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49910)[0m ------------------------------------------------
[2m[36m(pid=49910)[0m 8.7 K     Trainable params
[2m[36m(pid=49910)[0m 0         Non-trainable params
[2m[36m(pid=49910)[0m 8.7 K     Total params
[2m[36m(pid=16623)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=16623)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=16623)[0m GPU available: False, used: False
[2m[36m(pid=16623)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=16623)[0m 
[2m[36m(pid=16623)[0m   | Name      | Type              | Params
[2m[36m(pid=16623)[0m ------------------------------------------------
[2m[36m(pid=16623)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=16623)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=16623)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=16623)[0m ------------------------------------------------
[2m[36m(pid=16623)[0m 8.7 K     Trainable params
[2m[36m(pid=16623)[0m 0         Non-trainable params
[2m[36m(pid=16623)[0m 8.7 K     Total params
[2m[36m(pid=16623)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=16623)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=10131)[0m time to fit was 201.63339972496033
[2m[36m(pid=10131)[0m GPU available: False, used: False
[2m[36m(pid=10131)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10131)[0m 
[2m[36m(pid=10131)[0m   | Name      | Type              | Params
[2m[36m(pid=10131)[0m ------------------------------------------------
[2m[36m(pid=10131)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10131)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10131)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10131)[0m ------------------------------------------------
[2m[36m(pid=10131)[0m 8.7 K     Trainable params
[2m[36m(pid=10131)[0m 0         Non-trainable params
[2m[36m(pid=10131)[0m 8.7 K     Total params
[2m[36m(pid=16623)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=16623)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=16623)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=16623)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=10483)[0m time to fit was 111.53405952453613
[2m[36m(pid=10483)[0m GPU available: False, used: False
[2m[36m(pid=10483)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10483)[0m 
[2m[36m(pid=10483)[0m   | Name      | Type              | Params
[2m[36m(pid=10483)[0m ------------------------------------------------
[2m[36m(pid=10483)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10483)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10483)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10483)[0m ------------------------------------------------
[2m[36m(pid=10483)[0m 8.7 K     Trainable params
[2m[36m(pid=10483)[0m 0         Non-trainable params
[2m[36m(pid=10483)[0m 8.7 K     Total params
[2m[36m(pid=6537)[0m time to fit was 354.94371724128723
[2m[36m(pid=6537)[0m GPU available: False, used: False
[2m[36m(pid=6537)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=6537)[0m 
[2m[36m(pid=6537)[0m   | Name      | Type              | Params
[2m[36m(pid=6537)[0m ------------------------------------------------
[2m[36m(pid=6537)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=6537)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=6537)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=6537)[0m ------------------------------------------------
[2m[36m(pid=6537)[0m 8.7 K     Trainable params
[2m[36m(pid=6537)[0m 0         Non-trainable params
[2m[36m(pid=6537)[0m 8.7 K     Total params
[2m[36m(pid=35861)[0m time to fit was 1773.9416615962982
[2m[36m(pid=35861)[0m GPU available: False, used: False
[2m[36m(pid=35861)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35861)[0m 
[2m[36m(pid=35861)[0m   | Name      | Type              | Params
[2m[36m(pid=35861)[0m ------------------------------------------------
[2m[36m(pid=35861)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35861)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35861)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35861)[0m ------------------------------------------------
[2m[36m(pid=35861)[0m 8.7 K     Trainable params
[2m[36m(pid=35861)[0m 0         Non-trainable params
[2m[36m(pid=35861)[0m 8.7 K     Total params
[2m[36m(pid=10483)[0m time to fit was 40.22577261924744
[2m[36m(pid=10483)[0m GPU available: False, used: False
[2m[36m(pid=10483)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10483)[0m 
[2m[36m(pid=10483)[0m   | Name      | Type              | Params
[2m[36m(pid=10483)[0m ------------------------------------------------
[2m[36m(pid=10483)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10483)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10483)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10483)[0m ------------------------------------------------
[2m[36m(pid=10483)[0m 8.7 K     Trainable params
[2m[36m(pid=10483)[0m 0         Non-trainable params
[2m[36m(pid=10483)[0m 8.7 K     Total params
[2m[36m(pid=50407)[0m time to fit was 145.80904364585876
[2m[36m(pid=50407)[0m GPU available: False, used: False
[2m[36m(pid=50407)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=50407)[0m 
[2m[36m(pid=50407)[0m   | Name      | Type              | Params
[2m[36m(pid=50407)[0m ------------------------------------------------
[2m[36m(pid=50407)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=50407)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=50407)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=50407)[0m ------------------------------------------------
[2m[36m(pid=50407)[0m 8.7 K     Trainable params
[2m[36m(pid=50407)[0m 0         Non-trainable params
[2m[36m(pid=50407)[0m 8.7 K     Total params
[2m[36m(pid=10131)[0m time to fit was 76.92854571342468
[2m[36m(pid=10131)[0m GPU available: False, used: False
[2m[36m(pid=10131)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10131)[0m 
[2m[36m(pid=10131)[0m   | Name      | Type              | Params
[2m[36m(pid=10131)[0m ------------------------------------------------
[2m[36m(pid=10131)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10131)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10131)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10131)[0m ------------------------------------------------
[2m[36m(pid=10131)[0m 8.7 K     Trainable params
[2m[36m(pid=10131)[0m 0         Non-trainable params
[2m[36m(pid=10131)[0m 8.7 K     Total params
[2m[36m(pid=32133)[0m time to fit was 314.70971488952637
[2m[36m(pid=32133)[0m GPU available: False, used: False
[2m[36m(pid=32133)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=32133)[0m 
[2m[36m(pid=32133)[0m   | Name      | Type              | Params
[2m[36m(pid=32133)[0m ------------------------------------------------
[2m[36m(pid=32133)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=32133)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=32133)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=32133)[0m ------------------------------------------------
[2m[36m(pid=32133)[0m 8.7 K     Trainable params
[2m[36m(pid=32133)[0m 0         Non-trainable params
[2m[36m(pid=32133)[0m 8.7 K     Total params
[2m[36m(pid=10483)[0m time to fit was 40.97374105453491
[2m[36m(pid=10483)[0m GPU available: False, used: False
[2m[36m(pid=10483)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10483)[0m 
[2m[36m(pid=10483)[0m   | Name      | Type              | Params
[2m[36m(pid=10483)[0m ------------------------------------------------
[2m[36m(pid=10483)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10483)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10483)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10483)[0m ------------------------------------------------
[2m[36m(pid=10483)[0m 8.7 K     Trainable params
[2m[36m(pid=10483)[0m 0         Non-trainable params
[2m[36m(pid=10483)[0m 8.7 K     Total params
[2m[36m(pid=48139)[0m time to fit was 393.6576292514801
[2m[36m(pid=48139)[0m GPU available: False, used: False
[2m[36m(pid=48139)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48139)[0m 
[2m[36m(pid=48139)[0m   | Name      | Type              | Params
[2m[36m(pid=48139)[0m ------------------------------------------------
[2m[36m(pid=48139)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48139)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48139)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48139)[0m ------------------------------------------------
[2m[36m(pid=48139)[0m 8.7 K     Trainable params
[2m[36m(pid=48139)[0m 0         Non-trainable params
[2m[36m(pid=48139)[0m 8.7 K     Total params
[2m[36m(pid=16623)[0m time to fit was 133.182678937912
[2m[36m(pid=16623)[0m GPU available: False, used: False
[2m[36m(pid=16623)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=16623)[0m 
[2m[36m(pid=16623)[0m   | Name      | Type              | Params
[2m[36m(pid=16623)[0m ------------------------------------------------
[2m[36m(pid=16623)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=16623)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=16623)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=16623)[0m ------------------------------------------------
[2m[36m(pid=16623)[0m 8.7 K     Trainable params
[2m[36m(pid=16623)[0m 0         Non-trainable params
[2m[36m(pid=16623)[0m 8.7 K     Total params
[2m[36m(pid=50407)[0m time to fit was 123.53909945487976
Result for _inner_e8deb_00077:
  auc: 0.9121889114379883
  date: 2021-03-19_12-55-34
  done: false
  experiment_id: ea421e39265c4c9e8e734d9b4e6d7faa
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 50407
  time_since_restore: 781.7889103889465
  time_this_iter_s: 781.7889103889465
  time_total_s: 781.7889103889465
  timestamp: 1616154934
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00077
  
[2m[36m(pid=50407)[0m Finished run with seed 0 - lr 1 - sec_lr 0.1 - bs 128 - mean val auc: 0.9121889114379883
== Status ==
Memory usage on this node: 9.7/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 89/180 (1 PENDING, 26 RUNNING, 62 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |       |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |       |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00088 | PENDING    |       |          256 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 69 more trials not shown (16 RUNNING, 52 TERMINATED)


Result for _inner_e8deb_00077:
  auc: 0.9121889114379883
  date: 2021-03-19_12-55-34
  done: true
  experiment_id: ea421e39265c4c9e8e734d9b4e6d7faa
  experiment_tag: 77_batch_size=128,eta=0.0,lr=1,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 50407
  time_since_restore: 781.7889103889465
  time_this_iter_s: 781.7889103889465
  time_total_s: 781.7889103889465
  timestamp: 1616154934
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00077
  
[2m[36m(pid=5013)[0m time to fit was 562.5708973407745
[2m[36m(pid=5013)[0m GPU available: False, used: False
[2m[36m(pid=5013)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=5013)[0m 
[2m[36m(pid=5013)[0m   | Name      | Type              | Params
[2m[36m(pid=5013)[0m ------------------------------------------------
[2m[36m(pid=5013)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=5013)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=5013)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=5013)[0m ------------------------------------------------
[2m[36m(pid=5013)[0m 8.7 K     Trainable params
[2m[36m(pid=5013)[0m 0         Non-trainable params
[2m[36m(pid=5013)[0m 8.7 K     Total params
[2m[36m(pid=22000)[0m Starting run with seed 0 - lr 5 - sec_lr 0.1 - bs 256
[2m[36m(pid=22000)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=22000)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=22000)[0m GPU available: False, used: False
[2m[36m(pid=22000)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=22000)[0m 
[2m[36m(pid=22000)[0m   | Name      | Type              | Params
[2m[36m(pid=22000)[0m ------------------------------------------------
[2m[36m(pid=22000)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=22000)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=22000)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=22000)[0m ------------------------------------------------
[2m[36m(pid=22000)[0m 8.7 K     Trainable params
[2m[36m(pid=22000)[0m 0         Non-trainable params
[2m[36m(pid=22000)[0m 8.7 K     Total params
[2m[36m(pid=22000)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=22000)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=22000)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=22000)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=22000)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=22000)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8194)[0m time to fit was 244.35230445861816
[2m[36m(pid=8194)[0m GPU available: False, used: False
[2m[36m(pid=8194)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8194)[0m 
[2m[36m(pid=8194)[0m   | Name      | Type              | Params
[2m[36m(pid=8194)[0m ------------------------------------------------
[2m[36m(pid=8194)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8194)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8194)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8194)[0m ------------------------------------------------
[2m[36m(pid=8194)[0m 8.7 K     Trainable params
[2m[36m(pid=8194)[0m 0         Non-trainable params
[2m[36m(pid=8194)[0m 8.7 K     Total params
[2m[36m(pid=10483)[0m time to fit was 109.65090274810791
Result for _inner_e8deb_00084:
  auc: 0.7458271384239197
  date: 2021-03-19_12-55-57
  done: false
  experiment_id: fdfd3393211648ca96fa0140356e5012
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 10483
  time_since_restore: 403.06460404396057
  time_this_iter_s: 403.06460404396057
  time_total_s: 403.06460404396057
  timestamp: 1616154957
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00084
  
[2m[36m(pid=10483)[0m Finished run with seed 0 - lr 2 - sec_lr 0.1 - bs 512 - mean val auc: 0.7458271384239197
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 90/180 (1 PENDING, 26 RUNNING, 63 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |       |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |       |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00089 | PENDING    |       |          512 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 70 more trials not shown (16 RUNNING, 53 TERMINATED)


Result for _inner_e8deb_00084:
  auc: 0.7458271384239197
  date: 2021-03-19_12-55-57
  done: true
  experiment_id: fdfd3393211648ca96fa0140356e5012
  experiment_tag: 84_batch_size=512,eta=0.0,lr=2,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 10483
  time_since_restore: 403.06460404396057
  time_this_iter_s: 403.06460404396057
  time_total_s: 403.06460404396057
  timestamp: 1616154957
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00084
  
2021-03-19 12:55:58,262	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffffb5dea1c301000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=22652)[0m Starting run with seed 0 - lr 5 - sec_lr 0.1 - bs 512
[2m[36m(pid=22652)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=22652)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=22652)[0m GPU available: False, used: False
[2m[36m(pid=22652)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=22652)[0m 
[2m[36m(pid=22652)[0m   | Name      | Type              | Params
[2m[36m(pid=22652)[0m ------------------------------------------------
[2m[36m(pid=22652)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=22652)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=22652)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=22652)[0m ------------------------------------------------
[2m[36m(pid=22652)[0m 8.7 K     Trainable params
[2m[36m(pid=22652)[0m 0         Non-trainable params
[2m[36m(pid=22652)[0m 8.7 K     Total params
[2m[36m(pid=22652)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=22652)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=11272)[0m time to fit was 387.44602394104004
[2m[36m(pid=11272)[0m GPU available: False, used: False
[2m[36m(pid=11272)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=11272)[0m 
[2m[36m(pid=11272)[0m   | Name      | Type              | Params
[2m[36m(pid=11272)[0m ------------------------------------------------
[2m[36m(pid=11272)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=11272)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=11272)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=11272)[0m ------------------------------------------------
[2m[36m(pid=11272)[0m 8.7 K     Trainable params
[2m[36m(pid=11272)[0m 0         Non-trainable params
[2m[36m(pid=11272)[0m 8.7 K     Total params
[2m[36m(pid=22652)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=22652)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=22652)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=22652)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=10131)[0m time to fit was 150.4648916721344
[2m[36m(pid=10131)[0m GPU available: False, used: False
[2m[36m(pid=10131)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10131)[0m 
[2m[36m(pid=10131)[0m   | Name      | Type              | Params
[2m[36m(pid=10131)[0m ------------------------------------------------
[2m[36m(pid=10131)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10131)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10131)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10131)[0m ------------------------------------------------
[2m[36m(pid=10131)[0m 8.7 K     Trainable params
[2m[36m(pid=10131)[0m 0         Non-trainable params
[2m[36m(pid=10131)[0m 8.7 K     Total params
[2m[36m(pid=16623)[0m time to fit was 111.11309051513672
[2m[36m(pid=16623)[0m GPU available: False, used: False
[2m[36m(pid=16623)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=16623)[0m 
[2m[36m(pid=16623)[0m   | Name      | Type              | Params
[2m[36m(pid=16623)[0m ------------------------------------------------
[2m[36m(pid=16623)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=16623)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=16623)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=16623)[0m ------------------------------------------------
[2m[36m(pid=16623)[0m 8.7 K     Trainable params
[2m[36m(pid=16623)[0m 0         Non-trainable params
[2m[36m(pid=16623)[0m 8.7 K     Total params
[2m[36m(pid=23585)[0m time to fit was 494.19949746131897
[2m[36m(pid=23585)[0m GPU available: False, used: False
[2m[36m(pid=23585)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23585)[0m 
[2m[36m(pid=23585)[0m   | Name      | Type              | Params
[2m[36m(pid=23585)[0m ------------------------------------------------
[2m[36m(pid=23585)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23585)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23585)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23585)[0m ------------------------------------------------
[2m[36m(pid=23585)[0m 8.7 K     Trainable params
[2m[36m(pid=23585)[0m 0         Non-trainable params
[2m[36m(pid=23585)[0m 8.7 K     Total params
[2m[36m(pid=22000)[0m time to fit was 84.14587020874023
[2m[36m(pid=22000)[0m GPU available: False, used: False
[2m[36m(pid=22000)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=22000)[0m 
[2m[36m(pid=22000)[0m   | Name      | Type              | Params
[2m[36m(pid=22000)[0m ------------------------------------------------
[2m[36m(pid=22000)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=22000)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=22000)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=22000)[0m ------------------------------------------------
[2m[36m(pid=22000)[0m 8.7 K     Trainable params
[2m[36m(pid=22000)[0m 0         Non-trainable params
[2m[36m(pid=22000)[0m 8.7 K     Total params
[2m[36m(pid=10108)[0m time to fit was 526.6630215644836
[2m[36m(pid=10108)[0m GPU available: False, used: False
[2m[36m(pid=10108)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10108)[0m 
[2m[36m(pid=10108)[0m   | Name      | Type              | Params
[2m[36m(pid=10108)[0m ------------------------------------------------
[2m[36m(pid=10108)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10108)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10108)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10108)[0m ------------------------------------------------
[2m[36m(pid=10108)[0m 8.7 K     Trainable params
[2m[36m(pid=10108)[0m 0         Non-trainable params
[2m[36m(pid=10108)[0m 8.7 K     Total params
[2m[36m(pid=49910)[0m time to fit was 299.1935238838196
[2m[36m(pid=49910)[0m GPU available: False, used: False
[2m[36m(pid=49910)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49910)[0m 
[2m[36m(pid=49910)[0m   | Name      | Type              | Params
[2m[36m(pid=49910)[0m ------------------------------------------------
[2m[36m(pid=49910)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49910)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49910)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49910)[0m ------------------------------------------------
[2m[36m(pid=49910)[0m 8.7 K     Trainable params
[2m[36m(pid=49910)[0m 0         Non-trainable params
[2m[36m(pid=49910)[0m 8.7 K     Total params
[2m[36m(pid=32133)[0m time to fit was 219.10373759269714
Result for _inner_e8deb_00071:
  auc: 0.9119049906730652
  date: 2021-03-19_12-57-23
  done: false
  experiment_id: 2df42e0f13ac4c7c854b899e008cc08f
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 32133
  time_since_restore: 1420.840343952179
  time_this_iter_s: 1420.840343952179
  time_total_s: 1420.840343952179
  timestamp: 1616155043
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00071
  
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 91/180 (1 PENDING, 26 RUNNING, 64 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |       |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    |       |           32 |     0 | 0.01  |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    |       |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |       |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00090 | PENDING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 71 more trials not shown (16 RUNNING, 54 TERMINATED)


Result for _inner_e8deb_00071:
  auc: 0.9119049906730652
  date: 2021-03-19_12-57-23
  done: true
  experiment_id: 2df42e0f13ac4c7c854b899e008cc08f
  experiment_tag: 71_batch_size=64,eta=0.0,lr=0.1,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 32133
  time_since_restore: 1420.840343952179
  time_this_iter_s: 1420.840343952179
  time_total_s: 1420.840343952179
  timestamp: 1616155043
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00071
  
[2m[36m(pid=32133)[0m Finished run with seed 0 - lr 0.1 - sec_lr 0.1 - bs 64 - mean val auc: 0.9119049906730652
[2m[36m(pid=25102)[0m Starting run with seed 0 - lr 0.001 - sec_lr 1 - bs 32
[2m[36m(pid=25102)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=25102)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=25102)[0m GPU available: False, used: False
[2m[36m(pid=25102)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=25102)[0m 
[2m[36m(pid=25102)[0m   | Name      | Type              | Params
[2m[36m(pid=25102)[0m ------------------------------------------------
[2m[36m(pid=25102)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=25102)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=25102)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=25102)[0m ------------------------------------------------
[2m[36m(pid=25102)[0m 8.7 K     Trainable params
[2m[36m(pid=25102)[0m 0         Non-trainable params
[2m[36m(pid=25102)[0m 8.7 K     Total params
[2m[36m(pid=25102)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=25102)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=25102)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=25102)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=25102)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=25102)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=22652)[0m time to fit was 102.90171551704407
[2m[36m(pid=22652)[0m GPU available: False, used: False
[2m[36m(pid=22652)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=22652)[0m 
[2m[36m(pid=22652)[0m   | Name      | Type              | Params
[2m[36m(pid=22652)[0m ------------------------------------------------
[2m[36m(pid=22652)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=22652)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=22652)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=22652)[0m ------------------------------------------------
[2m[36m(pid=22652)[0m 8.7 K     Trainable params
[2m[36m(pid=22652)[0m 0         Non-trainable params
[2m[36m(pid=22652)[0m 8.7 K     Total params
[2m[36m(pid=35802)[0m time to fit was 753.0370688438416
Result for _inner_e8deb_00035:
  auc: 0.9121662497520446
  date: 2021-03-19_12-58-10
  done: false
  experiment_id: 939277547c2d4ba4bcdce52d65c3998f
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35802
  time_since_restore: 4552.963782310486
  time_this_iter_s: 4552.963782310486
  time_total_s: 4552.963782310486
  timestamp: 1616155090
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00035
  
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 92/180 (1 PENDING, 26 RUNNING, 65 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    |                     |           64 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00035 | RUNNING    | 145.101.32.82:35802 |           32 |     0 | 0.01  |    0.01  |      1 |          4552.96 | 0.912166 |
| _inner_e8deb_00045 | RUNNING    |                     |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |                     |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |                     |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |                     |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00091 | PENDING    |                     |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 72 more trials not shown (16 RUNNING, 55 TERMINATED)

[2m[36m(pid=35802)[0m Finished run with seed 0 - lr 0.01 - sec_lr 0.01 - bs 32 - mean val auc: 0.9121662497520446

Result for _inner_e8deb_00035:
  auc: 0.9121662497520446
  date: 2021-03-19_12-58-10
  done: true
  experiment_id: 939277547c2d4ba4bcdce52d65c3998f
  experiment_tag: 35_batch_size=32,eta=0.0,lr=0.01,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35802
  time_since_restore: 4552.963782310486
  time_this_iter_s: 4552.963782310486
  time_total_s: 4552.963782310486
  timestamp: 1616155090
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00035
  
[2m[36m(pid=16623)[0m time to fit was 112.0570662021637
[2m[36m(pid=16623)[0m GPU available: False, used: False
[2m[36m(pid=16623)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=16623)[0m 
[2m[36m(pid=16623)[0m   | Name      | Type              | Params
[2m[36m(pid=16623)[0m ------------------------------------------------
[2m[36m(pid=16623)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=16623)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=16623)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=16623)[0m ------------------------------------------------
[2m[36m(pid=16623)[0m 8.7 K     Trainable params
[2m[36m(pid=16623)[0m 0         Non-trainable params
[2m[36m(pid=16623)[0m 8.7 K     Total params
[2m[36m(pid=26407)[0m Starting run with seed 0 - lr 0.001 - sec_lr 1 - bs 64
[2m[36m(pid=26407)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=26407)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=26407)[0m GPU available: False, used: False
[2m[36m(pid=26407)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26407)[0m 
[2m[36m(pid=26407)[0m   | Name      | Type              | Params
[2m[36m(pid=26407)[0m ------------------------------------------------
[2m[36m(pid=26407)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26407)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26407)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26407)[0m ------------------------------------------------
[2m[36m(pid=26407)[0m 8.7 K     Trainable params
[2m[36m(pid=26407)[0m 0         Non-trainable params
[2m[36m(pid=26407)[0m 8.7 K     Total params
[2m[36m(pid=26407)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=26407)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=26407)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=26407)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=26407)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=26407)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=17269)[0m time to fit was 481.930025100708
[2m[36m(pid=17269)[0m GPU available: False, used: False
[2m[36m(pid=17269)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=17269)[0m 
[2m[36m(pid=17269)[0m   | Name      | Type              | Params
[2m[36m(pid=17269)[0m ------------------------------------------------
[2m[36m(pid=17269)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=17269)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=17269)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=17269)[0m ------------------------------------------------
[2m[36m(pid=17269)[0m 8.7 K     Trainable params
[2m[36m(pid=17269)[0m 0         Non-trainable params
[2m[36m(pid=17269)[0m 8.7 K     Total params
[2m[36m(pid=13798)[0m time to fit was 915.9483923912048
[2m[36m(pid=13798)[0m GPU available: False, used: False
[2m[36m(pid=13798)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=13798)[0m 
[2m[36m(pid=13798)[0m   | Name      | Type              | Params
[2m[36m(pid=13798)[0m ------------------------------------------------
[2m[36m(pid=13798)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=13798)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=13798)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=13798)[0m ------------------------------------------------
[2m[36m(pid=13798)[0m 8.7 K     Trainable params
[2m[36m(pid=13798)[0m 0         Non-trainable params
[2m[36m(pid=13798)[0m 8.7 K     Total params
[2m[36m(pid=16527)[0m time to fit was 382.88045620918274
[2m[36m(pid=16527)[0m GPU available: False, used: False
[2m[36m(pid=16527)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=16527)[0m 
[2m[36m(pid=16527)[0m   | Name      | Type              | Params
[2m[36m(pid=16527)[0m ------------------------------------------------
[2m[36m(pid=16527)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=16527)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=16527)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=16527)[0m ------------------------------------------------
[2m[36m(pid=16527)[0m 8.7 K     Trainable params
[2m[36m(pid=16527)[0m 0         Non-trainable params
[2m[36m(pid=16527)[0m 8.7 K     Total params
[2m[36m(pid=6537)[0m time to fit was 352.3970847129822
[2m[36m(pid=10131)[0m time to fit was 161.100013256073
[2m[36m(pid=6537)[0m GPU available: False, used: False
[2m[36m(pid=6537)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=6537)[0m 
[2m[36m(pid=6537)[0m   | Name      | Type              | Params
[2m[36m(pid=6537)[0m ------------------------------------------------
[2m[36m(pid=6537)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=6537)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=6537)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=6537)[0m ------------------------------------------------
[2m[36m(pid=6537)[0m 8.7 K     Trainable params
[2m[36m(pid=6537)[0m 0         Non-trainable params
[2m[36m(pid=6537)[0m 8.7 K     Total params
[2m[36m(pid=10131)[0m GPU available: False, used: False
[2m[36m(pid=10131)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10131)[0m 
[2m[36m(pid=10131)[0m   | Name      | Type              | Params
[2m[36m(pid=10131)[0m ------------------------------------------------
[2m[36m(pid=10131)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10131)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10131)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10131)[0m ------------------------------------------------
[2m[36m(pid=10131)[0m 8.7 K     Trainable params
[2m[36m(pid=10131)[0m 0         Non-trainable params
[2m[36m(pid=10131)[0m 8.7 K     Total params
[2m[36m(pid=48984)[0m time to fit was 526.8692076206207
[2m[36m(pid=48984)[0m GPU available: False, used: False
[2m[36m(pid=48984)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48984)[0m 
[2m[36m(pid=48984)[0m   | Name      | Type              | Params
[2m[36m(pid=48984)[0m ------------------------------------------------
[2m[36m(pid=48984)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48984)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48984)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48984)[0m ------------------------------------------------
[2m[36m(pid=48984)[0m 8.7 K     Trainable params
[2m[36m(pid=48984)[0m 0         Non-trainable params
[2m[36m(pid=48984)[0m 8.7 K     Total params
[2m[36m(pid=35818)[0m time to fit was 952.8976919651031
[2m[36m(pid=35818)[0m Finished run with seed 0 - lr 0.001 - sec_lr 0.01 - bs 64 - mean val auc: 0.9074962377548218
Result for _inner_e8deb_00031:
  auc: 0.9074962377548218
  date: 2021-03-19_12-59-01
  done: false
  experiment_id: 1240632578e14dd6a6d1fa716e1fab75
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35818
  time_since_restore: 4843.980869054794
  time_this_iter_s: 4843.980869054794
  time_total_s: 4843.980869054794
  timestamp: 1616155141
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00031
  
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 93/180 (1 PENDING, 26 RUNNING, 66 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00031 | RUNNING    | 145.101.32.82:35818 |           64 |     0 | 0.001 |    0.01  |      1 |          4843.98 | 0.907496 |
| _inner_e8deb_00045 | RUNNING    |                     |           32 |     0 | 1     |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |                     |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |                     |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |                     |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00062 | RUNNING    |                     |          128 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00092 | PENDING    |                     |          128 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 73 more trials not shown (16 RUNNING, 56 TERMINATED)


Result for _inner_e8deb_00031:
  auc: 0.9074962377548218
  date: 2021-03-19_12-59-01
  done: true
  experiment_id: 1240632578e14dd6a6d1fa716e1fab75
  experiment_tag: 31_batch_size=64,eta=0.0,lr=0.001,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35818
  time_since_restore: 4843.980869054794
  time_this_iter_s: 4843.980869054794
  time_total_s: 4843.980869054794
  timestamp: 1616155141
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00031
  
[2m[36m(pid=27849)[0m Starting run with seed 0 - lr 0.001 - sec_lr 1 - bs 128
[2m[36m(pid=27849)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=27849)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=27849)[0m GPU available: False, used: False
[2m[36m(pid=27849)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27849)[0m 
[2m[36m(pid=27849)[0m   | Name      | Type              | Params
[2m[36m(pid=27849)[0m ------------------------------------------------
[2m[36m(pid=27849)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27849)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27849)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27849)[0m ------------------------------------------------
[2m[36m(pid=27849)[0m 8.7 K     Trainable params
[2m[36m(pid=27849)[0m 0         Non-trainable params
[2m[36m(pid=27849)[0m 8.7 K     Total params
[2m[36m(pid=27849)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=27849)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=27849)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=27849)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=27849)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=27849)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35786)[0m time to fit was 566.6515576839447
Result for _inner_e8deb_00045:
  auc: 0.9120644092559814
  date: 2021-03-19_12-59-17
  done: false
  experiment_id: 63041f6f636042e9afadcda75554abbf
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35786
  time_since_restore: 3483.471924304962
  time_this_iter_s: 3483.471924304962
  time_total_s: 3483.471924304962
  timestamp: 1616155157
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00045
  
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 94/180 (1 PENDING, 26 RUNNING, 67 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00045 | RUNNING    | 145.101.32.82:35786 |           32 |     0 | 1     |    0.01  |      1 |          3483.47 | 0.912064 |
| _inner_e8deb_00050 | RUNNING    |                     |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |                     |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |                     |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00062 | RUNNING    |                     |          128 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00065 | RUNNING    |                     |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00093 | PENDING    |                     |          256 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 74 more trials not shown (16 RUNNING, 57 TERMINATED)

[2m[36m(pid=35786)[0m Finished run with seed 0 - lr 1 - sec_lr 0.01 - bs 32 - mean val auc: 0.9120644092559814

Result for _inner_e8deb_00045:
  auc: 0.9120644092559814
  date: 2021-03-19_12-59-17
  done: true
  experiment_id: 63041f6f636042e9afadcda75554abbf
  experiment_tag: 45_batch_size=32,eta=0.0,lr=1,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35786
  time_since_restore: 3483.471924304962
  time_this_iter_s: 3483.471924304962
  time_total_s: 3483.471924304962
  timestamp: 1616155157
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00045
  
2021-03-19 12:59:18,757	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff515c9cc401000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=28288)[0m Starting run with seed 0 - lr 0.001 - sec_lr 1 - bs 256
[2m[36m(pid=28288)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=28288)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=28288)[0m GPU available: False, used: False
[2m[36m(pid=28288)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=28288)[0m 
[2m[36m(pid=28288)[0m   | Name      | Type              | Params
[2m[36m(pid=28288)[0m ------------------------------------------------
[2m[36m(pid=28288)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=28288)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=28288)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=28288)[0m ------------------------------------------------
[2m[36m(pid=28288)[0m 8.7 K     Trainable params
[2m[36m(pid=28288)[0m 0         Non-trainable params
[2m[36m(pid=28288)[0m 8.7 K     Total params
[2m[36m(pid=28288)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=28288)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=28288)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=28288)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=28288)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=28288)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=10131)[0m time to fit was 65.73429536819458
Result for _inner_e8deb_00083:
  auc: 0.7467016100883483
  date: 2021-03-19_12-59-58
  done: false
  experiment_id: 91c8c2e8154a43298015f67c83d00250
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 10131
  time_since_restore: 657.0884227752686
  time_this_iter_s: 657.0884227752686
  time_total_s: 657.0884227752686
  timestamp: 1616155198
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00083
  
[2m[36m(pid=10131)[0m Finished run with seed 0 - lr 2 - sec_lr 0.1 - bs 256 - mean val auc: 0.7467016100883483
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 95/180 (1 PENDING, 26 RUNNING, 68 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00050 | RUNNING    |       |           32 |     0 | 2     |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00062 | RUNNING    |       |          128 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00065 | RUNNING    |       |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00066 | RUNNING    |       |           64 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00094 | PENDING    |       |          512 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 75 more trials not shown (16 RUNNING, 58 TERMINATED)


Result for _inner_e8deb_00083:
  auc: 0.7467016100883483
  date: 2021-03-19_12-59-58
  done: true
  experiment_id: 91c8c2e8154a43298015f67c83d00250
  experiment_tag: 83_batch_size=256,eta=0.0,lr=2,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 10131
  time_since_restore: 657.0884227752686
  time_this_iter_s: 657.0884227752686
  time_total_s: 657.0884227752686
  timestamp: 1616155198
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00083
  
[2m[36m(pid=35783)[0m time to fit was 600.2336165904999
Result for _inner_e8deb_00050:
  auc: 0.8101484656333924
  date: 2021-03-19_13-00-02
  done: false
  experiment_id: 9bae0a99ed7c4c60a91b186477159be3
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35783
  time_since_restore: 3060.309981584549
  time_this_iter_s: 3060.309981584549
  time_total_s: 3060.309981584549
  timestamp: 1616155202
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00050
  
[2m[36m(pid=35783)[0m Finished run with seed 0 - lr 2 - sec_lr 0.01 - bs 32 - mean val auc: 0.8101484656333924
Result for _inner_e8deb_00050:
  auc: 0.8101484656333924
  date: 2021-03-19_13-00-02
  done: true
  experiment_id: 9bae0a99ed7c4c60a91b186477159be3
  experiment_tag: 50_batch_size=32,eta=0.0,lr=2,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35783
  time_since_restore: 3060.309981584549
  time_this_iter_s: 3060.309981584549
  time_total_s: 3060.309981584549
  timestamp: 1616155202
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00050
  
[2m[36m(pid=29516)[0m Starting run with seed 0 - lr 0.001 - sec_lr 1 - bs 512
[2m[36m(pid=29516)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=29516)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=29516)[0m GPU available: False, used: False
[2m[36m(pid=29516)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29516)[0m 
[2m[36m(pid=29516)[0m   | Name      | Type              | Params
[2m[36m(pid=29516)[0m ------------------------------------------------
[2m[36m(pid=29516)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29516)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29516)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29516)[0m ------------------------------------------------
[2m[36m(pid=29516)[0m 8.7 K     Trainable params
[2m[36m(pid=29516)[0m 0         Non-trainable params
[2m[36m(pid=29516)[0m 8.7 K     Total params
[2m[36m(pid=29516)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=29516)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=29516)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=29516)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=29516)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=29516)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=29675)[0m Starting run with seed 0 - lr 0.01 - sec_lr 1 - bs 32
[2m[36m(pid=29675)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=29675)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=29675)[0m GPU available: False, used: False
[2m[36m(pid=29675)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29675)[0m 
[2m[36m(pid=29675)[0m   | Name      | Type              | Params
[2m[36m(pid=29675)[0m ------------------------------------------------
[2m[36m(pid=29675)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29675)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29675)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29675)[0m ------------------------------------------------
[2m[36m(pid=29675)[0m 8.7 K     Trainable params
[2m[36m(pid=29675)[0m 0         Non-trainable params
[2m[36m(pid=29675)[0m 8.7 K     Total params
[2m[36m(pid=29675)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=29675)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=29675)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=29675)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=29675)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=29675)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8194)[0m time to fit was 266.05056524276733
[2m[36m(pid=8194)[0m GPU available: False, used: False
[2m[36m(pid=8194)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8194)[0m 
[2m[36m(pid=8194)[0m   | Name      | Type              | Params
[2m[36m(pid=8194)[0m ------------------------------------------------
[2m[36m(pid=8194)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8194)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8194)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8194)[0m ------------------------------------------------
[2m[36m(pid=8194)[0m 8.7 K     Trainable params
[2m[36m(pid=8194)[0m 0         Non-trainable params
[2m[36m(pid=8194)[0m 8.7 K     Total params
[2m[36m(pid=16623)[0m time to fit was 134.2203767299652
[2m[36m(pid=16623)[0m GPU available: False, used: False
[2m[36m(pid=16623)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=16623)[0m 
[2m[36m(pid=16623)[0m   | Name      | Type              | Params
[2m[36m(pid=16623)[0m ------------------------------------------------
[2m[36m(pid=16623)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=16623)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=16623)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=16623)[0m ------------------------------------------------
[2m[36m(pid=16623)[0m 8.7 K     Trainable params
[2m[36m(pid=16623)[0m 0         Non-trainable params
[2m[36m(pid=16623)[0m 8.7 K     Total params
[2m[36m(pid=28288)[0m time to fit was 91.2135603427887
[2m[36m(pid=28288)[0m GPU available: False, used: False
[2m[36m(pid=28288)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=28288)[0m 
[2m[36m(pid=28288)[0m   | Name      | Type              | Params
[2m[36m(pid=28288)[0m ------------------------------------------------
[2m[36m(pid=28288)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=28288)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=28288)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=28288)[0m ------------------------------------------------
[2m[36m(pid=28288)[0m 8.7 K     Trainable params
[2m[36m(pid=28288)[0m 0         Non-trainable params
[2m[36m(pid=28288)[0m 8.7 K     Total params
[2m[36m(pid=22652)[0m time to fit was 215.43975687026978
[2m[36m(pid=22652)[0m GPU available: False, used: False
[2m[36m(pid=22652)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=22652)[0m 
[2m[36m(pid=22652)[0m   | Name      | Type              | Params
[2m[36m(pid=22652)[0m ------------------------------------------------
[2m[36m(pid=22652)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=22652)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=22652)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=22652)[0m ------------------------------------------------
[2m[36m(pid=22652)[0m 8.7 K     Trainable params
[2m[36m(pid=22652)[0m 0         Non-trainable params
[2m[36m(pid=22652)[0m 8.7 K     Total params
[2m[36m(pid=29516)[0m time to fit was 81.13999056816101
[2m[36m(pid=29516)[0m GPU available: False, used: False
[2m[36m(pid=29516)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29516)[0m 
[2m[36m(pid=29516)[0m   | Name      | Type              | Params
[2m[36m(pid=29516)[0m ------------------------------------------------
[2m[36m(pid=29516)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29516)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29516)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29516)[0m ------------------------------------------------
[2m[36m(pid=29516)[0m 8.7 K     Trainable params
[2m[36m(pid=29516)[0m 0         Non-trainable params
[2m[36m(pid=29516)[0m 8.7 K     Total params
[2m[36m(pid=22652)[0m time to fit was 54.234474658966064
[2m[36m(pid=22652)[0m GPU available: False, used: False
[2m[36m(pid=22652)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=22652)[0m 
[2m[36m(pid=22652)[0m   | Name      | Type              | Params
[2m[36m(pid=22652)[0m ------------------------------------------------
[2m[36m(pid=22652)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=22652)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=22652)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=22652)[0m ------------------------------------------------
[2m[36m(pid=22652)[0m 8.7 K     Trainable params
[2m[36m(pid=22652)[0m 0         Non-trainable params
[2m[36m(pid=22652)[0m 8.7 K     Total params
[2m[36m(pid=22000)[0m time to fit was 313.0141317844391
[2m[36m(pid=22000)[0m GPU available: False, used: False
[2m[36m(pid=22000)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=22000)[0m 
[2m[36m(pid=22000)[0m   | Name      | Type              | Params
[2m[36m(pid=22000)[0m ------------------------------------------------
[2m[36m(pid=22000)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=22000)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=22000)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=22000)[0m ------------------------------------------------
[2m[36m(pid=22000)[0m 8.7 K     Trainable params
[2m[36m(pid=22000)[0m 0         Non-trainable params
[2m[36m(pid=22000)[0m 8.7 K     Total params
[2m[36m(pid=16623)[0m time to fit was 124.14022946357727
Result for _inner_e8deb_00087:
  auc: 0.5803552746772767
  date: 2021-03-19_13-02-38
  done: false
  experiment_id: 9b6960ed9f264c5e89f4a1a11db4735d
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 16623
  time_since_restore: 615.9838917255402
  time_this_iter_s: 615.9838917255402
  time_total_s: 615.9838917255402
  timestamp: 1616155358
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00087
  
[2m[36m(pid=16623)[0m Finished run with seed 0 - lr 5 - sec_lr 0.1 - bs 128 - mean val auc: 0.5803552746772767
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 97/180 (1 PENDING, 26 RUNNING, 70 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00062 | RUNNING    |       |          128 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00065 | RUNNING    |       |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00066 | RUNNING    |       |           64 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00070 | RUNNING    |       |           32 |     0 | 0.1   |    0.1   |        |                  |          |
| _inner_e8deb_00096 | PENDING    |       |           64 |     0 | 0.01  |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 77 more trials not shown (16 RUNNING, 60 TERMINATED)


Result for _inner_e8deb_00087:
  auc: 0.5803552746772767
  date: 2021-03-19_13-02-38
  done: true
  experiment_id: 9b6960ed9f264c5e89f4a1a11db4735d
  experiment_tag: 87_batch_size=128,eta=0.0,lr=5,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 16623
  time_since_restore: 615.9838917255402
  time_this_iter_s: 615.9838917255402
  time_total_s: 615.9838917255402
  timestamp: 1616155358
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00087
  
2021-03-19 13:02:39,335	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff90b4bafb01000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=16527)[0m time to fit was 243.64825868606567
[2m[36m(pid=16527)[0m GPU available: False, used: False
[2m[36m(pid=16527)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=16527)[0m 
[2m[36m(pid=16527)[0m   | Name      | Type              | Params
[2m[36m(pid=16527)[0m ------------------------------------------------
[2m[36m(pid=16527)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=16527)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=16527)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=16527)[0m ------------------------------------------------
[2m[36m(pid=16527)[0m 8.7 K     Trainable params
[2m[36m(pid=16527)[0m 0         Non-trainable params
[2m[36m(pid=16527)[0m 8.7 K     Total params
[2m[36m(pid=34531)[0m Starting run with seed 0 - lr 0.01 - sec_lr 1 - bs 64
[2m[36m(pid=34531)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=34531)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=34531)[0m GPU available: False, used: False
[2m[36m(pid=34531)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=34531)[0m 
[2m[36m(pid=34531)[0m   | Name      | Type              | Params
[2m[36m(pid=34531)[0m ------------------------------------------------
[2m[36m(pid=34531)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=34531)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=34531)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=34531)[0m ------------------------------------------------
[2m[36m(pid=34531)[0m 8.7 K     Trainable params
[2m[36m(pid=34531)[0m 0         Non-trainable params
[2m[36m(pid=34531)[0m 8.7 K     Total params
[2m[36m(pid=34531)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=34531)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=34531)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=34531)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=34531)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=34531)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=29516)[0m time to fit was 82.29105806350708
[2m[36m(pid=29516)[0m GPU available: False, used: False
[2m[36m(pid=29516)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29516)[0m 
[2m[36m(pid=29516)[0m   | Name      | Type              | Params
[2m[36m(pid=29516)[0m ------------------------------------------------
[2m[36m(pid=29516)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29516)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29516)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29516)[0m ------------------------------------------------
[2m[36m(pid=29516)[0m 8.7 K     Trainable params
[2m[36m(pid=29516)[0m 0         Non-trainable params
[2m[36m(pid=29516)[0m 8.7 K     Total params
[2m[36m(pid=49910)[0m time to fit was 342.42899918556213
[2m[36m(pid=49910)[0m GPU available: False, used: False
[2m[36m(pid=49910)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49910)[0m 
[2m[36m(pid=49910)[0m   | Name      | Type              | Params
[2m[36m(pid=49910)[0m ------------------------------------------------
[2m[36m(pid=49910)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49910)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49910)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49910)[0m ------------------------------------------------
[2m[36m(pid=49910)[0m 8.7 K     Trainable params
[2m[36m(pid=49910)[0m 0         Non-trainable params
[2m[36m(pid=49910)[0m 8.7 K     Total params
[2m[36m(pid=17269)[0m time to fit was 281.97029423713684
[2m[36m(pid=17269)[0m Finished run with seed 0 - lr 0.01 - sec_lr 0.1 - bs 64 - mean val auc: 0.9109901547431946
Result for _inner_e8deb_00066:
  auc: 0.9109901547431946
  date: 2021-03-19_13-03-10
  done: false
  experiment_id: 6bdcf6dbe29844149f38e5724e8bee34
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 17269
  time_since_restore: 2249.821672439575
  time_this_iter_s: 2249.821672439575
  time_total_s: 2249.821672439575
  timestamp: 1616155390
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00066
  
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 98/180 (1 PENDING, 26 RUNNING, 71 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |                     |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |                     |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00062 | RUNNING    |                     |          128 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00065 | RUNNING    |                     |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00066 | RUNNING    | 145.101.32.82:17269 |           64 |     0 | 0.01  |    0.1   |      1 |          2249.82 | 0.91099  |
| _inner_e8deb_00070 | RUNNING    |                     |           32 |     0 | 0.1   |    0.1   |        |                  |          |
| _inner_e8deb_00097 | PENDING    |                     |          128 |     0 | 0.01  |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 78 more trials not shown (16 RUNNING, 61 TERMINATED)


Result for _inner_e8deb_00066:
  auc: 0.9109901547431946
  date: 2021-03-19_13-03-10
  done: true
  experiment_id: 6bdcf6dbe29844149f38e5724e8bee34
  experiment_tag: 66_batch_size=64,eta=0.0,lr=0.01,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 17269
  time_since_restore: 2249.821672439575
  time_this_iter_s: 2249.821672439575
  time_total_s: 2249.821672439575
  timestamp: 1616155390
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00066
  
[2m[36m(pid=22652)[0m time to fit was 52.064488887786865
[2m[36m(pid=22652)[0m GPU available: False, used: False
[2m[36m(pid=22652)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=22652)[0m 
[2m[36m(pid=22652)[0m   | Name      | Type              | Params
[2m[36m(pid=22652)[0m ------------------------------------------------
[2m[36m(pid=22652)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=22652)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=22652)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=22652)[0m ------------------------------------------------
[2m[36m(pid=22652)[0m 8.7 K     Trainable params
[2m[36m(pid=22652)[0m 0         Non-trainable params
[2m[36m(pid=22652)[0m 8.7 K     Total params
[2m[36m(pid=35542)[0m Starting run with seed 0 - lr 0.01 - sec_lr 1 - bs 128
[2m[36m(pid=35542)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35542)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35542)[0m GPU available: False, used: False
[2m[36m(pid=35542)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35542)[0m 
[2m[36m(pid=35542)[0m   | Name      | Type              | Params
[2m[36m(pid=35542)[0m ------------------------------------------------
[2m[36m(pid=35542)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35542)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35542)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35542)[0m ------------------------------------------------
[2m[36m(pid=35542)[0m 8.7 K     Trainable params
[2m[36m(pid=35542)[0m 0         Non-trainable params
[2m[36m(pid=35542)[0m 8.7 K     Total params
[2m[36m(pid=35542)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35542)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35542)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35542)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35542)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35542)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8194)[0m time to fit was 186.08224511146545
[2m[36m(pid=8194)[0m GPU available: False, used: False
[2m[36m(pid=8194)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8194)[0m 
[2m[36m(pid=8194)[0m   | Name      | Type              | Params
[2m[36m(pid=8194)[0m ------------------------------------------------
[2m[36m(pid=8194)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8194)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8194)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8194)[0m ------------------------------------------------
[2m[36m(pid=8194)[0m 8.7 K     Trainable params
[2m[36m(pid=8194)[0m 0         Non-trainable params
[2m[36m(pid=8194)[0m 8.7 K     Total params
[2m[36m(pid=22652)[0m time to fit was 43.092836141586304
Result for _inner_e8deb_00089:
  auc: 0.6714608907699585
  date: 2021-03-19_13-03-55
  done: false
  experiment_id: b4332145ee39426ea7244a41653d1af2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 22652
  time_since_restore: 468.8734447956085
  time_this_iter_s: 468.8734447956085
  time_total_s: 468.8734447956085
  timestamp: 1616155435
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00089
  
[2m[36m(pid=22652)[0m Finished run with seed 0 - lr 5 - sec_lr 0.1 - bs 512 - mean val auc: 0.6714608907699585
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 99/180 (1 PENDING, 26 RUNNING, 72 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00062 | RUNNING    |       |          128 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00065 | RUNNING    |       |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00070 | RUNNING    |       |           32 |     0 | 0.1   |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    |       |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00098 | PENDING    |       |          256 |     0 | 0.01  |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 79 more trials not shown (16 RUNNING, 62 TERMINATED)


Result for _inner_e8deb_00089:
  auc: 0.6714608907699585
  date: 2021-03-19_13-03-55
  done: true
  experiment_id: b4332145ee39426ea7244a41653d1af2
  experiment_tag: 89_batch_size=512,eta=0.0,lr=5,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 22652
  time_since_restore: 468.8734447956085
  time_this_iter_s: 468.8734447956085
  time_total_s: 468.8734447956085
  timestamp: 1616155435
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00089
  
[2m[36m(pid=37126)[0m Starting run with seed 0 - lr 0.01 - sec_lr 1 - bs 256
[2m[36m(pid=37126)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=37126)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=37126)[0m GPU available: False, used: False
[2m[36m(pid=37126)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=37126)[0m 
[2m[36m(pid=37126)[0m   | Name      | Type              | Params
[2m[36m(pid=37126)[0m ------------------------------------------------
[2m[36m(pid=37126)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=37126)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=37126)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=37126)[0m ------------------------------------------------
[2m[36m(pid=37126)[0m 8.7 K     Trainable params
[2m[36m(pid=37126)[0m 0         Non-trainable params
[2m[36m(pid=37126)[0m 8.7 K     Total params
[2m[36m(pid=37126)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=37126)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23585)[0m time to fit was 431.5719106197357
[2m[36m(pid=37126)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=37126)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23585)[0m GPU available: False, used: False
[2m[36m(pid=23585)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23585)[0m 
[2m[36m(pid=23585)[0m   | Name      | Type              | Params
[2m[36m(pid=23585)[0m ------------------------------------------------
[2m[36m(pid=23585)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23585)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23585)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23585)[0m ------------------------------------------------
[2m[36m(pid=23585)[0m 8.7 K     Trainable params
[2m[36m(pid=23585)[0m 0         Non-trainable params
[2m[36m(pid=23585)[0m 8.7 K     Total params
[2m[36m(pid=37126)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=37126)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=6537)[0m time to fit was 319.1197576522827
[2m[36m(pid=6537)[0m GPU available: False, used: False
[2m[36m(pid=6537)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=6537)[0m 
[2m[36m(pid=6537)[0m   | Name      | Type              | Params
[2m[36m(pid=6537)[0m ------------------------------------------------
[2m[36m(pid=6537)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=6537)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=6537)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=6537)[0m ------------------------------------------------
[2m[36m(pid=6537)[0m 8.7 K     Trainable params
[2m[36m(pid=6537)[0m 0         Non-trainable params
[2m[36m(pid=6537)[0m 8.7 K     Total params
[2m[36m(pid=22000)[0m time to fit was 118.06282877922058
[2m[36m(pid=22000)[0m GPU available: False, used: False
[2m[36m(pid=22000)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=22000)[0m 
[2m[36m(pid=22000)[0m   | Name      | Type              | Params
[2m[36m(pid=22000)[0m ------------------------------------------------
[2m[36m(pid=22000)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=22000)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=22000)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=22000)[0m ------------------------------------------------
[2m[36m(pid=22000)[0m 8.7 K     Trainable params
[2m[36m(pid=22000)[0m 0         Non-trainable params
[2m[36m(pid=22000)[0m 8.7 K     Total params
[2m[36m(pid=29516)[0m time to fit was 101.85084843635559
[2m[36m(pid=29516)[0m GPU available: False, used: False
[2m[36m(pid=29516)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29516)[0m 
[2m[36m(pid=29516)[0m   | Name      | Type              | Params
[2m[36m(pid=29516)[0m ------------------------------------------------
[2m[36m(pid=29516)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29516)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29516)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29516)[0m ------------------------------------------------
[2m[36m(pid=29516)[0m 8.7 K     Trainable params
[2m[36m(pid=29516)[0m 0         Non-trainable params
[2m[36m(pid=29516)[0m 8.7 K     Total params
[2m[36m(pid=28288)[0m time to fit was 231.15870785713196
[2m[36m(pid=28288)[0m GPU available: False, used: False
[2m[36m(pid=28288)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=28288)[0m 
[2m[36m(pid=28288)[0m   | Name      | Type              | Params
[2m[36m(pid=28288)[0m ------------------------------------------------
[2m[36m(pid=28288)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=28288)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=28288)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=28288)[0m ------------------------------------------------
[2m[36m(pid=28288)[0m 8.7 K     Trainable params
[2m[36m(pid=28288)[0m 0         Non-trainable params
[2m[36m(pid=28288)[0m 8.7 K     Total params
[2m[36m(pid=35838)[0m time to fit was 1775.8196811676025
[2m[36m(pid=35838)[0m GPU available: False, used: False
[2m[36m(pid=35838)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35838)[0m 
[2m[36m(pid=35838)[0m   | Name      | Type              | Params
[2m[36m(pid=35838)[0m ------------------------------------------------
[2m[36m(pid=35838)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35838)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35838)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35838)[0m ------------------------------------------------
[2m[36m(pid=35838)[0m 8.7 K     Trainable params
[2m[36m(pid=35838)[0m 0         Non-trainable params
[2m[36m(pid=35838)[0m 8.7 K     Total params
[2m[36m(pid=35848)[0m time to fit was 1678.7021861076355
[2m[36m(pid=35848)[0m GPU available: False, used: False
[2m[36m(pid=35848)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35848)[0m 
[2m[36m(pid=35848)[0m   | Name      | Type              | Params
[2m[36m(pid=35848)[0m ------------------------------------------------
[2m[36m(pid=35848)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35848)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35848)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35848)[0m ------------------------------------------------
[2m[36m(pid=35848)[0m 8.7 K     Trainable params
[2m[36m(pid=35848)[0m 0         Non-trainable params
[2m[36m(pid=35848)[0m 8.7 K     Total params
[2m[36m(pid=22000)[0m time to fit was 78.47182273864746
[2m[36m(pid=22000)[0m GPU available: False, used: False
[2m[36m(pid=22000)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=22000)[0m 
[2m[36m(pid=22000)[0m   | Name      | Type              | Params
[2m[36m(pid=22000)[0m ------------------------------------------------
[2m[36m(pid=22000)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=22000)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=22000)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=22000)[0m ------------------------------------------------
[2m[36m(pid=22000)[0m 8.7 K     Trainable params
[2m[36m(pid=22000)[0m 0         Non-trainable params
[2m[36m(pid=22000)[0m 8.7 K     Total params
[2m[36m(pid=8194)[0m time to fit was 142.1182086467743
Result for _inner_e8deb_00082:
  auc: 0.9104690670967102
  date: 2021-03-19_13-05-45
  done: false
  experiment_id: ee83fdbe55ff41c1ba9527814ed78133
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8194
  time_since_restore: 1066.8797371387482
  time_this_iter_s: 1066.8797371387482
  time_total_s: 1066.8797371387482
  timestamp: 1616155545
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00082
  
[2m[36m(pid=8194)[0m Finished run with seed 0 - lr 2 - sec_lr 0.1 - bs 128 - mean val auc: 0.9104690670967102
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 100/180 (1 PENDING, 26 RUNNING, 73 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00062 | RUNNING    |       |          128 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00065 | RUNNING    |       |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00070 | RUNNING    |       |           32 |     0 | 0.1   |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    |       |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00099 | PENDING    |       |          512 |     0 | 0.01  |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 80 more trials not shown (16 RUNNING, 63 TERMINATED)


Result for _inner_e8deb_00082:
  auc: 0.9104690670967102
  date: 2021-03-19_13-05-45
  done: true
  experiment_id: ee83fdbe55ff41c1ba9527814ed78133
  experiment_tag: 82_batch_size=128,eta=0.0,lr=2,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8194
  time_since_restore: 1066.8797371387482
  time_this_iter_s: 1066.8797371387482
  time_total_s: 1066.8797371387482
  timestamp: 1616155545
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00082
  
[2m[36m(pid=25102)[0m time to fit was 494.24343156814575
[2m[36m(pid=37126)[0m time to fit was 103.4828691482544
[2m[36m(pid=25102)[0m GPU available: False, used: False
[2m[36m(pid=25102)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=25102)[0m 
[2m[36m(pid=25102)[0m   | Name      | Type              | Params
[2m[36m(pid=25102)[0m ------------------------------------------------
[2m[36m(pid=25102)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=25102)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=25102)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=25102)[0m ------------------------------------------------
[2m[36m(pid=25102)[0m 8.7 K     Trainable params
[2m[36m(pid=25102)[0m 0         Non-trainable params
[2m[36m(pid=25102)[0m 8.7 K     Total params
[2m[36m(pid=37126)[0m GPU available: False, used: False
[2m[36m(pid=37126)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=37126)[0m 
[2m[36m(pid=37126)[0m   | Name      | Type              | Params
[2m[36m(pid=37126)[0m ------------------------------------------------
[2m[36m(pid=37126)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=37126)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=37126)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=37126)[0m ------------------------------------------------
[2m[36m(pid=37126)[0m 8.7 K     Trainable params
[2m[36m(pid=37126)[0m 0         Non-trainable params
[2m[36m(pid=37126)[0m 8.7 K     Total params
[2m[36m(pid=48984)[0m time to fit was 423.52638125419617
[2m[36m(pid=48984)[0m GPU available: False, used: False
[2m[36m(pid=48984)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48984)[0m 
[2m[36m(pid=48984)[0m   | Name      | Type              | Params
[2m[36m(pid=48984)[0m ------------------------------------------------
[2m[36m(pid=48984)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48984)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48984)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48984)[0m ------------------------------------------------
[2m[36m(pid=48984)[0m 8.7 K     Trainable params
[2m[36m(pid=48984)[0m 0         Non-trainable params
[2m[36m(pid=48984)[0m 8.7 K     Total params
[2m[36m(pid=40855)[0m Starting run with seed 0 - lr 0.01 - sec_lr 1 - bs 512
[2m[36m(pid=40855)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40855)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=40855)[0m GPU available: False, used: False
[2m[36m(pid=40855)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40855)[0m 
[2m[36m(pid=40855)[0m   | Name      | Type              | Params
[2m[36m(pid=40855)[0m ------------------------------------------------
[2m[36m(pid=40855)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40855)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40855)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40855)[0m ------------------------------------------------
[2m[36m(pid=40855)[0m 8.7 K     Trainable params
[2m[36m(pid=40855)[0m 0         Non-trainable params
[2m[36m(pid=40855)[0m 8.7 K     Total params
[2m[36m(pid=40855)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=40855)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=40855)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=40855)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=40855)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=40855)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=10108)[0m time to fit was 530.4697976112366
Result for _inner_e8deb_00062:
  auc: 0.9070754766464233
  date: 2021-03-19_13-06-02
  done: false
  experiment_id: 950421e6216141b1af7c23e5fc6df067
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 10108
  time_since_restore: 2655.62193775177
  time_this_iter_s: 2655.62193775177
  time_total_s: 2655.62193775177
  timestamp: 1616155562
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00062
  
[2m[36m(pid=10108)[0m Finished run with seed 0 - lr 0.001 - sec_lr 0.1 - bs 128 - mean val auc: 0.9070754766464233
== Status ==
Memory usage on this node: 9.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 101/180 (1 PENDING, 26 RUNNING, 74 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |                     |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |                     |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00062 | RUNNING    | 145.101.32.82:10108 |          128 |     0 | 0.001 |    0.1   |      1 |          2655.62 | 0.907075 |
| _inner_e8deb_00065 | RUNNING    |                     |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00070 | RUNNING    |                     |           32 |     0 | 0.1   |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    |                     |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00100 | PENDING    |                     |           32 |     0 | 0.1   |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 81 more trials not shown (16 RUNNING, 64 TERMINATED)


Result for _inner_e8deb_00062:
  auc: 0.9070754766464233
  date: 2021-03-19_13-06-02
  done: true
  experiment_id: 950421e6216141b1af7c23e5fc6df067
  experiment_tag: 62_batch_size=128,eta=0.0,lr=0.001,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 10108
  time_since_restore: 2655.62193775177
  time_this_iter_s: 2655.62193775177
  time_total_s: 2655.62193775177
  timestamp: 1616155562
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00062
  
[2m[36m(pid=35542)[0m time to fit was 172.492356300354
[2m[36m(pid=35542)[0m GPU available: False, used: False
[2m[36m(pid=35542)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35542)[0m 
[2m[36m(pid=35542)[0m   | Name      | Type              | Params
[2m[36m(pid=35542)[0m ------------------------------------------------
[2m[36m(pid=35542)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35542)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35542)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35542)[0m ------------------------------------------------
[2m[36m(pid=35542)[0m 8.7 K     Trainable params
[2m[36m(pid=35542)[0m 0         Non-trainable params
[2m[36m(pid=35542)[0m 8.7 K     Total params
[2m[36m(pid=41401)[0m Starting run with seed 0 - lr 0.1 - sec_lr 1 - bs 32
[2m[36m(pid=41401)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41401)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=41401)[0m GPU available: False, used: False
[2m[36m(pid=41401)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=41401)[0m 
[2m[36m(pid=41401)[0m   | Name      | Type              | Params
[2m[36m(pid=41401)[0m ------------------------------------------------
[2m[36m(pid=41401)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=41401)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=41401)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=41401)[0m ------------------------------------------------
[2m[36m(pid=41401)[0m 8.7 K     Trainable params
[2m[36m(pid=41401)[0m 0         Non-trainable params
[2m[36m(pid=41401)[0m 8.7 K     Total params
[2m[36m(pid=41401)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=41401)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=41401)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=41401)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=41401)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=41401)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=28288)[0m time to fit was 97.40158343315125
[2m[36m(pid=28288)[0m GPU available: False, used: False
[2m[36m(pid=28288)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=28288)[0m 
[2m[36m(pid=28288)[0m   | Name      | Type              | Params
[2m[36m(pid=28288)[0m ------------------------------------------------
[2m[36m(pid=28288)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=28288)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=28288)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=28288)[0m ------------------------------------------------
[2m[36m(pid=28288)[0m 8.7 K     Trainable params
[2m[36m(pid=28288)[0m 0         Non-trainable params
[2m[36m(pid=28288)[0m 8.7 K     Total params
[2m[36m(pid=49910)[0m time to fit was 247.09989857673645
Result for _inner_e8deb_00076:
  auc: 0.9115935802459717
  date: 2021-03-19_13-07-12
  done: false
  experiment_id: b381e6c066b54c6295d26c63997ea5c6
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 49910
  time_since_restore: 1495.0775854587555
  time_this_iter_s: 1495.0775854587555
  time_total_s: 1495.0775854587555
  timestamp: 1616155632
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00076
  
[2m[36m(pid=49910)[0m Finished run with seed 0 - lr 1 - sec_lr 0.1 - bs 64 - mean val auc: 0.9115935802459717
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 102/180 (1 PENDING, 26 RUNNING, 75 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |                     |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |                     |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00065 | RUNNING    |                     |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00070 | RUNNING    |                     |           32 |     0 | 0.1   |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    |                     |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00076 | RUNNING    | 145.101.32.82:49910 |           64 |     0 | 1     |    0.1   |      1 |          1495.08 | 0.911594 |
| _inner_e8deb_00101 | PENDING    |                     |           64 |     0 | 0.1   |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 82 more trials not shown (16 RUNNING, 65 TERMINATED)


Result for _inner_e8deb_00076:
  auc: 0.9115935802459717
  date: 2021-03-19_13-07-12
  done: true
  experiment_id: b381e6c066b54c6295d26c63997ea5c6
  experiment_tag: 76_batch_size=64,eta=0.0,lr=1,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 49910
  time_since_restore: 1495.0775854587555
  time_this_iter_s: 1495.0775854587555
  time_total_s: 1495.0775854587555
  timestamp: 1616155632
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00076
  
[2m[36m(pid=29675)[0m time to fit was 423.9347183704376
[2m[36m(pid=29675)[0m GPU available: False, used: False
[2m[36m(pid=29675)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29675)[0m 
[2m[36m(pid=29675)[0m   | Name      | Type              | Params
[2m[36m(pid=29675)[0m ------------------------------------------------
[2m[36m(pid=29675)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29675)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29675)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29675)[0m ------------------------------------------------
[2m[36m(pid=29675)[0m 8.7 K     Trainable params
[2m[36m(pid=29675)[0m 0         Non-trainable params
[2m[36m(pid=29675)[0m 8.7 K     Total params
[2m[36m(pid=43563)[0m Starting run with seed 0 - lr 0.1 - sec_lr 1 - bs 64
[2m[36m(pid=43563)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=43563)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=43563)[0m GPU available: False, used: False
[2m[36m(pid=43563)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=43563)[0m 
[2m[36m(pid=43563)[0m   | Name      | Type              | Params
[2m[36m(pid=43563)[0m ------------------------------------------------
[2m[36m(pid=43563)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=43563)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=43563)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=43563)[0m ------------------------------------------------
[2m[36m(pid=43563)[0m 8.7 K     Trainable params
[2m[36m(pid=43563)[0m 0         Non-trainable params
[2m[36m(pid=43563)[0m 8.7 K     Total params
[2m[36m(pid=43563)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=43563)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=43563)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=43563)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=43563)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=43563)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=29516)[0m time to fit was 187.3566539287567
[2m[36m(pid=29516)[0m GPU available: False, used: False
[2m[36m(pid=29516)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29516)[0m 
[2m[36m(pid=29516)[0m   | Name      | Type              | Params
[2m[36m(pid=29516)[0m ------------------------------------------------
[2m[36m(pid=29516)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29516)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29516)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29516)[0m ------------------------------------------------
[2m[36m(pid=29516)[0m 8.7 K     Trainable params
[2m[36m(pid=29516)[0m 0         Non-trainable params
[2m[36m(pid=29516)[0m 8.7 K     Total params
[2m[36m(pid=34531)[0m time to fit was 299.4375059604645
[2m[36m(pid=34531)[0m GPU available: False, used: False
[2m[36m(pid=34531)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=34531)[0m 
[2m[36m(pid=34531)[0m   | Name      | Type              | Params
[2m[36m(pid=34531)[0m ------------------------------------------------
[2m[36m(pid=34531)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=34531)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=34531)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=34531)[0m ------------------------------------------------
[2m[36m(pid=34531)[0m 8.7 K     Trainable params
[2m[36m(pid=34531)[0m 0         Non-trainable params
[2m[36m(pid=34531)[0m 8.7 K     Total params
[2m[36m(pid=27849)[0m time to fit was 519.3496532440186
[2m[36m(pid=27849)[0m GPU available: False, used: False
[2m[36m(pid=27849)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27849)[0m 
[2m[36m(pid=27849)[0m   | Name      | Type              | Params
[2m[36m(pid=27849)[0m ------------------------------------------------
[2m[36m(pid=27849)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27849)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27849)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27849)[0m ------------------------------------------------
[2m[36m(pid=27849)[0m 8.7 K     Trainable params
[2m[36m(pid=27849)[0m 0         Non-trainable params
[2m[36m(pid=27849)[0m 8.7 K     Total params
[2m[36m(pid=40855)[0m time to fit was 116.57139611244202
[2m[36m(pid=40855)[0m GPU available: False, used: False
[2m[36m(pid=40855)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40855)[0m 
[2m[36m(pid=40855)[0m   | Name      | Type              | Params
[2m[36m(pid=40855)[0m ------------------------------------------------
[2m[36m(pid=40855)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40855)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40855)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40855)[0m ------------------------------------------------
[2m[36m(pid=40855)[0m 8.7 K     Trainable params
[2m[36m(pid=40855)[0m 0         Non-trainable params
[2m[36m(pid=40855)[0m 8.7 K     Total params
[2m[36m(pid=8106)[0m time to fit was 937.3653061389923
[2m[36m(pid=8106)[0m GPU available: False, used: False
[2m[36m(pid=8106)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8106)[0m 
[2m[36m(pid=8106)[0m   | Name      | Type              | Params
[2m[36m(pid=8106)[0m ------------------------------------------------
[2m[36m(pid=8106)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8106)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8106)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8106)[0m ------------------------------------------------
[2m[36m(pid=8106)[0m 8.7 K     Trainable params
[2m[36m(pid=8106)[0m 0         Non-trainable params
[2m[36m(pid=8106)[0m 8.7 K     Total params
[2m[36m(pid=22000)[0m time to fit was 166.28313040733337
Result for _inner_e8deb_00088:
  auc: 0.6540756106376648
  date: 2021-03-19_13-08-27
  done: false
  experiment_id: e891a2e9c55847aaa309530b100341c1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 22000
  time_since_restore: 761.3245449066162
  time_this_iter_s: 761.3245449066162
  time_total_s: 761.3245449066162
  timestamp: 1616155707
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00088
  
[2m[36m(pid=22000)[0m Finished run with seed 0 - lr 5 - sec_lr 0.1 - bs 256 - mean val auc: 0.6540756106376648
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 103/180 (1 PENDING, 26 RUNNING, 76 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00065 | RUNNING    |       |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00070 | RUNNING    |       |           32 |     0 | 0.1   |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    |       |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |       |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00102 | PENDING    |       |          128 |     0 | 0.1   |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 83 more trials not shown (16 RUNNING, 66 TERMINATED)


Result for _inner_e8deb_00088:
  auc: 0.6540756106376648
  date: 2021-03-19_13-08-27
  done: true
  experiment_id: e891a2e9c55847aaa309530b100341c1
  experiment_tag: 88_batch_size=256,eta=0.0,lr=5,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 22000
  time_since_restore: 761.3245449066162
  time_this_iter_s: 761.3245449066162
  time_total_s: 761.3245449066162
  timestamp: 1616155707
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00088
  
[2m[36m(pid=6537)[0m time to fit was 262.76602149009705
[2m[36m(pid=6537)[0m GPU available: False, used: False
[2m[36m(pid=6537)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=6537)[0m 
[2m[36m(pid=6537)[0m   | Name      | Type              | Params
[2m[36m(pid=6537)[0m ------------------------------------------------
[2m[36m(pid=6537)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=6537)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=6537)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=6537)[0m ------------------------------------------------
[2m[36m(pid=6537)[0m 8.7 K     Trainable params
[2m[36m(pid=6537)[0m 0         Non-trainable params
[2m[36m(pid=6537)[0m 8.7 K     Total params
[2m[36m(pid=46019)[0m Starting run with seed 0 - lr 0.1 - sec_lr 1 - bs 128
[2m[36m(pid=46019)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46019)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=46019)[0m GPU available: False, used: False
[2m[36m(pid=46019)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=46019)[0m 
[2m[36m(pid=46019)[0m   | Name      | Type              | Params
[2m[36m(pid=46019)[0m ------------------------------------------------
[2m[36m(pid=46019)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=46019)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=46019)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=46019)[0m ------------------------------------------------
[2m[36m(pid=46019)[0m 8.7 K     Trainable params
[2m[36m(pid=46019)[0m 0         Non-trainable params
[2m[36m(pid=46019)[0m 8.7 K     Total params
[2m[36m(pid=46019)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=46019)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=46019)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=46019)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=46019)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=46019)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=5013)[0m time to fit was 816.51353764534
[2m[36m(pid=5013)[0m GPU available: False, used: False
[2m[36m(pid=5013)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=5013)[0m 
[2m[36m(pid=5013)[0m   | Name      | Type              | Params
[2m[36m(pid=5013)[0m ------------------------------------------------
[2m[36m(pid=5013)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=5013)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=5013)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=5013)[0m ------------------------------------------------
[2m[36m(pid=5013)[0m 8.7 K     Trainable params
[2m[36m(pid=5013)[0m 0         Non-trainable params
[2m[36m(pid=5013)[0m 8.7 K     Total params
[2m[36m(pid=37126)[0m time to fit was 264.6220893859863
[2m[36m(pid=37126)[0m GPU available: False, used: False
[2m[36m(pid=37126)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=37126)[0m 
[2m[36m(pid=37126)[0m   | Name      | Type              | Params
[2m[36m(pid=37126)[0m ------------------------------------------------
[2m[36m(pid=37126)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=37126)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=37126)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=37126)[0m ------------------------------------------------
[2m[36m(pid=37126)[0m 8.7 K     Trainable params
[2m[36m(pid=37126)[0m 0         Non-trainable params
[2m[36m(pid=37126)[0m 8.7 K     Total params
[2m[36m(pid=46019)[0m time to fit was 112.40175795555115
[2m[36m(pid=46019)[0m GPU available: False, used: False
[2m[36m(pid=46019)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=46019)[0m 
[2m[36m(pid=46019)[0m   | Name      | Type              | Params
[2m[36m(pid=46019)[0m ------------------------------------------------
[2m[36m(pid=46019)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=46019)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=46019)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=46019)[0m ------------------------------------------------
[2m[36m(pid=46019)[0m 8.7 K     Trainable params
[2m[36m(pid=46019)[0m 0         Non-trainable params
[2m[36m(pid=46019)[0m 8.7 K     Total params
[2m[36m(pid=23585)[0m time to fit was 389.627014875412
Result for _inner_e8deb_00070:
  auc: 0.9116244912147522
  date: 2021-03-19_13-10-37
  done: false
  experiment_id: 48eaf59a40cc4d088ed7f8f3dc526da7
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 23585
  time_since_restore: 2492.013884305954
  time_this_iter_s: 2492.013884305954
  time_total_s: 2492.013884305954
  timestamp: 1616155837
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00070
  
[2m[36m(pid=23585)[0m Finished run with seed 0 - lr 0.1 - sec_lr 0.1 - bs 32 - mean val auc: 0.9116244912147522
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 104/180 (1 PENDING, 26 RUNNING, 77 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |                     |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |                     |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00065 | RUNNING    |                     |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00070 | RUNNING    | 145.101.32.82:23585 |           32 |     0 | 0.1   |    0.1   |      1 |          2492.01 | 0.911624 |
| _inner_e8deb_00075 | RUNNING    |                     |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |                     |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00103 | PENDING    |                     |          256 |     0 | 0.1   |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 84 more trials not shown (16 RUNNING, 67 TERMINATED)


Result for _inner_e8deb_00070:
  auc: 0.9116244912147522
  date: 2021-03-19_13-10-37
  done: true
  experiment_id: 48eaf59a40cc4d088ed7f8f3dc526da7
  experiment_tag: 70_batch_size=32,eta=0.0,lr=0.1,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 23585
  time_since_restore: 2492.013884305954
  time_this_iter_s: 2492.013884305954
  time_total_s: 2492.013884305954
  timestamp: 1616155837
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00070
  
[2m[36m(pid=43563)[0m time to fit was 202.1484546661377
[2m[36m(pid=43563)[0m GPU available: False, used: False
[2m[36m(pid=43563)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=43563)[0m 
[2m[36m(pid=43563)[0m   | Name      | Type              | Params
[2m[36m(pid=43563)[0m ------------------------------------------------
[2m[36m(pid=43563)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=43563)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=43563)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=43563)[0m ------------------------------------------------
[2m[36m(pid=43563)[0m 8.7 K     Trainable params
[2m[36m(pid=43563)[0m 0         Non-trainable params
[2m[36m(pid=43563)[0m 8.7 K     Total params
[2m[36m(pid=49945)[0m Starting run with seed 0 - lr 0.1 - sec_lr 1 - bs 256
[2m[36m(pid=49945)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=49945)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=49945)[0m GPU available: False, used: False
[2m[36m(pid=49945)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49945)[0m 
[2m[36m(pid=49945)[0m   | Name      | Type              | Params
[2m[36m(pid=49945)[0m ------------------------------------------------
[2m[36m(pid=49945)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49945)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49945)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49945)[0m ------------------------------------------------
[2m[36m(pid=49945)[0m 8.7 K     Trainable params
[2m[36m(pid=49945)[0m 0         Non-trainable params
[2m[36m(pid=49945)[0m 8.7 K     Total params
[2m[36m(pid=49945)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=49945)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49945)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=49945)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49945)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=49945)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=13798)[0m time to fit was 742.1860008239746
[2m[36m(pid=13798)[0m GPU available: False, used: False
[2m[36m(pid=13798)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=13798)[0m 
[2m[36m(pid=13798)[0m   | Name      | Type              | Params
[2m[36m(pid=13798)[0m ------------------------------------------------
[2m[36m(pid=13798)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=13798)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=13798)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=13798)[0m ------------------------------------------------
[2m[36m(pid=13798)[0m 8.7 K     Trainable params
[2m[36m(pid=13798)[0m 0         Non-trainable params
[2m[36m(pid=13798)[0m 8.7 K     Total params
[2m[36m(pid=29516)[0m time to fit was 198.41193652153015
Result for _inner_e8deb_00094:
  auc: 0.8547196745872497
  date: 2021-03-19_13-11-01
  done: false
  experiment_id: 483680a2f21248b0b77bce5e8290dde9
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 29516
  time_since_restore: 652.2409763336182
  time_this_iter_s: 652.2409763336182
  time_total_s: 652.2409763336182
  timestamp: 1616155861
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00094
  
[2m[36m(pid=29516)[0m Finished run with seed 0 - lr 0.001 - sec_lr 1 - bs 512 - mean val auc: 0.8547196745872497
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 105/180 (1 PENDING, 26 RUNNING, 78 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00065 | RUNNING    |       |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    |       |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |       |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00081 | RUNNING    |       |           64 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00104 | PENDING    |       |          512 |     0 | 0.1   |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 85 more trials not shown (16 RUNNING, 68 TERMINATED)


Result for _inner_e8deb_00094:
  auc: 0.8547196745872497
  date: 2021-03-19_13-11-01
  done: true
  experiment_id: 483680a2f21248b0b77bce5e8290dde9
  experiment_tag: 94_batch_size=512,eta=0.0,lr=0.001,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 29516
  time_since_restore: 652.2409763336182
  time_this_iter_s: 652.2409763336182
  time_total_s: 652.2409763336182
  timestamp: 1616155861
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00094
  
[2m[36m(pid=28288)[0m time to fit was 274.8933460712433
[2m[36m(pid=28288)[0m GPU available: False, used: False
[2m[36m(pid=28288)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=28288)[0m 
[2m[36m(pid=28288)[0m   | Name      | Type              | Params
[2m[36m(pid=28288)[0m ------------------------------------------------
[2m[36m(pid=28288)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=28288)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=28288)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=28288)[0m ------------------------------------------------
[2m[36m(pid=28288)[0m 8.7 K     Trainable params
[2m[36m(pid=28288)[0m 0         Non-trainable params
[2m[36m(pid=28288)[0m 8.7 K     Total params
[2m[36m(pid=50729)[0m Starting run with seed 0 - lr 0.1 - sec_lr 1 - bs 512
[2m[36m(pid=50729)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=50729)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=50729)[0m GPU available: False, used: False
[2m[36m(pid=50729)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=50729)[0m 
[2m[36m(pid=50729)[0m   | Name      | Type              | Params
[2m[36m(pid=50729)[0m ------------------------------------------------
[2m[36m(pid=50729)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=50729)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=50729)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=50729)[0m ------------------------------------------------
[2m[36m(pid=50729)[0m 8.7 K     Trainable params
[2m[36m(pid=50729)[0m 0         Non-trainable params
[2m[36m(pid=50729)[0m 8.7 K     Total params
[2m[36m(pid=50729)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=50729)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=50729)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=50729)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=50729)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=50729)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48139)[0m time to fit was 1012.5043127536774
[2m[36m(pid=48139)[0m GPU available: False, used: False
[2m[36m(pid=48139)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48139)[0m 
[2m[36m(pid=48139)[0m   | Name      | Type              | Params
[2m[36m(pid=48139)[0m ------------------------------------------------
[2m[36m(pid=48139)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48139)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48139)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48139)[0m ------------------------------------------------
[2m[36m(pid=48139)[0m 8.7 K     Trainable params
[2m[36m(pid=48139)[0m 0         Non-trainable params
[2m[36m(pid=48139)[0m 8.7 K     Total params
[2m[36m(pid=40855)[0m time to fit was 218.4125952720642
[2m[36m(pid=40855)[0m GPU available: False, used: False
[2m[36m(pid=40855)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40855)[0m 
[2m[36m(pid=40855)[0m   | Name      | Type              | Params
[2m[36m(pid=40855)[0m ------------------------------------------------
[2m[36m(pid=40855)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40855)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40855)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40855)[0m ------------------------------------------------
[2m[36m(pid=40855)[0m 8.7 K     Trainable params
[2m[36m(pid=40855)[0m 0         Non-trainable params
[2m[36m(pid=40855)[0m 8.7 K     Total params
[2m[36m(pid=49945)[0m time to fit was 79.58995246887207
[2m[36m(pid=49945)[0m GPU available: False, used: False
[2m[36m(pid=49945)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49945)[0m 
[2m[36m(pid=49945)[0m   | Name      | Type              | Params
[2m[36m(pid=49945)[0m ------------------------------------------------
[2m[36m(pid=49945)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49945)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49945)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49945)[0m ------------------------------------------------
[2m[36m(pid=49945)[0m 8.7 K     Trainable params
[2m[36m(pid=49945)[0m 0         Non-trainable params
[2m[36m(pid=49945)[0m 8.7 K     Total params
[2m[36m(pid=50729)[0m time to fit was 59.80366325378418
[2m[36m(pid=50729)[0m GPU available: False, used: False
[2m[36m(pid=50729)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=50729)[0m 
[2m[36m(pid=50729)[0m   | Name      | Type              | Params
[2m[36m(pid=50729)[0m ------------------------------------------------
[2m[36m(pid=50729)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=50729)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=50729)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=50729)[0m ------------------------------------------------
[2m[36m(pid=50729)[0m 8.7 K     Trainable params
[2m[36m(pid=50729)[0m 0         Non-trainable params
[2m[36m(pid=50729)[0m 8.7 K     Total params
[2m[36m(pid=35542)[0m time to fit was 386.809534072876
[2m[36m(pid=35542)[0m GPU available: False, used: False
[2m[36m(pid=35542)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35542)[0m 
[2m[36m(pid=35542)[0m   | Name      | Type              | Params
[2m[36m(pid=35542)[0m ------------------------------------------------
[2m[36m(pid=35542)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35542)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35542)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35542)[0m ------------------------------------------------
[2m[36m(pid=35542)[0m 8.7 K     Trainable params
[2m[36m(pid=35542)[0m 0         Non-trainable params
[2m[36m(pid=35542)[0m 8.7 K     Total params
[2m[36m(pid=46019)[0m time to fit was 142.93677401542664
[2m[36m(pid=46019)[0m GPU available: False, used: False
[2m[36m(pid=46019)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=46019)[0m 
[2m[36m(pid=46019)[0m   | Name      | Type              | Params
[2m[36m(pid=46019)[0m ------------------------------------------------
[2m[36m(pid=46019)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=46019)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=46019)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=46019)[0m ------------------------------------------------
[2m[36m(pid=46019)[0m 8.7 K     Trainable params
[2m[36m(pid=46019)[0m 0         Non-trainable params
[2m[36m(pid=46019)[0m 8.7 K     Total params
[2m[36m(pid=50729)[0m time to fit was 58.0539755821228
[2m[36m(pid=50729)[0m GPU available: False, used: False
[2m[36m(pid=50729)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=50729)[0m 
[2m[36m(pid=50729)[0m   | Name      | Type              | Params
[2m[36m(pid=50729)[0m ------------------------------------------------
[2m[36m(pid=50729)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=50729)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=50729)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=50729)[0m ------------------------------------------------
[2m[36m(pid=50729)[0m 8.7 K     Trainable params
[2m[36m(pid=50729)[0m 0         Non-trainable params
[2m[36m(pid=50729)[0m 8.7 K     Total params
[2m[36m(pid=49945)[0m time to fit was 91.71327567100525
[2m[36m(pid=49945)[0m GPU available: False, used: False
[2m[36m(pid=49945)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49945)[0m 
[2m[36m(pid=49945)[0m   | Name      | Type              | Params
[2m[36m(pid=49945)[0m ------------------------------------------------
[2m[36m(pid=49945)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49945)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49945)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49945)[0m ------------------------------------------------
[2m[36m(pid=49945)[0m 8.7 K     Trainable params
[2m[36m(pid=49945)[0m 0         Non-trainable params
[2m[36m(pid=49945)[0m 8.7 K     Total params
[2m[36m(pid=26407)[0m time to fit was 930.7962045669556
[2m[36m(pid=26407)[0m GPU available: False, used: False
[2m[36m(pid=26407)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26407)[0m 
[2m[36m(pid=26407)[0m   | Name      | Type              | Params
[2m[36m(pid=26407)[0m ------------------------------------------------
[2m[36m(pid=26407)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26407)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26407)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26407)[0m ------------------------------------------------
[2m[36m(pid=26407)[0m 8.7 K     Trainable params
[2m[36m(pid=26407)[0m 0         Non-trainable params
[2m[36m(pid=26407)[0m 8.7 K     Total params
[2m[36m(pid=41401)[0m time to fit was 460.82230019569397
[2m[36m(pid=41401)[0m GPU available: False, used: False
[2m[36m(pid=41401)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=41401)[0m 
[2m[36m(pid=41401)[0m   | Name      | Type              | Params
[2m[36m(pid=41401)[0m ------------------------------------------------
[2m[36m(pid=41401)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=41401)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=41401)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=41401)[0m ------------------------------------------------
[2m[36m(pid=41401)[0m 8.7 K     Trainable params
[2m[36m(pid=41401)[0m 0         Non-trainable params
[2m[36m(pid=41401)[0m 8.7 K     Total params
[2m[36m(pid=6537)[0m time to fit was 321.00929069519043
[2m[36m(pid=6537)[0m Finished run with seed 0 - lr 2 - sec_lr 0.1 - bs 64 - mean val auc: 0.9092507123947143
Result for _inner_e8deb_00081:
  auc: 0.9092507123947143
  date: 2021-03-19_13-13-55
  done: false
  experiment_id: bb4ccf00eedc491f87fb73ffdb32d6c9
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 6537
  time_since_restore: 1611.3965482711792
  time_this_iter_s: 1611.3965482711792
  time_total_s: 1611.3965482711792
  timestamp: 1616156035
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00081
  
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 106/180 (1 PENDING, 26 RUNNING, 79 TERMINATED)
+--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                    |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                    |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                    |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |                    |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                    |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |                    |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00065 | RUNNING    |                    |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    |                    |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |                    |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00081 | RUNNING    | 145.101.32.82:6537 |           64 |     0 | 2     |    0.1   |      1 |          1611.4  | 0.909251 |
| _inner_e8deb_00105 | PENDING    |                    |           32 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                    |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                    |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                    |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                    |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                    |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                    |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                    |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                    |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                    |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                    |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 86 more trials not shown (16 RUNNING, 69 TERMINATED)


Result for _inner_e8deb_00081:
  auc: 0.9092507123947143
  date: 2021-03-19_13-13-55
  done: true
  experiment_id: bb4ccf00eedc491f87fb73ffdb32d6c9
  experiment_tag: 81_batch_size=64,eta=0.0,lr=2,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 6537
  time_since_restore: 1611.3965482711792
  time_this_iter_s: 1611.3965482711792
  time_total_s: 1611.3965482711792
  timestamp: 1616156035
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00081
  
[2m[36m(pid=37126)[0m time to fit was 225.69123482704163
[2m[36m(pid=37126)[0m GPU available: False, used: False
[2m[36m(pid=37126)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=37126)[0m 
[2m[36m(pid=37126)[0m   | Name      | Type              | Params
[2m[36m(pid=37126)[0m ------------------------------------------------
[2m[36m(pid=37126)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=37126)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=37126)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=37126)[0m ------------------------------------------------
[2m[36m(pid=37126)[0m 8.7 K     Trainable params
[2m[36m(pid=37126)[0m 0         Non-trainable params
[2m[36m(pid=37126)[0m 8.7 K     Total params
[2m[36m(pid=34531)[0m time to fit was 374.73732566833496
[2m[36m(pid=34531)[0m GPU available: False, used: False
[2m[36m(pid=34531)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=34531)[0m 
[2m[36m(pid=34531)[0m   | Name      | Type              | Params
[2m[36m(pid=34531)[0m ------------------------------------------------
[2m[36m(pid=34531)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=34531)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=34531)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=34531)[0m ------------------------------------------------
[2m[36m(pid=34531)[0m 8.7 K     Trainable params
[2m[36m(pid=34531)[0m 0         Non-trainable params
[2m[36m(pid=34531)[0m 8.7 K     Total params
[2m[36m(pid=3723)[0m Starting run with seed 0 - lr 1 - sec_lr 1 - bs 32
[2m[36m(pid=3723)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3723)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=3723)[0m GPU available: False, used: False
[2m[36m(pid=3723)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=3723)[0m 
[2m[36m(pid=3723)[0m   | Name      | Type              | Params
[2m[36m(pid=3723)[0m ------------------------------------------------
[2m[36m(pid=3723)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=3723)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=3723)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=3723)[0m ------------------------------------------------
[2m[36m(pid=3723)[0m 8.7 K     Trainable params
[2m[36m(pid=3723)[0m 0         Non-trainable params
[2m[36m(pid=3723)[0m 8.7 K     Total params
[2m[36m(pid=3723)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=3723)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=3723)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=3723)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=50729)[0m time to fit was 58.27784872055054
[2m[36m(pid=3723)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=3723)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=50729)[0m GPU available: False, used: False
[2m[36m(pid=50729)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=50729)[0m 
[2m[36m(pid=50729)[0m   | Name      | Type              | Params
[2m[36m(pid=50729)[0m ------------------------------------------------
[2m[36m(pid=50729)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=50729)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=50729)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=50729)[0m ------------------------------------------------
[2m[36m(pid=50729)[0m 8.7 K     Trainable params
[2m[36m(pid=50729)[0m 0         Non-trainable params
[2m[36m(pid=50729)[0m 8.7 K     Total params
[2m[36m(pid=43563)[0m time to fit was 225.20806407928467
[2m[36m(pid=43563)[0m GPU available: False, used: False
[2m[36m(pid=43563)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=43563)[0m 
[2m[36m(pid=43563)[0m   | Name      | Type              | Params
[2m[36m(pid=43563)[0m ------------------------------------------------
[2m[36m(pid=43563)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=43563)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=43563)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=43563)[0m ------------------------------------------------
[2m[36m(pid=43563)[0m 8.7 K     Trainable params
[2m[36m(pid=43563)[0m 0         Non-trainable params
[2m[36m(pid=43563)[0m 8.7 K     Total params
[2m[36m(pid=46019)[0m time to fit was 123.94294476509094
[2m[36m(pid=46019)[0m GPU available: False, used: False
[2m[36m(pid=46019)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=46019)[0m 
[2m[36m(pid=46019)[0m   | Name      | Type              | Params
[2m[36m(pid=46019)[0m ------------------------------------------------
[2m[36m(pid=46019)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=46019)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=46019)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=46019)[0m ------------------------------------------------
[2m[36m(pid=46019)[0m 8.7 K     Trainable params
[2m[36m(pid=46019)[0m 0         Non-trainable params
[2m[36m(pid=46019)[0m 8.7 K     Total params
[2m[36m(pid=40855)[0m time to fit was 220.5106167793274
[2m[36m(pid=40855)[0m GPU available: False, used: False
[2m[36m(pid=40855)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40855)[0m 
[2m[36m(pid=40855)[0m   | Name      | Type              | Params
[2m[36m(pid=40855)[0m ------------------------------------------------
[2m[36m(pid=40855)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40855)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40855)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40855)[0m ------------------------------------------------
[2m[36m(pid=40855)[0m 8.7 K     Trainable params
[2m[36m(pid=40855)[0m 0         Non-trainable params
[2m[36m(pid=40855)[0m 8.7 K     Total params
[2m[36m(pid=50729)[0m time to fit was 71.63670659065247
[2m[36m(pid=50729)[0m GPU available: False, used: False
[2m[36m(pid=50729)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=50729)[0m 
[2m[36m(pid=50729)[0m   | Name      | Type              | Params
[2m[36m(pid=50729)[0m ------------------------------------------------
[2m[36m(pid=50729)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=50729)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=50729)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=50729)[0m ------------------------------------------------
[2m[36m(pid=50729)[0m 8.7 K     Trainable params
[2m[36m(pid=50729)[0m 0         Non-trainable params
[2m[36m(pid=50729)[0m 8.7 K     Total params
[2m[36m(pid=49945)[0m time to fit was 111.4503288269043
[2m[36m(pid=49945)[0m GPU available: False, used: False
[2m[36m(pid=49945)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49945)[0m 
[2m[36m(pid=49945)[0m   | Name      | Type              | Params
[2m[36m(pid=49945)[0m ------------------------------------------------
[2m[36m(pid=49945)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49945)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49945)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49945)[0m ------------------------------------------------
[2m[36m(pid=49945)[0m 8.7 K     Trainable params
[2m[36m(pid=49945)[0m 0         Non-trainable params
[2m[36m(pid=49945)[0m 8.7 K     Total params
[2m[36m(pid=37126)[0m time to fit was 117.33271074295044
[2m[36m(pid=37126)[0m GPU available: False, used: False
[2m[36m(pid=37126)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=37126)[0m 
[2m[36m(pid=37126)[0m   | Name      | Type              | Params
[2m[36m(pid=37126)[0m ------------------------------------------------
[2m[36m(pid=37126)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=37126)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=37126)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=37126)[0m ------------------------------------------------
[2m[36m(pid=37126)[0m 8.7 K     Trainable params
[2m[36m(pid=37126)[0m 0         Non-trainable params
[2m[36m(pid=37126)[0m 8.7 K     Total params
[2m[36m(pid=28288)[0m time to fit was 315.94609355926514
Result for _inner_e8deb_00093:
  auc: 0.8764090299606323
  date: 2021-03-19_13-16-19
  done: false
  experiment_id: 4890de21138348b4b68d9b37a2e697b7
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 28288
  time_since_restore: 1011.9993662834167
  time_this_iter_s: 1011.9993662834167
  time_total_s: 1011.9993662834167
  timestamp: 1616156179
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00093
  
[2m[36m(pid=28288)[0m Finished run with seed 0 - lr 0.001 - sec_lr 1 - bs 256 - mean val auc: 0.8764090299606323
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 107/180 (1 PENDING, 26 RUNNING, 80 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    |       |           32 |     0 | 5     |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00065 | RUNNING    |       |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    |       |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |       |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00106 | PENDING    |       |           64 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 87 more trials not shown (16 RUNNING, 70 TERMINATED)


Result for _inner_e8deb_00093:
  auc: 0.8764090299606323
  date: 2021-03-19_13-16-19
  done: true
  experiment_id: 4890de21138348b4b68d9b37a2e697b7
  experiment_tag: 93_batch_size=256,eta=0.0,lr=0.001,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 28288
  time_since_restore: 1011.9993662834167
  time_this_iter_s: 1011.9993662834167
  time_total_s: 1011.9993662834167
  timestamp: 1616156179
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00093
  
[2m[36m(pid=50729)[0m time to fit was 58.74270796775818
Result for _inner_e8deb_00104:
  auc: 0.9117217659950256
  date: 2021-03-19_13-16-20
  done: false
  experiment_id: 7f4c5c49d2a5430dbaee6d272b134840
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 50729
  time_since_restore: 307.7920618057251
  time_this_iter_s: 307.7920618057251
  time_total_s: 307.7920618057251
  timestamp: 1616156180
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00104
  
Result for _inner_e8deb_00104:
  auc: 0.9117217659950256
  date: 2021-03-19_13-16-20
  done: true
  experiment_id: 7f4c5c49d2a5430dbaee6d272b134840
  experiment_tag: 104_batch_size=512,eta=0.0,lr=0.1,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 50729
  time_since_restore: 307.7920618057251
  time_this_iter_s: 307.7920618057251
  time_total_s: 307.7920618057251
  timestamp: 1616156180
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00104
  
[2m[36m(pid=50729)[0m Finished run with seed 0 - lr 0.1 - sec_lr 1 - bs 512 - mean val auc: 0.9117217659950256
2021-03-19 13:16:21,491	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffffdd5d56a001000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 4.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 2 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=8415)[0m Starting run with seed 0 - lr 1 - sec_lr 1 - bs 64
[2m[36m(pid=8415)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=8415)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=8415)[0m GPU available: False, used: False
[2m[36m(pid=8415)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8453)[0m Starting run with seed 0 - lr 1 - sec_lr 1 - bs 128
[2m[36m(pid=8415)[0m 
[2m[36m(pid=8415)[0m   | Name      | Type              | Params
[2m[36m(pid=8415)[0m ------------------------------------------------
[2m[36m(pid=8415)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8415)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8415)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8415)[0m ------------------------------------------------
[2m[36m(pid=8415)[0m 8.7 K     Trainable params
[2m[36m(pid=8415)[0m 0         Non-trainable params
[2m[36m(pid=8415)[0m 8.7 K     Total params
[2m[36m(pid=8415)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8415)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8453)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=8453)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=8453)[0m GPU available: False, used: False
[2m[36m(pid=8453)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8453)[0m 
[2m[36m(pid=8453)[0m   | Name      | Type              | Params
[2m[36m(pid=8453)[0m ------------------------------------------------
[2m[36m(pid=8453)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8453)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8453)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8453)[0m ------------------------------------------------
[2m[36m(pid=8453)[0m 8.7 K     Trainable params
[2m[36m(pid=8453)[0m 0         Non-trainable params
[2m[36m(pid=8453)[0m 8.7 K     Total params
[2m[36m(pid=8453)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8453)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8415)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8415)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8415)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=8415)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8453)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8453)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8453)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=8453)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48984)[0m time to fit was 641.2209272384644
[2m[36m(pid=48984)[0m GPU available: False, used: False
[2m[36m(pid=48984)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48984)[0m 
[2m[36m(pid=48984)[0m   | Name      | Type              | Params
[2m[36m(pid=48984)[0m ------------------------------------------------
[2m[36m(pid=48984)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48984)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48984)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48984)[0m ------------------------------------------------
[2m[36m(pid=48984)[0m 8.7 K     Trainable params
[2m[36m(pid=48984)[0m 0         Non-trainable params
[2m[36m(pid=48984)[0m 8.7 K     Total params
[2m[36m(pid=27849)[0m time to fit was 529.8154296875
[2m[36m(pid=27849)[0m GPU available: False, used: False
[2m[36m(pid=27849)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27849)[0m 
[2m[36m(pid=27849)[0m   | Name      | Type              | Params
[2m[36m(pid=27849)[0m ------------------------------------------------
[2m[36m(pid=27849)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27849)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27849)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27849)[0m ------------------------------------------------
[2m[36m(pid=27849)[0m 8.7 K     Trainable params
[2m[36m(pid=27849)[0m 0         Non-trainable params
[2m[36m(pid=27849)[0m 8.7 K     Total params
[2m[36m(pid=40855)[0m time to fit was 112.40210509300232
[2m[36m(pid=40855)[0m GPU available: False, used: False
[2m[36m(pid=40855)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40855)[0m 
[2m[36m(pid=40855)[0m   | Name      | Type              | Params
[2m[36m(pid=40855)[0m ------------------------------------------------
[2m[36m(pid=40855)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40855)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40855)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40855)[0m ------------------------------------------------
[2m[36m(pid=40855)[0m 8.7 K     Trainable params
[2m[36m(pid=40855)[0m 0         Non-trainable params
[2m[36m(pid=40855)[0m 8.7 K     Total params
[2m[36m(pid=49945)[0m time to fit was 104.84893870353699
[2m[36m(pid=49945)[0m GPU available: False, used: False
[2m[36m(pid=49945)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49945)[0m 
[2m[36m(pid=49945)[0m   | Name      | Type              | Params
[2m[36m(pid=49945)[0m ------------------------------------------------
[2m[36m(pid=49945)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49945)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49945)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49945)[0m ------------------------------------------------
[2m[36m(pid=49945)[0m 8.7 K     Trainable params
[2m[36m(pid=49945)[0m 0         Non-trainable params
[2m[36m(pid=49945)[0m 8.7 K     Total params
[2m[36m(pid=11272)[0m time to fit was 1293.9346406459808
[2m[36m(pid=11272)[0m GPU available: False, used: False
[2m[36m(pid=11272)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=11272)[0m 
[2m[36m(pid=11272)[0m   | Name      | Type              | Params
[2m[36m(pid=11272)[0m ------------------------------------------------
[2m[36m(pid=11272)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=11272)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=11272)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=11272)[0m ------------------------------------------------
[2m[36m(pid=11272)[0m 8.7 K     Trainable params
[2m[36m(pid=11272)[0m 0         Non-trainable params
[2m[36m(pid=11272)[0m 8.7 K     Total params
[2m[36m(pid=46019)[0m time to fit was 168.25839376449585
[2m[36m(pid=46019)[0m GPU available: False, used: False
[2m[36m(pid=46019)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=46019)[0m 
[2m[36m(pid=46019)[0m   | Name      | Type              | Params
[2m[36m(pid=46019)[0m ------------------------------------------------
[2m[36m(pid=46019)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=46019)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=46019)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=46019)[0m ------------------------------------------------
[2m[36m(pid=46019)[0m 8.7 K     Trainable params
[2m[36m(pid=46019)[0m 0         Non-trainable params
[2m[36m(pid=46019)[0m 8.7 K     Total params
[2m[36m(pid=29675)[0m time to fit was 641.7239301204681
[2m[36m(pid=29675)[0m GPU available: False, used: False
[2m[36m(pid=29675)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29675)[0m 
[2m[36m(pid=29675)[0m   | Name      | Type              | Params
[2m[36m(pid=29675)[0m ------------------------------------------------
[2m[36m(pid=29675)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29675)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29675)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29675)[0m ------------------------------------------------
[2m[36m(pid=29675)[0m 8.7 K     Trainable params
[2m[36m(pid=29675)[0m 0         Non-trainable params
[2m[36m(pid=29675)[0m 8.7 K     Total params
[2m[36m(pid=48139)[0m time to fit was 398.8402919769287
Result for _inner_e8deb_00055:
  auc: 0.6587060332298279
  date: 2021-03-19_13-18-06
  done: false
  experiment_id: 646a2ae57aed4757bbd0e0b2a557413f
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 48139
  time_since_restore: 3843.8018493652344
  time_this_iter_s: 3843.8018493652344
  time_total_s: 3843.8018493652344
  timestamp: 1616156286
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00055
  
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 109/180 (1 PENDING, 26 RUNNING, 82 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00055 | RUNNING    | 145.101.32.82:48139 |           32 |     0 | 5     |    0.01  |      1 |          3843.8  | 0.658706 |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |                     |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00065 | RUNNING    |                     |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    |                     |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |                     |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |                     |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00108 | PENDING    |                     |          256 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 89 more trials not shown (16 RUNNING, 72 TERMINATED)


[2m[36m(pid=48139)[0m Finished run with seed 0 - lr 5 - sec_lr 0.01 - bs 32 - mean val auc: 0.6587060332298279
Result for _inner_e8deb_00055:
  auc: 0.6587060332298279
  date: 2021-03-19_13-18-06
  done: true
  experiment_id: 646a2ae57aed4757bbd0e0b2a557413f
  experiment_tag: 55_batch_size=32,eta=0.0,lr=5,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 48139
  time_since_restore: 3843.8018493652344
  time_this_iter_s: 3843.8018493652344
  time_total_s: 3843.8018493652344
  timestamp: 1616156286
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00055
  
[2m[36m(pid=37126)[0m time to fit was 137.14584612846375
[2m[36m(pid=8525)[0m Starting run with seed 0 - lr 1 - sec_lr 1 - bs 256
[2m[36m(pid=8525)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=8525)[0m   return self.dataset.memberships[self.indices]
Result for _inner_e8deb_00098:
  auc: 0.9093109011650086
  date: 2021-03-19_13-18-15
  done: false
  experiment_id: 82d5058498454680b3b18f75f300989f
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 37126
  time_since_restore: 849.5046756267548
  time_this_iter_s: 849.5046756267548
  time_total_s: 849.5046756267548
  timestamp: 1616156295
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00098
  
[2m[36m(pid=37126)[0m Finished run with seed 0 - lr 0.01 - sec_lr 1 - bs 256 - mean val auc: 0.9093109011650086
[2m[36m(pid=8525)[0m GPU available: False, used: False
[2m[36m(pid=8525)[0m TPU available: None, using: 0 TPU cores
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 110/180 (1 PENDING, 26 RUNNING, 83 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00065 | RUNNING    |       |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    |       |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |       |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00109 | PENDING    |       |          512 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 90 more trials not shown (16 RUNNING, 73 TERMINATED)


Result for _inner_e8deb_00098:
  auc: 0.9093109011650086
  date: 2021-03-19_13-18-15
  done: true
  experiment_id: 82d5058498454680b3b18f75f300989f
  experiment_tag: 98_batch_size=256,eta=0.0,lr=0.01,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 37126
  time_since_restore: 849.5046756267548
  time_this_iter_s: 849.5046756267548
  time_total_s: 849.5046756267548
  timestamp: 1616156295
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00098
  
[2m[36m(pid=8525)[0m 
[2m[36m(pid=8525)[0m   | Name      | Type              | Params
[2m[36m(pid=8525)[0m ------------------------------------------------
[2m[36m(pid=8525)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8525)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8525)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8525)[0m ------------------------------------------------
[2m[36m(pid=8525)[0m 8.7 K     Trainable params
[2m[36m(pid=8525)[0m 0         Non-trainable params
[2m[36m(pid=8525)[0m 8.7 K     Total params
[2m[36m(pid=8525)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8525)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8525)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8525)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8525)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=8525)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8459)[0m Starting run with seed 0 - lr 1 - sec_lr 1 - bs 512
[2m[36m(pid=8459)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=8459)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=8459)[0m GPU available: False, used: False
[2m[36m(pid=8459)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8459)[0m 
[2m[36m(pid=8459)[0m   | Name      | Type              | Params
[2m[36m(pid=8459)[0m ------------------------------------------------
[2m[36m(pid=8459)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8459)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8459)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8459)[0m ------------------------------------------------
[2m[36m(pid=8459)[0m 8.7 K     Trainable params
[2m[36m(pid=8459)[0m 0         Non-trainable params
[2m[36m(pid=8459)[0m 8.7 K     Total params
[2m[36m(pid=8459)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8459)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=16527)[0m time to fit was 936.3620185852051
[2m[36m(pid=16527)[0m GPU available: False, used: False
[2m[36m(pid=16527)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=16527)[0m 
[2m[36m(pid=16527)[0m   | Name      | Type              | Params
[2m[36m(pid=16527)[0m ------------------------------------------------
[2m[36m(pid=16527)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=16527)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=16527)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=16527)[0m ------------------------------------------------
[2m[36m(pid=16527)[0m 8.7 K     Trainable params
[2m[36m(pid=16527)[0m 0         Non-trainable params
[2m[36m(pid=16527)[0m 8.7 K     Total params
[2m[36m(pid=8459)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8459)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8459)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=8459)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49945)[0m time to fit was 73.64888191223145
[2m[36m(pid=49945)[0m Finished run with seed 0 - lr 0.1 - sec_lr 1 - bs 256 - mean val auc: 0.9114055514335633
Result for _inner_e8deb_00103:
  auc: 0.9114055514335633
  date: 2021-03-19_13-18-31
  done: false
  experiment_id: 158726739316428f910bccf9a6cab7cc
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 49945
  time_since_restore: 462.436359167099
  time_this_iter_s: 462.436359167099
  time_total_s: 462.436359167099
  timestamp: 1616156311
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00103
  
== Status ==
Memory usage on this node: 9.6/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 111/180 (1 PENDING, 26 RUNNING, 84 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00065 | RUNNING    |       |           32 |     0 | 0.01  |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    |       |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |       |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00110 | PENDING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 91 more trials not shown (16 RUNNING, 74 TERMINATED)


Result for _inner_e8deb_00103:
  auc: 0.9114055514335633
  date: 2021-03-19_13-18-31
  done: true
  experiment_id: 158726739316428f910bccf9a6cab7cc
  experiment_tag: 103_batch_size=256,eta=0.0,lr=0.1,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 49945
  time_since_restore: 462.436359167099
  time_this_iter_s: 462.436359167099
  time_total_s: 462.436359167099
  timestamp: 1616156311
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00103
  
[2m[36m(pid=8457)[0m Starting run with seed 0 - lr 2 - sec_lr 1 - bs 32
[2m[36m(pid=8457)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=8457)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=8457)[0m GPU available: False, used: False
[2m[36m(pid=8457)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8457)[0m 
[2m[36m(pid=8457)[0m   | Name      | Type              | Params
[2m[36m(pid=8457)[0m ------------------------------------------------
[2m[36m(pid=8457)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8457)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8457)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8457)[0m ------------------------------------------------
[2m[36m(pid=8457)[0m 8.7 K     Trainable params
[2m[36m(pid=8457)[0m 0         Non-trainable params
[2m[36m(pid=8457)[0m 8.7 K     Total params
[2m[36m(pid=8457)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8457)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8457)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8457)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8457)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=8457)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8453)[0m time to fit was 133.04549431800842
[2m[36m(pid=8453)[0m GPU available: False, used: False
[2m[36m(pid=8453)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=13798)[0m time to fit was 467.9290943145752
[2m[36m(pid=8453)[0m 
[2m[36m(pid=8453)[0m   | Name      | Type              | Params
[2m[36m(pid=8453)[0m ------------------------------------------------
[2m[36m(pid=8453)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8453)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8453)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8453)[0m ------------------------------------------------
[2m[36m(pid=8453)[0m 8.7 K     Trainable params
[2m[36m(pid=8453)[0m 0         Non-trainable params
[2m[36m(pid=8453)[0m 8.7 K     Total params
[2m[36m(pid=13798)[0m Finished run with seed 0 - lr 0.01 - sec_lr 0.1 - bs 32 - mean val auc: 0.9101103186607361
Result for _inner_e8deb_00065:
  auc: 0.9101103186607361
  date: 2021-03-19_13-18-45
  done: false
  experiment_id: 9c187ece33fc4ec2b71d82d137922193
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 13798
  time_since_restore: 3298.098415374756
  time_this_iter_s: 3298.098415374756
  time_total_s: 3298.098415374756
  timestamp: 1616156325
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00065
  
== Status ==
Memory usage on this node: 9.6/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 112/180 (1 PENDING, 26 RUNNING, 85 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |                     |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00065 | RUNNING    | 145.101.32.82:13798 |           32 |     0 | 0.01  |    0.1   |      1 |          3298.1  | 0.91011  |
| _inner_e8deb_00075 | RUNNING    |                     |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |                     |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |                     |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |                     |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00111 | PENDING    |                     |           64 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 92 more trials not shown (16 RUNNING, 75 TERMINATED)


Result for _inner_e8deb_00065:
  auc: 0.9101103186607361
  date: 2021-03-19_13-18-45
  done: true
  experiment_id: 9c187ece33fc4ec2b71d82d137922193
  experiment_tag: 65_batch_size=32,eta=0.0,lr=0.01,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 13798
  time_since_restore: 3298.098415374756
  time_this_iter_s: 3298.098415374756
  time_total_s: 3298.098415374756
  timestamp: 1616156325
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00065
  
[2m[36m(pid=8460)[0m Starting run with seed 0 - lr 2 - sec_lr 1 - bs 64
[2m[36m(pid=8460)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=8460)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=8460)[0m GPU available: False, used: False
[2m[36m(pid=8460)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8460)[0m 
[2m[36m(pid=8460)[0m   | Name      | Type              | Params
[2m[36m(pid=8460)[0m ------------------------------------------------
[2m[36m(pid=8460)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8460)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8460)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8460)[0m ------------------------------------------------
[2m[36m(pid=8460)[0m 8.7 K     Trainable params
[2m[36m(pid=8460)[0m 0         Non-trainable params
[2m[36m(pid=8460)[0m 8.7 K     Total params
[2m[36m(pid=8460)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8460)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8460)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8460)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8460)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=8460)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=6688)[0m time to fit was 1767.7427389621735
[2m[36m(pid=6688)[0m GPU available: False, used: False
[2m[36m(pid=6688)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=6688)[0m 
[2m[36m(pid=6688)[0m   | Name      | Type              | Params
[2m[36m(pid=6688)[0m ------------------------------------------------
[2m[36m(pid=6688)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=6688)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=6688)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=6688)[0m ------------------------------------------------
[2m[36m(pid=6688)[0m 8.7 K     Trainable params
[2m[36m(pid=6688)[0m 0         Non-trainable params
[2m[36m(pid=6688)[0m 8.7 K     Total params
[2m[36m(pid=35542)[0m time to fit was 379.4005551338196
[2m[36m(pid=35542)[0m GPU available: False, used: False
[2m[36m(pid=35542)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35542)[0m 
[2m[36m(pid=35542)[0m   | Name      | Type              | Params
[2m[36m(pid=35542)[0m ------------------------------------------------
[2m[36m(pid=35542)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35542)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35542)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35542)[0m ------------------------------------------------
[2m[36m(pid=35542)[0m 8.7 K     Trainable params
[2m[36m(pid=35542)[0m 0         Non-trainable params
[2m[36m(pid=35542)[0m 8.7 K     Total params
[2m[36m(pid=40855)[0m time to fit was 145.8975806236267
[2m[36m(pid=40855)[0m Finished run with seed 0 - lr 0.01 - sec_lr 1 - bs 512 - mean val auc: 0.9092746496200561
Result for _inner_e8deb_00099:
  auc: 0.9092746496200561
  date: 2021-03-19_13-19-32
  done: false
  experiment_id: cae02db7d93a495b8f09ac43303b7b10
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 40855
  time_since_restore: 815.0416176319122
  time_this_iter_s: 815.0416176319122
  time_total_s: 815.0416176319122
  timestamp: 1616156372
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00099
  
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 113/180 (1 PENDING, 26 RUNNING, 86 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    |       |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |       |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00112 | PENDING    |       |          128 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 93 more trials not shown (16 RUNNING, 76 TERMINATED)


Result for _inner_e8deb_00099:
  auc: 0.9092746496200561
  date: 2021-03-19_13-19-32
  done: true
  experiment_id: cae02db7d93a495b8f09ac43303b7b10
  experiment_tag: 99_batch_size=512,eta=0.0,lr=0.01,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 40855
  time_since_restore: 815.0416176319122
  time_this_iter_s: 815.0416176319122
  time_total_s: 815.0416176319122
  timestamp: 1616156372
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00099
  
[2m[36m(pid=8459)[0m time to fit was 68.30685877799988
[2m[36m(pid=8459)[0m GPU available: False, used: False
[2m[36m(pid=8459)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8459)[0m 
[2m[36m(pid=8459)[0m   | Name      | Type              | Params
[2m[36m(pid=8459)[0m ------------------------------------------------
[2m[36m(pid=8459)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8459)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8459)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8459)[0m ------------------------------------------------
[2m[36m(pid=8459)[0m 8.7 K     Trainable params
[2m[36m(pid=8459)[0m 0         Non-trainable params
[2m[36m(pid=8459)[0m 8.7 K     Total params
[2m[36m(pid=8525)[0m time to fit was 83.68634629249573
[2m[36m(pid=8525)[0m GPU available: False, used: False
[2m[36m(pid=8525)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8525)[0m 
[2m[36m(pid=8525)[0m   | Name      | Type              | Params
[2m[36m(pid=8525)[0m ------------------------------------------------
[2m[36m(pid=8525)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8525)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8525)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8525)[0m ------------------------------------------------
[2m[36m(pid=8525)[0m 8.7 K     Trainable params
[2m[36m(pid=8525)[0m 0         Non-trainable params
[2m[36m(pid=8525)[0m 8.7 K     Total params
[2m[36m(pid=14376)[0m Starting run with seed 0 - lr 2 - sec_lr 1 - bs 128
[2m[36m(pid=14376)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=14376)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=14376)[0m GPU available: False, used: False
[2m[36m(pid=14376)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=14376)[0m 
[2m[36m(pid=14376)[0m   | Name      | Type              | Params
[2m[36m(pid=14376)[0m ------------------------------------------------
[2m[36m(pid=14376)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=14376)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=14376)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=14376)[0m ------------------------------------------------
[2m[36m(pid=14376)[0m 8.7 K     Trainable params
[2m[36m(pid=14376)[0m 0         Non-trainable params
[2m[36m(pid=14376)[0m 8.7 K     Total params
[2m[36m(pid=14376)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=14376)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=14376)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=14376)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=14376)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=14376)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=5013)[0m time to fit was 642.1118981838226
[2m[36m(pid=5013)[0m GPU available: False, used: False
[2m[36m(pid=5013)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=5013)[0m 
[2m[36m(pid=5013)[0m   | Name      | Type              | Params
[2m[36m(pid=5013)[0m ------------------------------------------------
[2m[36m(pid=5013)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=5013)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=5013)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=5013)[0m ------------------------------------------------
[2m[36m(pid=5013)[0m 8.7 K     Trainable params
[2m[36m(pid=5013)[0m 0         Non-trainable params
[2m[36m(pid=5013)[0m 8.7 K     Total params
[2m[36m(pid=43563)[0m time to fit was 339.7601718902588
[2m[36m(pid=43563)[0m GPU available: False, used: False
[2m[36m(pid=43563)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=43563)[0m 
[2m[36m(pid=43563)[0m   | Name      | Type              | Params
[2m[36m(pid=43563)[0m ------------------------------------------------
[2m[36m(pid=43563)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=43563)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=43563)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=43563)[0m ------------------------------------------------
[2m[36m(pid=43563)[0m 8.7 K     Trainable params
[2m[36m(pid=43563)[0m 0         Non-trainable params
[2m[36m(pid=43563)[0m 8.7 K     Total params
[2m[36m(pid=46019)[0m time to fit was 153.77057361602783
Result for _inner_e8deb_00102:
  auc: 0.9103068947792053
  date: 2021-03-19_13-20-21
  done: false
  experiment_id: 2fbae85781d64faab2bbce5f06fafdb4
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 46019
  time_since_restore: 702.6889927387238
  time_this_iter_s: 702.6889927387238
  time_total_s: 702.6889927387238
  timestamp: 1616156421
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00102
  
[2m[36m(pid=46019)[0m Finished run with seed 0 - lr 0.1 - sec_lr 1 - bs 128 - mean val auc: 0.9103068947792053
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 114/180 (1 PENDING, 26 RUNNING, 87 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    |       |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |       |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00113 | PENDING    |       |          256 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 94 more trials not shown (16 RUNNING, 77 TERMINATED)


Result for _inner_e8deb_00102:
  auc: 0.9103068947792053
  date: 2021-03-19_13-20-21
  done: true
  experiment_id: 2fbae85781d64faab2bbce5f06fafdb4
  experiment_tag: 102_batch_size=128,eta=0.0,lr=0.1,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 46019
  time_since_restore: 702.6889927387238
  time_this_iter_s: 702.6889927387238
  time_total_s: 702.6889927387238
  timestamp: 1616156421
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00102
  
2021-03-19 13:20:22,078	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff47a4ec9d01000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=8459)[0m time to fit was 58.8135883808136
[2m[36m(pid=8459)[0m GPU available: False, used: False
[2m[36m(pid=8459)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8459)[0m 
[2m[36m(pid=8459)[0m   | Name      | Type              | Params
[2m[36m(pid=8459)[0m ------------------------------------------------
[2m[36m(pid=8459)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8459)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8459)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8459)[0m ------------------------------------------------
[2m[36m(pid=8459)[0m 8.7 K     Trainable params
[2m[36m(pid=8459)[0m 0         Non-trainable params
[2m[36m(pid=8459)[0m 8.7 K     Total params
[2m[36m(pid=8415)[0m time to fit was 241.25804734230042
[2m[36m(pid=15746)[0m Starting run with seed 0 - lr 2 - sec_lr 1 - bs 256
[2m[36m(pid=15746)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15746)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=15746)[0m GPU available: False, used: False
[2m[36m(pid=15746)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8415)[0m GPU available: False, used: False
[2m[36m(pid=8415)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=15746)[0m 
[2m[36m(pid=15746)[0m   | Name      | Type              | Params
[2m[36m(pid=15746)[0m ------------------------------------------------
[2m[36m(pid=15746)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=15746)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=15746)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=15746)[0m ------------------------------------------------
[2m[36m(pid=15746)[0m 8.7 K     Trainable params
[2m[36m(pid=15746)[0m 0         Non-trainable params
[2m[36m(pid=15746)[0m 8.7 K     Total params
[2m[36m(pid=15746)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=15746)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8415)[0m 
[2m[36m(pid=8415)[0m   | Name      | Type              | Params
[2m[36m(pid=8415)[0m ------------------------------------------------
[2m[36m(pid=8415)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8415)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8415)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8415)[0m ------------------------------------------------
[2m[36m(pid=8415)[0m 8.7 K     Trainable params
[2m[36m(pid=8415)[0m 0         Non-trainable params
[2m[36m(pid=8415)[0m 8.7 K     Total params
[2m[36m(pid=15746)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=15746)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=15746)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=15746)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=3723)[0m time to fit was 424.3233561515808
[2m[36m(pid=3723)[0m GPU available: False, used: False
[2m[36m(pid=3723)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=3723)[0m 
[2m[36m(pid=3723)[0m   | Name      | Type              | Params
[2m[36m(pid=3723)[0m ------------------------------------------------
[2m[36m(pid=3723)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=3723)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=3723)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=3723)[0m ------------------------------------------------
[2m[36m(pid=3723)[0m 8.7 K     Trainable params
[2m[36m(pid=3723)[0m 0         Non-trainable params
[2m[36m(pid=3723)[0m 8.7 K     Total params
[2m[36m(pid=26407)[0m time to fit was 448.4588086605072
[2m[36m(pid=26407)[0m GPU available: False, used: False
[2m[36m(pid=26407)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26407)[0m 
[2m[36m(pid=26407)[0m   | Name      | Type              | Params
[2m[36m(pid=26407)[0m ------------------------------------------------
[2m[36m(pid=26407)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26407)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26407)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26407)[0m ------------------------------------------------
[2m[36m(pid=26407)[0m 8.7 K     Trainable params
[2m[36m(pid=26407)[0m 0         Non-trainable params
[2m[36m(pid=26407)[0m 8.7 K     Total params
[2m[36m(pid=8525)[0m time to fit was 101.24652934074402
[2m[36m(pid=8525)[0m GPU available: False, used: False
[2m[36m(pid=8525)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8525)[0m 
[2m[36m(pid=8525)[0m   | Name      | Type              | Params
[2m[36m(pid=8525)[0m ------------------------------------------------
[2m[36m(pid=8525)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8525)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8525)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8525)[0m ------------------------------------------------
[2m[36m(pid=8525)[0m 8.7 K     Trainable params
[2m[36m(pid=8525)[0m 0         Non-trainable params
[2m[36m(pid=8525)[0m 8.7 K     Total params
[2m[36m(pid=8453)[0m time to fit was 163.437824010849
[2m[36m(pid=8453)[0m GPU available: False, used: False
[2m[36m(pid=8453)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8453)[0m 
[2m[36m(pid=8453)[0m   | Name      | Type              | Params
[2m[36m(pid=8453)[0m ------------------------------------------------
[2m[36m(pid=8453)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8453)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8453)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8453)[0m ------------------------------------------------
[2m[36m(pid=8453)[0m 8.7 K     Trainable params
[2m[36m(pid=8453)[0m 0         Non-trainable params
[2m[36m(pid=8453)[0m 8.7 K     Total params
[2m[36m(pid=8459)[0m time to fit was 66.99586606025696
[2m[36m(pid=8459)[0m GPU available: False, used: False
[2m[36m(pid=8459)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8459)[0m 
[2m[36m(pid=8459)[0m   | Name      | Type              | Params
[2m[36m(pid=8459)[0m ------------------------------------------------
[2m[36m(pid=8459)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8459)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8459)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8459)[0m ------------------------------------------------
[2m[36m(pid=8459)[0m 8.7 K     Trainable params
[2m[36m(pid=8459)[0m 0         Non-trainable params
[2m[36m(pid=8459)[0m 8.7 K     Total params
[2m[36m(pid=25102)[0m time to fit was 957.0643601417542
[2m[36m(pid=25102)[0m GPU available: False, used: False
[2m[36m(pid=25102)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=25102)[0m 
[2m[36m(pid=25102)[0m   | Name      | Type              | Params
[2m[36m(pid=25102)[0m ------------------------------------------------
[2m[36m(pid=25102)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=25102)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=25102)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=25102)[0m ------------------------------------------------
[2m[36m(pid=25102)[0m 8.7 K     Trainable params
[2m[36m(pid=25102)[0m 0         Non-trainable params
[2m[36m(pid=25102)[0m 8.7 K     Total params
[2m[36m(pid=15746)[0m time to fit was 83.98004698753357
[2m[36m(pid=15746)[0m GPU available: False, used: False
[2m[36m(pid=15746)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=15746)[0m 
[2m[36m(pid=15746)[0m   | Name      | Type              | Params
[2m[36m(pid=15746)[0m ------------------------------------------------
[2m[36m(pid=15746)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=15746)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=15746)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=15746)[0m ------------------------------------------------
[2m[36m(pid=15746)[0m 8.7 K     Trainable params
[2m[36m(pid=15746)[0m 0         Non-trainable params
[2m[36m(pid=15746)[0m 8.7 K     Total params
[2m[36m(pid=35542)[0m time to fit was 192.21839928627014
[2m[36m(pid=35542)[0m GPU available: False, used: False
[2m[36m(pid=35542)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35542)[0m 
[2m[36m(pid=35542)[0m   | Name      | Type              | Params
[2m[36m(pid=35542)[0m ------------------------------------------------
[2m[36m(pid=35542)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35542)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35542)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35542)[0m ------------------------------------------------
[2m[36m(pid=35542)[0m 8.7 K     Trainable params
[2m[36m(pid=35542)[0m 0         Non-trainable params
[2m[36m(pid=35542)[0m 8.7 K     Total params
[2m[36m(pid=8459)[0m time to fit was 65.62655210494995
[2m[36m(pid=8459)[0m GPU available: False, used: False
[2m[36m(pid=8459)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8459)[0m 
[2m[36m(pid=8459)[0m   | Name      | Type              | Params
[2m[36m(pid=8459)[0m ------------------------------------------------
[2m[36m(pid=8459)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8459)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8459)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8459)[0m ------------------------------------------------
[2m[36m(pid=8459)[0m 8.7 K     Trainable params
[2m[36m(pid=8459)[0m 0         Non-trainable params
[2m[36m(pid=8459)[0m 8.7 K     Total params
[2m[36m(pid=8525)[0m time to fit was 90.41959524154663
[2m[36m(pid=8525)[0m GPU available: False, used: False
[2m[36m(pid=8525)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8525)[0m 
[2m[36m(pid=8525)[0m   | Name      | Type              | Params
[2m[36m(pid=8525)[0m ------------------------------------------------
[2m[36m(pid=8525)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8525)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8525)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8525)[0m ------------------------------------------------
[2m[36m(pid=8525)[0m 8.7 K     Trainable params
[2m[36m(pid=8525)[0m 0         Non-trainable params
[2m[36m(pid=8525)[0m 8.7 K     Total params
[2m[36m(pid=35861)[0m time to fit was 1777.726440668106
[2m[36m(pid=35861)[0m GPU available: False, used: False
[2m[36m(pid=35861)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35861)[0m 
[2m[36m(pid=35861)[0m   | Name      | Type              | Params
[2m[36m(pid=35861)[0m ------------------------------------------------
[2m[36m(pid=35861)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35861)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35861)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35861)[0m ------------------------------------------------
[2m[36m(pid=35861)[0m 8.7 K     Trainable params
[2m[36m(pid=35861)[0m 0         Non-trainable params
[2m[36m(pid=35861)[0m 8.7 K     Total params
[2m[36m(pid=34531)[0m time to fit was 545.6308100223541
[2m[36m(pid=34531)[0m GPU available: False, used: False
[2m[36m(pid=34531)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=34531)[0m 
[2m[36m(pid=34531)[0m   | Name      | Type              | Params
[2m[36m(pid=34531)[0m ------------------------------------------------
[2m[36m(pid=34531)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=34531)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=34531)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=34531)[0m ------------------------------------------------
[2m[36m(pid=34531)[0m 8.7 K     Trainable params
[2m[36m(pid=34531)[0m 0         Non-trainable params
[2m[36m(pid=34531)[0m 8.7 K     Total params
[2m[36m(pid=14376)[0m time to fit was 214.82193613052368
[2m[36m(pid=14376)[0m GPU available: False, used: False
[2m[36m(pid=14376)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=14376)[0m 
[2m[36m(pid=14376)[0m   | Name      | Type              | Params
[2m[36m(pid=14376)[0m ------------------------------------------------
[2m[36m(pid=14376)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=14376)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=14376)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=14376)[0m ------------------------------------------------
[2m[36m(pid=14376)[0m 8.7 K     Trainable params
[2m[36m(pid=14376)[0m 0         Non-trainable params
[2m[36m(pid=14376)[0m 8.7 K     Total params
[2m[36m(pid=41401)[0m time to fit was 569.945675611496
[2m[36m(pid=41401)[0m GPU available: False, used: False
[2m[36m(pid=41401)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=41401)[0m 
[2m[36m(pid=41401)[0m   | Name      | Type              | Params
[2m[36m(pid=41401)[0m ------------------------------------------------
[2m[36m(pid=41401)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=41401)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=41401)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=41401)[0m ------------------------------------------------
[2m[36m(pid=41401)[0m 8.7 K     Trainable params
[2m[36m(pid=41401)[0m 0         Non-trainable params
[2m[36m(pid=41401)[0m 8.7 K     Total params
[2m[36m(pid=15746)[0m time to fit was 101.0779116153717
[2m[36m(pid=15746)[0m GPU available: False, used: False
[2m[36m(pid=15746)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=15746)[0m 
[2m[36m(pid=15746)[0m   | Name      | Type              | Params
[2m[36m(pid=15746)[0m ------------------------------------------------
[2m[36m(pid=15746)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=15746)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=15746)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=15746)[0m ------------------------------------------------
[2m[36m(pid=15746)[0m 8.7 K     Trainable params
[2m[36m(pid=15746)[0m 0         Non-trainable params
[2m[36m(pid=15746)[0m 8.7 K     Total params
[2m[36m(pid=8106)[0m time to fit was 944.9276580810547
[2m[36m(pid=8106)[0m GPU available: False, used: False
[2m[36m(pid=8106)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8106)[0m 
[2m[36m(pid=8106)[0m   | Name      | Type              | Params
[2m[36m(pid=8106)[0m ------------------------------------------------
[2m[36m(pid=8106)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8106)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8106)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8106)[0m ------------------------------------------------
[2m[36m(pid=8106)[0m 8.7 K     Trainable params
[2m[36m(pid=8106)[0m 0         Non-trainable params
[2m[36m(pid=8106)[0m 8.7 K     Total params
[2m[36m(pid=8459)[0m time to fit was 60.07149052619934
Result for _inner_e8deb_00109:
  auc: 0.912228786945343
  date: 2021-03-19_13-23-44
  done: false
  experiment_id: ae2fe25b6db4433381843b6caba4afa9
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8459
  time_since_restore: 321.00065565109253
  time_this_iter_s: 321.00065565109253
  time_total_s: 321.00065565109253
  timestamp: 1616156624
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00109
  
[2m[36m(pid=8459)[0m Finished run with seed 0 - lr 1 - sec_lr 1 - bs 512 - mean val auc: 0.912228786945343
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 115/180 (1 PENDING, 26 RUNNING, 88 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    |       |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |       |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00114 | PENDING    |       |          512 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 95 more trials not shown (16 RUNNING, 78 TERMINATED)


Result for _inner_e8deb_00109:
  auc: 0.912228786945343
  date: 2021-03-19_13-23-44
  done: true
  experiment_id: ae2fe25b6db4433381843b6caba4afa9
  experiment_tag: 109_batch_size=512,eta=0.0,lr=1,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8459
  time_since_restore: 321.00065565109253
  time_this_iter_s: 321.00065565109253
  time_total_s: 321.00065565109253
  timestamp: 1616156624
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00109
  
[2m[36m(pid=8453)[0m time to fit was 142.87823128700256
[2m[36m(pid=8453)[0m GPU available: False, used: False
[2m[36m(pid=8453)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8453)[0m 
[2m[36m(pid=8453)[0m   | Name      | Type              | Params
[2m[36m(pid=8453)[0m ------------------------------------------------
[2m[36m(pid=8453)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8453)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8453)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8453)[0m ------------------------------------------------
[2m[36m(pid=8453)[0m 8.7 K     Trainable params
[2m[36m(pid=8453)[0m 0         Non-trainable params
[2m[36m(pid=8453)[0m 8.7 K     Total params
[2m[36m(pid=21665)[0m Starting run with seed 0 - lr 2 - sec_lr 1 - bs 512
[2m[36m(pid=21665)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21665)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=21665)[0m GPU available: False, used: False
[2m[36m(pid=21665)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=21665)[0m 
[2m[36m(pid=21665)[0m   | Name      | Type              | Params
[2m[36m(pid=21665)[0m ------------------------------------------------
[2m[36m(pid=21665)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=21665)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=21665)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=21665)[0m ------------------------------------------------
[2m[36m(pid=21665)[0m 8.7 K     Trainable params
[2m[36m(pid=21665)[0m 0         Non-trainable params
[2m[36m(pid=21665)[0m 8.7 K     Total params
[2m[36m(pid=21665)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=21665)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=21665)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=21665)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=21665)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=21665)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=43563)[0m time to fit was 240.31136751174927
[2m[36m(pid=43563)[0m GPU available: False, used: False
[2m[36m(pid=43563)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=43563)[0m 
[2m[36m(pid=43563)[0m   | Name      | Type              | Params
[2m[36m(pid=43563)[0m ------------------------------------------------
[2m[36m(pid=43563)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=43563)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=43563)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=43563)[0m ------------------------------------------------
[2m[36m(pid=43563)[0m 8.7 K     Trainable params
[2m[36m(pid=43563)[0m 0         Non-trainable params
[2m[36m(pid=43563)[0m 8.7 K     Total params
[2m[36m(pid=21665)[0m time to fit was 46.53286528587341
[2m[36m(pid=21665)[0m GPU available: False, used: False
[2m[36m(pid=21665)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=21665)[0m 
[2m[36m(pid=21665)[0m   | Name      | Type              | Params
[2m[36m(pid=21665)[0m ------------------------------------------------
[2m[36m(pid=21665)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=21665)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=21665)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=21665)[0m ------------------------------------------------
[2m[36m(pid=21665)[0m 8.7 K     Trainable params
[2m[36m(pid=21665)[0m 0         Non-trainable params
[2m[36m(pid=21665)[0m 8.7 K     Total params
[2m[36m(pid=8525)[0m time to fit was 116.66037654876709
[2m[36m(pid=8525)[0m GPU available: False, used: False
[2m[36m(pid=8525)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8525)[0m 
[2m[36m(pid=8525)[0m   | Name      | Type              | Params
[2m[36m(pid=8525)[0m ------------------------------------------------
[2m[36m(pid=8525)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8525)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8525)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8525)[0m ------------------------------------------------
[2m[36m(pid=8525)[0m 8.7 K     Trainable params
[2m[36m(pid=8525)[0m 0         Non-trainable params
[2m[36m(pid=8525)[0m 8.7 K     Total params
[2m[36m(pid=35542)[0m time to fit was 159.59061002731323
Result for _inner_e8deb_00097:
  auc: 0.9093352794647217
  date: 2021-03-19_13-24-52
  done: false
  experiment_id: 6058109e76884f1787cd4f5a4ba55397
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35542
  time_since_restore: 1291.7429473400116
  time_this_iter_s: 1291.7429473400116
  time_total_s: 1291.7429473400116
  timestamp: 1616156692
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00097
  
[2m[36m(pid=35542)[0m Finished run with seed 0 - lr 0.01 - sec_lr 1 - bs 128 - mean val auc: 0.9093352794647217
== Status ==
Memory usage on this node: 9.6/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 116/180 (1 PENDING, 26 RUNNING, 89 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    |       |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |       |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00115 | PENDING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 96 more trials not shown (16 RUNNING, 79 TERMINATED)


Result for _inner_e8deb_00097:
  auc: 0.9093352794647217
  date: 2021-03-19_13-24-52
  done: true
  experiment_id: 6058109e76884f1787cd4f5a4ba55397
  experiment_tag: 97_batch_size=128,eta=0.0,lr=0.01,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35542
  time_since_restore: 1291.7429473400116
  time_this_iter_s: 1291.7429473400116
  time_total_s: 1291.7429473400116
  timestamp: 1616156692
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00097
  
[2m[36m(pid=23636)[0m Starting run with seed 0 - lr 5 - sec_lr 1 - bs 32
[2m[36m(pid=23636)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=23636)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=23636)[0m GPU available: False, used: False
[2m[36m(pid=23636)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23636)[0m 
[2m[36m(pid=23636)[0m   | Name      | Type              | Params
[2m[36m(pid=23636)[0m ------------------------------------------------
[2m[36m(pid=23636)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23636)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23636)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23636)[0m ------------------------------------------------
[2m[36m(pid=23636)[0m 8.7 K     Trainable params
[2m[36m(pid=23636)[0m 0         Non-trainable params
[2m[36m(pid=23636)[0m 8.7 K     Total params
[2m[36m(pid=23636)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=23636)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23636)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=23636)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23636)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=23636)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=27849)[0m time to fit was 525.5765812397003
[2m[36m(pid=27849)[0m GPU available: False, used: False
[2m[36m(pid=27849)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27849)[0m 
[2m[36m(pid=27849)[0m   | Name      | Type              | Params
[2m[36m(pid=27849)[0m ------------------------------------------------
[2m[36m(pid=27849)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27849)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27849)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27849)[0m ------------------------------------------------
[2m[36m(pid=27849)[0m 8.7 K     Trainable params
[2m[36m(pid=27849)[0m 0         Non-trainable params
[2m[36m(pid=27849)[0m 8.7 K     Total params
[2m[36m(pid=15746)[0m time to fit was 114.04598355293274
[2m[36m(pid=15746)[0m GPU available: False, used: False
[2m[36m(pid=15746)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=15746)[0m 
[2m[36m(pid=15746)[0m   | Name      | Type              | Params
[2m[36m(pid=15746)[0m ------------------------------------------------
[2m[36m(pid=15746)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=15746)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=15746)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=15746)[0m ------------------------------------------------
[2m[36m(pid=15746)[0m 8.7 K     Trainable params
[2m[36m(pid=15746)[0m 0         Non-trainable params
[2m[36m(pid=15746)[0m 8.7 K     Total params
[2m[36m(pid=21665)[0m time to fit was 54.44207811355591
[2m[36m(pid=21665)[0m GPU available: False, used: False
[2m[36m(pid=21665)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=21665)[0m 
[2m[36m(pid=21665)[0m   | Name      | Type              | Params
[2m[36m(pid=21665)[0m ------------------------------------------------
[2m[36m(pid=21665)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=21665)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=21665)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=21665)[0m ------------------------------------------------
[2m[36m(pid=21665)[0m 8.7 K     Trainable params
[2m[36m(pid=21665)[0m 0         Non-trainable params
[2m[36m(pid=21665)[0m 8.7 K     Total params
[2m[36m(pid=8460)[0m time to fit was 409.51031017303467
[2m[36m(pid=8460)[0m GPU available: False, used: False
[2m[36m(pid=8460)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8460)[0m 
[2m[36m(pid=8460)[0m   | Name      | Type              | Params
[2m[36m(pid=8460)[0m ------------------------------------------------
[2m[36m(pid=8460)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8460)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8460)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8460)[0m ------------------------------------------------
[2m[36m(pid=8460)[0m 8.7 K     Trainable params
[2m[36m(pid=8460)[0m 0         Non-trainable params
[2m[36m(pid=8460)[0m 8.7 K     Total params
[2m[36m(pid=8415)[0m time to fit was 313.9018144607544
[2m[36m(pid=8415)[0m GPU available: False, used: False
[2m[36m(pid=8415)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8415)[0m 
[2m[36m(pid=8415)[0m   | Name      | Type              | Params
[2m[36m(pid=8415)[0m ------------------------------------------------
[2m[36m(pid=8415)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8415)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8415)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8415)[0m ------------------------------------------------
[2m[36m(pid=8415)[0m 8.7 K     Trainable params
[2m[36m(pid=8415)[0m 0         Non-trainable params
[2m[36m(pid=8415)[0m 8.7 K     Total params
[2m[36m(pid=14376)[0m time to fit was 152.38386964797974
[2m[36m(pid=14376)[0m GPU available: False, used: False
[2m[36m(pid=14376)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=14376)[0m 
[2m[36m(pid=14376)[0m   | Name      | Type              | Params
[2m[36m(pid=14376)[0m ------------------------------------------------
[2m[36m(pid=14376)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=14376)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=14376)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=14376)[0m ------------------------------------------------
[2m[36m(pid=14376)[0m 8.7 K     Trainable params
[2m[36m(pid=14376)[0m 0         Non-trainable params
[2m[36m(pid=14376)[0m 8.7 K     Total params
[2m[36m(pid=8525)[0m time to fit was 72.45951271057129
[2m[36m(pid=8525)[0m Finished run with seed 0 - lr 1 - sec_lr 1 - bs 256 - mean val auc: 0.9111915349960327
Result for _inner_e8deb_00108:
  auc: 0.9111915349960327
  date: 2021-03-19_13-26-01
  done: false
  experiment_id: 8258a06ff69a4dcebc5432a7844faac5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8525
  time_since_restore: 465.7980124950409
  time_this_iter_s: 465.7980124950409
  time_total_s: 465.7980124950409
  timestamp: 1616156761
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00108
  
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 117/180 (1 PENDING, 26 RUNNING, 90 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |       |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    |       |           32 |     0 | 1     |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |       |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00116 | PENDING    |       |           64 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 97 more trials not shown (16 RUNNING, 80 TERMINATED)


Result for _inner_e8deb_00108:
  auc: 0.9111915349960327
  date: 2021-03-19_13-26-01
  done: true
  experiment_id: 8258a06ff69a4dcebc5432a7844faac5
  experiment_tag: 108_batch_size=256,eta=0.0,lr=1,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8525
  time_since_restore: 465.7980124950409
  time_this_iter_s: 465.7980124950409
  time_total_s: 465.7980124950409
  timestamp: 1616156761
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00108
  
2021-03-19 13:26:02,917	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff20bec3c101000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=25581)[0m Starting run with seed 0 - lr 5 - sec_lr 1 - bs 64
[2m[36m(pid=25581)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=25581)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=25581)[0m GPU available: False, used: False
[2m[36m(pid=25581)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=25581)[0m 
[2m[36m(pid=25581)[0m   | Name      | Type              | Params
[2m[36m(pid=25581)[0m ------------------------------------------------
[2m[36m(pid=25581)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=25581)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=25581)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=25581)[0m ------------------------------------------------
[2m[36m(pid=25581)[0m 8.7 K     Trainable params
[2m[36m(pid=25581)[0m 0         Non-trainable params
[2m[36m(pid=25581)[0m 8.7 K     Total params
[2m[36m(pid=25581)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=25581)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8453)[0m time to fit was 142.2187488079071
[2m[36m(pid=25581)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=25581)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8453)[0m GPU available: False, used: False
[2m[36m(pid=8453)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8453)[0m 
[2m[36m(pid=8453)[0m   | Name      | Type              | Params
[2m[36m(pid=8453)[0m ------------------------------------------------
[2m[36m(pid=8453)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8453)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8453)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8453)[0m ------------------------------------------------
[2m[36m(pid=8453)[0m 8.7 K     Trainable params
[2m[36m(pid=8453)[0m 0         Non-trainable params
[2m[36m(pid=8453)[0m 8.7 K     Total params
[2m[36m(pid=25581)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=25581)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48984)[0m time to fit was 601.9407563209534
Result for _inner_e8deb_00075:
  auc: 0.9115293264389038
  date: 2021-03-19_13-26-40
  done: false
  experiment_id: c4ae7d57a0f3405392480b1518777f4b
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 48984
  time_since_restore: 2687.205946445465
  time_this_iter_s: 2687.205946445465
  time_total_s: 2687.205946445465
  timestamp: 1616156800
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00075
  
[2m[36m(pid=48984)[0m Finished run with seed 0 - lr 1 - sec_lr 0.1 - bs 32 - mean val auc: 0.9115293264389038
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 118/180 (1 PENDING, 26 RUNNING, 91 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    |                     |           32 |     0 | 0.01  |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |                     |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00075 | RUNNING    | 145.101.32.82:48984 |           32 |     0 | 1     |    0.1   |      1 |          2687.21 | 0.911529 |
| _inner_e8deb_00080 | RUNNING    |                     |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |                     |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |                     |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |                     |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00117 | PENDING    |                     |          128 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 98 more trials not shown (16 RUNNING, 81 TERMINATED)


Result for _inner_e8deb_00075:
  auc: 0.9115293264389038
  date: 2021-03-19_13-26-40
  done: true
  experiment_id: c4ae7d57a0f3405392480b1518777f4b
  experiment_tag: 75_batch_size=32,eta=0.0,lr=1,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 48984
  time_since_restore: 2687.205946445465
  time_this_iter_s: 2687.205946445465
  time_total_s: 2687.205946445465
  timestamp: 1616156800
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00075
  
[2m[36m(pid=26617)[0m Starting run with seed 0 - lr 5 - sec_lr 1 - bs 128
[2m[36m(pid=26617)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=26617)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=26617)[0m GPU available: False, used: False
[2m[36m(pid=26617)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26617)[0m 
[2m[36m(pid=26617)[0m   | Name      | Type              | Params
[2m[36m(pid=26617)[0m ------------------------------------------------
[2m[36m(pid=26617)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26617)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26617)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26617)[0m ------------------------------------------------
[2m[36m(pid=26617)[0m 8.7 K     Trainable params
[2m[36m(pid=26617)[0m 0         Non-trainable params
[2m[36m(pid=26617)[0m 8.7 K     Total params
[2m[36m(pid=26617)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=26617)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=26617)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=26617)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=26617)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=26617)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=21665)[0m time to fit was 87.64857053756714
[2m[36m(pid=21665)[0m GPU available: False, used: False
[2m[36m(pid=21665)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=21665)[0m 
[2m[36m(pid=21665)[0m   | Name      | Type              | Params
[2m[36m(pid=21665)[0m ------------------------------------------------
[2m[36m(pid=21665)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=21665)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=21665)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=21665)[0m ------------------------------------------------
[2m[36m(pid=21665)[0m 8.7 K     Trainable params
[2m[36m(pid=21665)[0m 0         Non-trainable params
[2m[36m(pid=21665)[0m 8.7 K     Total params
[2m[36m(pid=35848)[0m time to fit was 1343.852957725525
[2m[36m(pid=35848)[0m Finished run with seed 0 - lr 0.01 - sec_lr 0.001 - bs 32 - mean val auc: 0.9132655262947083
Result for _inner_e8deb_00005:
  auc: 0.9132655262947083
  date: 2021-03-19_13-27-31
  done: false
  experiment_id: 3192b1e434df4e2f928f5eae313fe287
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35848
  time_since_restore: 7539.400884151459
  time_this_iter_s: 7539.400884151459
  time_total_s: 7539.400884151459
  timestamp: 1616156851
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00005
  
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 119/180 (1 PENDING, 26 RUNNING, 92 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00005 | RUNNING    | 145.101.32.82:35848 |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |                     |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |                     |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |                     |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |                     |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |                     |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |                     |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00118 | PENDING    |                     |          256 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 99 more trials not shown (16 RUNNING, 82 TERMINATED)


Result for _inner_e8deb_00005:
  auc: 0.9132655262947083
  date: 2021-03-19_13-27-31
  done: true
  experiment_id: 3192b1e434df4e2f928f5eae313fe287
  experiment_tag: 5_batch_size=32,eta=0.0,lr=0.01,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35848
  time_since_restore: 7539.400884151459
  time_this_iter_s: 7539.400884151459
  time_total_s: 7539.400884151459
  timestamp: 1616156851
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00005
  
2021-03-19 13:27:33,159	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff0ca8593301000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=43563)[0m time to fit was 206.58604669570923
Result for _inner_e8deb_00101:
  auc: 0.909694504737854
  date: 2021-03-19_13-27-39
  done: false
  experiment_id: d29faf6b52e64c90bf3b69446ec4a719
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 43563
  time_since_restore: 1215.366515636444
  time_this_iter_s: 1215.366515636444
  time_total_s: 1215.366515636444
  timestamp: 1616156859
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00101
  
== Status ==
Memory usage on this node: 9.7/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 120/180 (1 PENDING, 26 RUNNING, 93 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |       |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00092 | RUNNING    |       |          128 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00119 | PENDING    |       |          512 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 100 more trials not shown (16 RUNNING, 83 TERMINATED)


Result for _inner_e8deb_00101:
  auc: 0.909694504737854
  date: 2021-03-19_13-27-39
  done: true
  experiment_id: d29faf6b52e64c90bf3b69446ec4a719
  experiment_tag: 101_batch_size=64,eta=0.0,lr=0.1,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 43563
  time_since_restore: 1215.366515636444
  time_this_iter_s: 1215.366515636444
  time_total_s: 1215.366515636444
  timestamp: 1616156859
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00101
  
[2m[36m(pid=43563)[0m Finished run with seed 0 - lr 0.1 - sec_lr 1 - bs 64 - mean val auc: 0.909694504737854
[2m[36m(pid=27924)[0m Starting run with seed 0 - lr 5 - sec_lr 1 - bs 256
[2m[36m(pid=27924)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=27924)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=27924)[0m GPU available: False, used: False
[2m[36m(pid=27924)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27924)[0m 
[2m[36m(pid=27924)[0m   | Name      | Type              | Params
[2m[36m(pid=27924)[0m ------------------------------------------------
[2m[36m(pid=27924)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27924)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27924)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27924)[0m ------------------------------------------------
[2m[36m(pid=27924)[0m 8.7 K     Trainable params
[2m[36m(pid=27924)[0m 0         Non-trainable params
[2m[36m(pid=27924)[0m 8.7 K     Total params
[2m[36m(pid=27924)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=27924)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=27924)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=27924)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=27924)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=27924)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=28179)[0m Starting run with seed 0 - lr 5 - sec_lr 1 - bs 512
[2m[36m(pid=28179)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=28179)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=28179)[0m GPU available: False, used: False
[2m[36m(pid=28179)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=28179)[0m 
[2m[36m(pid=28179)[0m   | Name      | Type              | Params
[2m[36m(pid=28179)[0m ------------------------------------------------
[2m[36m(pid=28179)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=28179)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=28179)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=28179)[0m ------------------------------------------------
[2m[36m(pid=28179)[0m 8.7 K     Trainable params
[2m[36m(pid=28179)[0m 0         Non-trainable params
[2m[36m(pid=28179)[0m 8.7 K     Total params
[2m[36m(pid=28179)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=28179)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=28179)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=28179)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=28179)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=28179)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=29675)[0m time to fit was 601.5736112594604
[2m[36m(pid=29675)[0m GPU available: False, used: False
[2m[36m(pid=29675)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29675)[0m 
[2m[36m(pid=29675)[0m   | Name      | Type              | Params
[2m[36m(pid=29675)[0m ------------------------------------------------
[2m[36m(pid=29675)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29675)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29675)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29675)[0m ------------------------------------------------
[2m[36m(pid=29675)[0m 8.7 K     Trainable params
[2m[36m(pid=29675)[0m 0         Non-trainable params
[2m[36m(pid=29675)[0m 8.7 K     Total params
[2m[36m(pid=21665)[0m time to fit was 65.4296190738678
[2m[36m(pid=21665)[0m GPU available: False, used: False
[2m[36m(pid=21665)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=21665)[0m 
[2m[36m(pid=21665)[0m   | Name      | Type              | Params
[2m[36m(pid=21665)[0m ------------------------------------------------
[2m[36m(pid=21665)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=21665)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=21665)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=21665)[0m ------------------------------------------------
[2m[36m(pid=21665)[0m 8.7 K     Trainable params
[2m[36m(pid=21665)[0m 0         Non-trainable params
[2m[36m(pid=21665)[0m 8.7 K     Total params
[2m[36m(pid=14376)[0m time to fit was 144.89008331298828
[2m[36m(pid=14376)[0m GPU available: False, used: False
[2m[36m(pid=14376)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=14376)[0m 
[2m[36m(pid=14376)[0m   | Name      | Type              | Params
[2m[36m(pid=14376)[0m ------------------------------------------------
[2m[36m(pid=14376)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=14376)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=14376)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=14376)[0m ------------------------------------------------
[2m[36m(pid=14376)[0m 8.7 K     Trainable params
[2m[36m(pid=14376)[0m 0         Non-trainable params
[2m[36m(pid=14376)[0m 8.7 K     Total params
[2m[36m(pid=8453)[0m time to fit was 134.41459321975708
[2m[36m(pid=8453)[0m Finished run with seed 0 - lr 1 - sec_lr 1 - bs 128 - mean val auc: 0.9104915857315063
Result for _inner_e8deb_00107:
  auc: 0.9104915857315063
  date: 2021-03-19_13-28-28
  done: false
  experiment_id: 937e251e6c7845868a6a2f3153340690
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8453
  time_since_restore: 717.2660682201385
  time_this_iter_s: 717.2660682201385
  time_total_s: 717.2660682201385
  timestamp: 1616156908
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00107
  
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 121/180 (1 PENDING, 26 RUNNING, 94 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |       |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00092 | RUNNING    |       |          128 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00120 | PENDING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 101 more trials not shown (16 RUNNING, 84 TERMINATED)


Result for _inner_e8deb_00107:
  auc: 0.9104915857315063
  date: 2021-03-19_13-28-28
  done: true
  experiment_id: 937e251e6c7845868a6a2f3153340690
  experiment_tag: 107_batch_size=128,eta=0.0,lr=1,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8453
  time_since_restore: 717.2660682201385
  time_this_iter_s: 717.2660682201385
  time_total_s: 717.2660682201385
  timestamp: 1616156908
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00107
  
[2m[36m(pid=28179)[0m time to fit was 44.015849351882935
[2m[36m(pid=28179)[0m GPU available: False, used: False
[2m[36m(pid=28179)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=28179)[0m 
[2m[36m(pid=28179)[0m   | Name      | Type              | Params
[2m[36m(pid=28179)[0m ------------------------------------------------
[2m[36m(pid=28179)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=28179)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=28179)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=28179)[0m ------------------------------------------------
[2m[36m(pid=28179)[0m 8.7 K     Trainable params
[2m[36m(pid=28179)[0m 0         Non-trainable params
[2m[36m(pid=28179)[0m 8.7 K     Total params
[2m[36m(pid=29801)[0m Starting run with seed 0 - lr 0.001 - sec_lr 2 - bs 32
[2m[36m(pid=29801)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=29801)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=29801)[0m GPU available: False, used: False
[2m[36m(pid=29801)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29801)[0m 
[2m[36m(pid=29801)[0m   | Name      | Type              | Params
[2m[36m(pid=29801)[0m ------------------------------------------------
[2m[36m(pid=29801)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29801)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29801)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29801)[0m ------------------------------------------------
[2m[36m(pid=29801)[0m 8.7 K     Trainable params
[2m[36m(pid=29801)[0m 0         Non-trainable params
[2m[36m(pid=29801)[0m 8.7 K     Total params
[2m[36m(pid=29801)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=29801)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=29801)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=29801)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=29801)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=29801)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=26617)[0m time to fit was 113.15831971168518
[2m[36m(pid=26617)[0m GPU available: False, used: False
[2m[36m(pid=26617)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26617)[0m 
[2m[36m(pid=26617)[0m   | Name      | Type              | Params
[2m[36m(pid=26617)[0m ------------------------------------------------
[2m[36m(pid=26617)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26617)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26617)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26617)[0m ------------------------------------------------
[2m[36m(pid=26617)[0m 8.7 K     Trainable params
[2m[36m(pid=26617)[0m 0         Non-trainable params
[2m[36m(pid=26617)[0m 8.7 K     Total params
[2m[36m(pid=15746)[0m time to fit was 200.0570707321167
[2m[36m(pid=15746)[0m GPU available: False, used: False
[2m[36m(pid=15746)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=15746)[0m 
[2m[36m(pid=15746)[0m   | Name      | Type              | Params
[2m[36m(pid=15746)[0m ------------------------------------------------
[2m[36m(pid=15746)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=15746)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=15746)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=15746)[0m ------------------------------------------------
[2m[36m(pid=15746)[0m 8.7 K     Trainable params
[2m[36m(pid=15746)[0m 0         Non-trainable params
[2m[36m(pid=15746)[0m 8.7 K     Total params
[2m[36m(pid=27924)[0m time to fit was 71.50785946846008
[2m[36m(pid=27924)[0m GPU available: False, used: False
[2m[36m(pid=27924)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27924)[0m 
[2m[36m(pid=27924)[0m   | Name      | Type              | Params
[2m[36m(pid=27924)[0m ------------------------------------------------
[2m[36m(pid=27924)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27924)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27924)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27924)[0m ------------------------------------------------
[2m[36m(pid=27924)[0m 8.7 K     Trainable params
[2m[36m(pid=27924)[0m 0         Non-trainable params
[2m[36m(pid=27924)[0m 8.7 K     Total params
[2m[36m(pid=21665)[0m time to fit was 58.55395698547363
Result for _inner_e8deb_00114:
  auc: 0.8290156483650207
  date: 2021-03-19_13-29-09
  done: false
  experiment_id: 5c2becb4d35c43afaee418a9fede4c81
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 21665
  time_since_restore: 313.8270092010498
  time_this_iter_s: 313.8270092010498
  time_total_s: 313.8270092010498
  timestamp: 1616156949
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00114
  
[2m[36m(pid=21665)[0m Finished run with seed 0 - lr 2 - sec_lr 1 - bs 512 - mean val auc: 0.8290156483650207
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 122/180 (1 PENDING, 26 RUNNING, 95 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |       |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00092 | RUNNING    |       |          128 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00121 | PENDING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 102 more trials not shown (16 RUNNING, 85 TERMINATED)


Result for _inner_e8deb_00114:
  auc: 0.8290156483650207
  date: 2021-03-19_13-29-09
  done: true
  experiment_id: 5c2becb4d35c43afaee418a9fede4c81
  experiment_tag: 114_batch_size=512,eta=0.0,lr=2,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 21665
  time_since_restore: 313.8270092010498
  time_this_iter_s: 313.8270092010498
  time_total_s: 313.8270092010498
  timestamp: 1616156949
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00114
  
[2m[36m(pid=31106)[0m Starting run with seed 0 - lr 0.001 - sec_lr 2 - bs 64
[2m[36m(pid=31106)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=31106)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=31106)[0m GPU available: False, used: False
[2m[36m(pid=31106)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31106)[0m 
[2m[36m(pid=31106)[0m   | Name      | Type              | Params
[2m[36m(pid=31106)[0m ------------------------------------------------
[2m[36m(pid=31106)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31106)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31106)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31106)[0m ------------------------------------------------
[2m[36m(pid=31106)[0m 8.7 K     Trainable params
[2m[36m(pid=31106)[0m 0         Non-trainable params
[2m[36m(pid=31106)[0m 8.7 K     Total params
[2m[36m(pid=31106)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=31106)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31106)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=31106)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31106)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=31106)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=5013)[0m time to fit was 563.1481664180756
[2m[36m(pid=5013)[0m GPU available: False, used: False
[2m[36m(pid=5013)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=5013)[0m 
[2m[36m(pid=5013)[0m   | Name      | Type              | Params
[2m[36m(pid=5013)[0m ------------------------------------------------
[2m[36m(pid=5013)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=5013)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=5013)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=5013)[0m ------------------------------------------------
[2m[36m(pid=5013)[0m 8.7 K     Trainable params
[2m[36m(pid=5013)[0m 0         Non-trainable params
[2m[36m(pid=5013)[0m 8.7 K     Total params
[2m[36m(pid=25102)[0m time to fit was 460.4954946041107
[2m[36m(pid=25102)[0m GPU available: False, used: False
[2m[36m(pid=25102)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=25102)[0m 
[2m[36m(pid=25102)[0m   | Name      | Type              | Params
[2m[36m(pid=25102)[0m ------------------------------------------------
[2m[36m(pid=25102)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=25102)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=25102)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=25102)[0m ------------------------------------------------
[2m[36m(pid=25102)[0m 8.7 K     Trainable params
[2m[36m(pid=25102)[0m 0         Non-trainable params
[2m[36m(pid=25102)[0m 8.7 K     Total params
[2m[36m(pid=25581)[0m time to fit was 205.18775343894958
[2m[36m(pid=25581)[0m GPU available: False, used: False
[2m[36m(pid=25581)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=25581)[0m 
[2m[36m(pid=25581)[0m   | Name      | Type              | Params
[2m[36m(pid=25581)[0m ------------------------------------------------
[2m[36m(pid=25581)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=25581)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=25581)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=25581)[0m ------------------------------------------------
[2m[36m(pid=25581)[0m 8.7 K     Trainable params
[2m[36m(pid=25581)[0m 0         Non-trainable params
[2m[36m(pid=25581)[0m 8.7 K     Total params
[2m[36m(pid=8415)[0m time to fit was 241.0083794593811
[2m[36m(pid=8415)[0m GPU available: False, used: False
[2m[36m(pid=8415)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8415)[0m 
[2m[36m(pid=8415)[0m   | Name      | Type              | Params
[2m[36m(pid=8415)[0m ------------------------------------------------
[2m[36m(pid=8415)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8415)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8415)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8415)[0m ------------------------------------------------
[2m[36m(pid=8415)[0m 8.7 K     Trainable params
[2m[36m(pid=8415)[0m 0         Non-trainable params
[2m[36m(pid=8415)[0m 8.7 K     Total params
[2m[36m(pid=27924)[0m time to fit was 71.45198512077332
[2m[36m(pid=27924)[0m GPU available: False, used: False
[2m[36m(pid=27924)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27924)[0m 
[2m[36m(pid=27924)[0m   | Name      | Type              | Params
[2m[36m(pid=27924)[0m ------------------------------------------------
[2m[36m(pid=27924)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27924)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27924)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27924)[0m ------------------------------------------------
[2m[36m(pid=27924)[0m 8.7 K     Trainable params
[2m[36m(pid=27924)[0m 0         Non-trainable params
[2m[36m(pid=27924)[0m 8.7 K     Total params
[2m[36m(pid=3723)[0m time to fit was 563.2303404808044
[2m[36m(pid=3723)[0m GPU available: False, used: False
[2m[36m(pid=3723)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=3723)[0m 
[2m[36m(pid=3723)[0m   | Name      | Type              | Params
[2m[36m(pid=3723)[0m ------------------------------------------------
[2m[36m(pid=3723)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=3723)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=3723)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=3723)[0m ------------------------------------------------
[2m[36m(pid=3723)[0m 8.7 K     Trainable params
[2m[36m(pid=3723)[0m 0         Non-trainable params
[2m[36m(pid=3723)[0m 8.7 K     Total params
[2m[36m(pid=34531)[0m time to fit was 449.8414309024811
[2m[36m(pid=34531)[0m GPU available: False, used: False
[2m[36m(pid=34531)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=34531)[0m 
[2m[36m(pid=34531)[0m   | Name      | Type              | Params
[2m[36m(pid=34531)[0m ------------------------------------------------
[2m[36m(pid=34531)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=34531)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=34531)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=34531)[0m ------------------------------------------------
[2m[36m(pid=34531)[0m 8.7 K     Trainable params
[2m[36m(pid=34531)[0m 0         Non-trainable params
[2m[36m(pid=34531)[0m 8.7 K     Total params
[2m[36m(pid=26617)[0m time to fit was 122.00665283203125
[2m[36m(pid=26617)[0m GPU available: False, used: False
[2m[36m(pid=26617)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26617)[0m 
[2m[36m(pid=26617)[0m   | Name      | Type              | Params
[2m[36m(pid=26617)[0m ------------------------------------------------
[2m[36m(pid=26617)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26617)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26617)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26617)[0m ------------------------------------------------
[2m[36m(pid=26617)[0m 8.7 K     Trainable params
[2m[36m(pid=26617)[0m 0         Non-trainable params
[2m[36m(pid=26617)[0m 8.7 K     Total params
[2m[36m(pid=16527)[0m time to fit was 745.7210462093353
[2m[36m(pid=16527)[0m GPU available: False, used: False
[2m[36m(pid=16527)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=16527)[0m 
[2m[36m(pid=16527)[0m   | Name      | Type              | Params
[2m[36m(pid=16527)[0m ------------------------------------------------
[2m[36m(pid=16527)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=16527)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=16527)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=16527)[0m ------------------------------------------------
[2m[36m(pid=16527)[0m 8.7 K     Trainable params
[2m[36m(pid=16527)[0m 0         Non-trainable params
[2m[36m(pid=16527)[0m 8.7 K     Total params
[2m[36m(pid=8460)[0m time to fit was 316.8636543750763
[2m[36m(pid=8460)[0m GPU available: False, used: False
[2m[36m(pid=8460)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8460)[0m 
[2m[36m(pid=8460)[0m   | Name      | Type              | Params
[2m[36m(pid=8460)[0m ------------------------------------------------
[2m[36m(pid=8460)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8460)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8460)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8460)[0m ------------------------------------------------
[2m[36m(pid=8460)[0m 8.7 K     Trainable params
[2m[36m(pid=8460)[0m 0         Non-trainable params
[2m[36m(pid=8460)[0m 8.7 K     Total params
[2m[36m(pid=41401)[0m time to fit was 460.3724772930145
[2m[36m(pid=41401)[0m GPU available: False, used: False
[2m[36m(pid=41401)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=41401)[0m 
[2m[36m(pid=41401)[0m   | Name      | Type              | Params
[2m[36m(pid=41401)[0m ------------------------------------------------
[2m[36m(pid=41401)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=41401)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=41401)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=41401)[0m ------------------------------------------------
[2m[36m(pid=41401)[0m 8.7 K     Trainable params
[2m[36m(pid=41401)[0m 0         Non-trainable params
[2m[36m(pid=41401)[0m 8.7 K     Total params
[2m[36m(pid=15746)[0m time to fit was 139.56607031822205
Result for _inner_e8deb_00113:
  auc: 0.9078212261199952
  date: 2021-03-19_13-31-12
  done: false
  experiment_id: aa3ec1bad7b748e4927a97691af62f90
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 15746
  time_since_restore: 640.0997083187103
  time_this_iter_s: 640.0997083187103
  time_total_s: 640.0997083187103
  timestamp: 1616157072
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00113
  
[2m[36m(pid=15746)[0m Finished run with seed 0 - lr 2 - sec_lr 1 - bs 256 - mean val auc: 0.9078212261199952
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 123/180 (1 PENDING, 26 RUNNING, 96 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |       |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00092 | RUNNING    |       |          128 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00122 | PENDING    |       |          128 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 103 more trials not shown (16 RUNNING, 86 TERMINATED)


Result for _inner_e8deb_00113:
  auc: 0.9078212261199952
  date: 2021-03-19_13-31-12
  done: true
  experiment_id: aa3ec1bad7b748e4927a97691af62f90
  experiment_tag: 113_batch_size=256,eta=0.0,lr=2,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 15746
  time_since_restore: 640.0997083187103
  time_this_iter_s: 640.0997083187103
  time_total_s: 640.0997083187103
  timestamp: 1616157072
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00113
  
2021-03-19 13:31:13,748	WARNING worker.py:1034 -- The actor or task with ID fffffffffffffffff6a462c501000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=34577)[0m Starting run with seed 0 - lr 0.001 - sec_lr 2 - bs 128
[2m[36m(pid=34577)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=34577)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=34577)[0m GPU available: False, used: False
[2m[36m(pid=34577)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=34577)[0m 
[2m[36m(pid=34577)[0m   | Name      | Type              | Params
[2m[36m(pid=34577)[0m ------------------------------------------------
[2m[36m(pid=34577)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=34577)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=34577)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=34577)[0m ------------------------------------------------
[2m[36m(pid=34577)[0m 8.7 K     Trainable params
[2m[36m(pid=34577)[0m 0         Non-trainable params
[2m[36m(pid=34577)[0m 8.7 K     Total params
[2m[36m(pid=34577)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=34577)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=34577)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=34577)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=34577)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=34577)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23636)[0m time to fit was 389.6148579120636
[2m[36m(pid=23636)[0m GPU available: False, used: False
[2m[36m(pid=23636)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23636)[0m 
[2m[36m(pid=23636)[0m   | Name      | Type              | Params
[2m[36m(pid=23636)[0m ------------------------------------------------
[2m[36m(pid=23636)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23636)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23636)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23636)[0m ------------------------------------------------
[2m[36m(pid=23636)[0m 8.7 K     Trainable params
[2m[36m(pid=23636)[0m 0         Non-trainable params
[2m[36m(pid=23636)[0m 8.7 K     Total params
[2m[36m(pid=28179)[0m time to fit was 187.54835534095764
[2m[36m(pid=28179)[0m GPU available: False, used: False
[2m[36m(pid=28179)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=28179)[0m 
[2m[36m(pid=28179)[0m   | Name      | Type              | Params
[2m[36m(pid=28179)[0m ------------------------------------------------
[2m[36m(pid=28179)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=28179)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=28179)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=28179)[0m ------------------------------------------------
[2m[36m(pid=28179)[0m 8.7 K     Trainable params
[2m[36m(pid=28179)[0m 0         Non-trainable params
[2m[36m(pid=28179)[0m 8.7 K     Total params
[2m[36m(pid=14376)[0m time to fit was 238.3653588294983
[2m[36m(pid=14376)[0m GPU available: False, used: False
[2m[36m(pid=14376)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=14376)[0m 
[2m[36m(pid=14376)[0m   | Name      | Type              | Params
[2m[36m(pid=14376)[0m ------------------------------------------------
[2m[36m(pid=14376)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=14376)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=14376)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=14376)[0m ------------------------------------------------
[2m[36m(pid=14376)[0m 8.7 K     Trainable params
[2m[36m(pid=14376)[0m 0         Non-trainable params
[2m[36m(pid=14376)[0m 8.7 K     Total params
[2m[36m(pid=28179)[0m time to fit was 41.89807891845703
[2m[36m(pid=28179)[0m GPU available: False, used: False
[2m[36m(pid=28179)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=28179)[0m 
[2m[36m(pid=28179)[0m   | Name      | Type              | Params
[2m[36m(pid=28179)[0m ------------------------------------------------
[2m[36m(pid=28179)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=28179)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=28179)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=28179)[0m ------------------------------------------------
[2m[36m(pid=28179)[0m 8.7 K     Trainable params
[2m[36m(pid=28179)[0m 0         Non-trainable params
[2m[36m(pid=28179)[0m 8.7 K     Total params
[2m[36m(pid=25581)[0m time to fit was 204.42421889305115
[2m[36m(pid=25581)[0m GPU available: False, used: False
[2m[36m(pid=25581)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=25581)[0m 
[2m[36m(pid=25581)[0m   | Name      | Type              | Params
[2m[36m(pid=25581)[0m ------------------------------------------------
[2m[36m(pid=25581)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=25581)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=25581)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=25581)[0m ------------------------------------------------
[2m[36m(pid=25581)[0m 8.7 K     Trainable params
[2m[36m(pid=25581)[0m 0         Non-trainable params
[2m[36m(pid=25581)[0m 8.7 K     Total params
[2m[36m(pid=27849)[0m time to fit was 468.3536911010742
[2m[36m(pid=27849)[0m GPU available: False, used: False
[2m[36m(pid=27849)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27849)[0m 
[2m[36m(pid=27849)[0m   | Name      | Type              | Params
[2m[36m(pid=27849)[0m ------------------------------------------------
[2m[36m(pid=27849)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27849)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27849)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27849)[0m ------------------------------------------------
[2m[36m(pid=27849)[0m 8.7 K     Trainable params
[2m[36m(pid=27849)[0m 0         Non-trainable params
[2m[36m(pid=27849)[0m 8.7 K     Total params
[2m[36m(pid=35838)[0m time to fit was 1759.7389991283417
[2m[36m(pid=35838)[0m GPU available: False, used: False
[2m[36m(pid=35838)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35838)[0m 
[2m[36m(pid=35838)[0m   | Name      | Type              | Params
[2m[36m(pid=35838)[0m ------------------------------------------------
[2m[36m(pid=35838)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35838)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35838)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35838)[0m ------------------------------------------------
[2m[36m(pid=35838)[0m 8.7 K     Trainable params
[2m[36m(pid=35838)[0m 0         Non-trainable params
[2m[36m(pid=35838)[0m 8.7 K     Total params
[2m[36m(pid=14376)[0m time to fit was 133.3888852596283
[2m[36m(pid=14376)[0m Finished run with seed 0 - lr 2 - sec_lr 1 - bs 128 - mean val auc: 0.9069811105728149
Result for _inner_e8deb_00112:
  auc: 0.9069811105728149
  date: 2021-03-19_13-34-28
  done: false
  experiment_id: 4408a7042aae4837836cecb78c4c8c76
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 14376
  time_since_restore: 885.0257451534271
  time_this_iter_s: 885.0257451534271
  time_total_s: 885.0257451534271
  timestamp: 1616157268
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00112
  
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 124/180 (1 PENDING, 26 RUNNING, 97 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |       |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00092 | RUNNING    |       |          128 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00123 | PENDING    |       |          256 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 104 more trials not shown (16 RUNNING, 87 TERMINATED)


Result for _inner_e8deb_00112:
  auc: 0.9069811105728149
  date: 2021-03-19_13-34-28
  done: true
  experiment_id: 4408a7042aae4837836cecb78c4c8c76
  experiment_tag: 112_batch_size=128,eta=0.0,lr=2,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 14376
  time_since_restore: 885.0257451534271
  time_this_iter_s: 885.0257451534271
  time_total_s: 885.0257451534271
  timestamp: 1616157268
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00112
  
[2m[36m(pid=40014)[0m Starting run with seed 0 - lr 0.001 - sec_lr 2 - bs 256
[2m[36m(pid=40014)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=40014)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=40014)[0m GPU available: False, used: False
[2m[36m(pid=40014)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40014)[0m 
[2m[36m(pid=40014)[0m   | Name      | Type              | Params
[2m[36m(pid=40014)[0m ------------------------------------------------
[2m[36m(pid=40014)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40014)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40014)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40014)[0m ------------------------------------------------
[2m[36m(pid=40014)[0m 8.7 K     Trainable params
[2m[36m(pid=40014)[0m 0         Non-trainable params
[2m[36m(pid=40014)[0m 8.7 K     Total params
[2m[36m(pid=40014)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=40014)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=40014)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=40014)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=40014)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=40014)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=27924)[0m time to fit was 313.7839443683624
[2m[36m(pid=27924)[0m GPU available: False, used: False
[2m[36m(pid=27924)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27924)[0m 
[2m[36m(pid=27924)[0m   | Name      | Type              | Params
[2m[36m(pid=27924)[0m ------------------------------------------------
[2m[36m(pid=27924)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27924)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27924)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27924)[0m ------------------------------------------------
[2m[36m(pid=27924)[0m 8.7 K     Trainable params
[2m[36m(pid=27924)[0m 0         Non-trainable params
[2m[36m(pid=27924)[0m 8.7 K     Total params
[2m[36m(pid=8460)[0m time to fit was 259.8396553993225
[2m[36m(pid=8460)[0m GPU available: False, used: False
[2m[36m(pid=8460)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8460)[0m 
[2m[36m(pid=8460)[0m   | Name      | Type              | Params
[2m[36m(pid=8460)[0m ------------------------------------------------
[2m[36m(pid=8460)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8460)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8460)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8460)[0m ------------------------------------------------
[2m[36m(pid=8460)[0m 8.7 K     Trainable params
[2m[36m(pid=8460)[0m 0         Non-trainable params
[2m[36m(pid=8460)[0m 8.7 K     Total params
[2m[36m(pid=8415)[0m time to fit was 336.5158340930939
[2m[36m(pid=8415)[0m GPU available: False, used: False
[2m[36m(pid=8415)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8415)[0m 
[2m[36m(pid=8415)[0m   | Name      | Type              | Params
[2m[36m(pid=8415)[0m ------------------------------------------------
[2m[36m(pid=8415)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8415)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8415)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8415)[0m ------------------------------------------------
[2m[36m(pid=8415)[0m 8.7 K     Trainable params
[2m[36m(pid=8415)[0m 0         Non-trainable params
[2m[36m(pid=8415)[0m 8.7 K     Total params
[2m[36m(pid=8457)[0m time to fit was 1021.3634746074677
[2m[36m(pid=29675)[0m time to fit was 460.13099694252014
[2m[36m(pid=8457)[0m GPU available: False, used: False
[2m[36m(pid=8457)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8457)[0m 
[2m[36m(pid=8457)[0m   | Name      | Type              | Params
[2m[36m(pid=8457)[0m ------------------------------------------------
[2m[36m(pid=8457)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8457)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8457)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8457)[0m ------------------------------------------------
[2m[36m(pid=8457)[0m 8.7 K     Trainable params
[2m[36m(pid=8457)[0m 0         Non-trainable params
[2m[36m(pid=8457)[0m 8.7 K     Total params
[2m[36m(pid=29675)[0m GPU available: False, used: False
[2m[36m(pid=29675)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29675)[0m 
[2m[36m(pid=29675)[0m   | Name      | Type              | Params
[2m[36m(pid=29675)[0m ------------------------------------------------
[2m[36m(pid=29675)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29675)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29675)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29675)[0m ------------------------------------------------
[2m[36m(pid=29675)[0m 8.7 K     Trainable params
[2m[36m(pid=29675)[0m 0         Non-trainable params
[2m[36m(pid=29675)[0m 8.7 K     Total params
[2m[36m(pid=40014)[0m time to fit was 82.26042008399963
[2m[36m(pid=40014)[0m GPU available: False, used: False
[2m[36m(pid=40014)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40014)[0m 
[2m[36m(pid=40014)[0m   | Name      | Type              | Params
[2m[36m(pid=40014)[0m ------------------------------------------------
[2m[36m(pid=40014)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40014)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40014)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40014)[0m ------------------------------------------------
[2m[36m(pid=40014)[0m 8.7 K     Trainable params
[2m[36m(pid=40014)[0m 0         Non-trainable params
[2m[36m(pid=40014)[0m 8.7 K     Total params
[2m[36m(pid=28179)[0m time to fit was 218.20713448524475
[2m[36m(pid=28179)[0m GPU available: False, used: False
[2m[36m(pid=28179)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=28179)[0m 
[2m[36m(pid=28179)[0m   | Name      | Type              | Params
[2m[36m(pid=28179)[0m ------------------------------------------------
[2m[36m(pid=28179)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=28179)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=28179)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=28179)[0m ------------------------------------------------
[2m[36m(pid=28179)[0m 8.7 K     Trainable params
[2m[36m(pid=28179)[0m 0         Non-trainable params
[2m[36m(pid=28179)[0m 8.7 K     Total params
[2m[36m(pid=29801)[0m time to fit was 488.4449317455292
[2m[36m(pid=29801)[0m GPU available: False, used: False
[2m[36m(pid=29801)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29801)[0m 
[2m[36m(pid=29801)[0m   | Name      | Type              | Params
[2m[36m(pid=29801)[0m ------------------------------------------------
[2m[36m(pid=29801)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29801)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29801)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29801)[0m ------------------------------------------------
[2m[36m(pid=29801)[0m 8.7 K     Trainable params
[2m[36m(pid=29801)[0m 0         Non-trainable params
[2m[36m(pid=29801)[0m 8.7 K     Total params
[2m[36m(pid=26407)[0m time to fit was 931.5889699459076
[2m[36m(pid=26407)[0m GPU available: False, used: False
[2m[36m(pid=26407)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26407)[0m 
[2m[36m(pid=26407)[0m   | Name      | Type              | Params
[2m[36m(pid=26407)[0m ------------------------------------------------
[2m[36m(pid=26407)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26407)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26407)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26407)[0m ------------------------------------------------
[2m[36m(pid=26407)[0m 8.7 K     Trainable params
[2m[36m(pid=26407)[0m 0         Non-trainable params
[2m[36m(pid=26407)[0m 8.7 K     Total params
[2m[36m(pid=34531)[0m time to fit was 374.9215188026428
Result for _inner_e8deb_00096:
  auc: 0.9100831985473633
  date: 2021-03-19_13-36-56
  done: false
  experiment_id: 9aa8b1112371458fa71df90a77200628
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 34531
  time_since_restore: 2045.8013796806335
  time_this_iter_s: 2045.8013796806335
  time_total_s: 2045.8013796806335
  timestamp: 1616157416
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00096
  
[2m[36m(pid=34531)[0m Finished run with seed 0 - lr 0.01 - sec_lr 1 - bs 64 - mean val auc: 0.9100831985473633
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 125/180 (1 PENDING, 26 RUNNING, 98 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    |       |           32 |     0 | 2     |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00092 | RUNNING    |       |          128 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00124 | PENDING    |       |          512 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 105 more trials not shown (16 RUNNING, 88 TERMINATED)


Result for _inner_e8deb_00096:
  auc: 0.9100831985473633
  date: 2021-03-19_13-36-56
  done: true
  experiment_id: 9aa8b1112371458fa71df90a77200628
  experiment_tag: 96_batch_size=64,eta=0.0,lr=0.01,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 34531
  time_since_restore: 2045.8013796806335
  time_this_iter_s: 2045.8013796806335
  time_total_s: 2045.8013796806335
  timestamp: 1616157416
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00096
  
[2m[36m(pid=44479)[0m Starting run with seed 0 - lr 0.001 - sec_lr 2 - bs 512
[2m[36m(pid=44479)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=44479)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=44479)[0m GPU available: False, used: False
[2m[36m(pid=44479)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=44479)[0m 
[2m[36m(pid=44479)[0m   | Name      | Type              | Params
[2m[36m(pid=44479)[0m ------------------------------------------------
[2m[36m(pid=44479)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=44479)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=44479)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=44479)[0m ------------------------------------------------
[2m[36m(pid=44479)[0m 8.7 K     Trainable params
[2m[36m(pid=44479)[0m 0         Non-trainable params
[2m[36m(pid=44479)[0m 8.7 K     Total params
[2m[36m(pid=44479)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=44479)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=44479)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=44479)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=44479)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=44479)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=41401)[0m time to fit was 422.34654355049133
[2m[36m(pid=41401)[0m GPU available: False, used: False
[2m[36m(pid=41401)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=41401)[0m 
[2m[36m(pid=41401)[0m   | Name      | Type              | Params
[2m[36m(pid=41401)[0m ------------------------------------------------
[2m[36m(pid=41401)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=41401)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=41401)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=41401)[0m ------------------------------------------------
[2m[36m(pid=41401)[0m 8.7 K     Trainable params
[2m[36m(pid=41401)[0m 0         Non-trainable params
[2m[36m(pid=41401)[0m 8.7 K     Total params
[2m[36m(pid=27924)[0m time to fit was 188.86611437797546
[2m[36m(pid=27924)[0m GPU available: False, used: False
[2m[36m(pid=27924)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27924)[0m 
[2m[36m(pid=27924)[0m   | Name      | Type              | Params
[2m[36m(pid=27924)[0m ------------------------------------------------
[2m[36m(pid=27924)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27924)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27924)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27924)[0m ------------------------------------------------
[2m[36m(pid=27924)[0m 8.7 K     Trainable params
[2m[36m(pid=27924)[0m 0         Non-trainable params
[2m[36m(pid=27924)[0m 8.7 K     Total params
[2m[36m(pid=8460)[0m time to fit was 208.28296446800232
[2m[36m(pid=8460)[0m GPU available: False, used: False
[2m[36m(pid=8460)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8460)[0m 
[2m[36m(pid=8460)[0m   | Name      | Type              | Params
[2m[36m(pid=8460)[0m ------------------------------------------------
[2m[36m(pid=8460)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8460)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8460)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8460)[0m ------------------------------------------------
[2m[36m(pid=8460)[0m 8.7 K     Trainable params
[2m[36m(pid=8460)[0m 0         Non-trainable params
[2m[36m(pid=8460)[0m 8.7 K     Total params
[2m[36m(pid=5013)[0m time to fit was 564.8466787338257
Result for _inner_e8deb_00080:
  auc: 0.9051255226135254
  date: 2021-03-19_13-38-51
  done: false
  experiment_id: fbdfb3b7426741be94d7f2d873a256c8
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 5013
  time_since_restore: 3150.5327298641205
  time_this_iter_s: 3150.5327298641205
  time_total_s: 3150.5327298641205
  timestamp: 1616157531
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00080
  
[2m[36m(pid=5013)[0m Finished run with seed 0 - lr 2 - sec_lr 0.1 - bs 32 - mean val auc: 0.9051255226135254
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 126/180 (1 PENDING, 26 RUNNING, 99 TERMINATED)
+--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                    |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                    |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                    |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |                    |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00080 | RUNNING    | 145.101.32.82:5013 |           32 |     0 | 2     |    0.1   |      1 |          3150.53 | 0.905126 |
| _inner_e8deb_00085 | RUNNING    |                    |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |                    |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |                    |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |                    |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00092 | RUNNING    |                    |          128 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00125 | PENDING    |                    |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                    |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                    |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                    |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                    |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                    |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                    |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                    |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                    |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                    |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                    |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 106 more trials not shown (16 RUNNING, 89 TERMINATED)


Result for _inner_e8deb_00080:
  auc: 0.9051255226135254
  date: 2021-03-19_13-38-51
  done: true
  experiment_id: fbdfb3b7426741be94d7f2d873a256c8
  experiment_tag: 80_batch_size=32,eta=0.0,lr=2,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 5013
  time_since_restore: 3150.5327298641205
  time_this_iter_s: 3150.5327298641205
  time_total_s: 3150.5327298641205
  timestamp: 1616157531
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00080
  
[2m[36m(pid=47886)[0m Starting run with seed 0 - lr 0.01 - sec_lr 2 - bs 32
[2m[36m(pid=47886)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=47886)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=47886)[0m GPU available: False, used: False
[2m[36m(pid=47886)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=47886)[0m 
[2m[36m(pid=47886)[0m   | Name      | Type              | Params
[2m[36m(pid=47886)[0m ------------------------------------------------
[2m[36m(pid=47886)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=47886)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=47886)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=47886)[0m ------------------------------------------------
[2m[36m(pid=47886)[0m 8.7 K     Trainable params
[2m[36m(pid=47886)[0m 0         Non-trainable params
[2m[36m(pid=47886)[0m 8.7 K     Total params
[2m[36m(pid=47886)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=47886)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=47886)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=47886)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=47886)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=47886)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=28179)[0m time to fit was 197.303231716156
Result for _inner_e8deb_00119:
  auc: 0.6313942432403564
  date: 2021-03-19_13-39-20
  done: false
  experiment_id: e6cfb170a1f0414385ba798e6a979d7e
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 28179
  time_since_restore: 690.1514139175415
  time_this_iter_s: 690.1514139175415
  time_total_s: 690.1514139175415
  timestamp: 1616157560
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00119
  
[2m[36m(pid=28179)[0m Finished run with seed 0 - lr 5 - sec_lr 1 - bs 512 - mean val auc: 0.6313942432403564
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 127/180 (1 PENDING, 26 RUNNING, 100 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00061 | RUNNING    |       |           64 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00092 | RUNNING    |       |          128 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00095 | RUNNING    |       |           32 |     0 | 0.01  |    1     |        |                  |          |
| _inner_e8deb_00126 | PENDING    |       |           64 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 107 more trials not shown (16 RUNNING, 90 TERMINATED)


Result for _inner_e8deb_00119:
  auc: 0.6313942432403564
  date: 2021-03-19_13-39-20
  done: true
  experiment_id: e6cfb170a1f0414385ba798e6a979d7e
  experiment_tag: 119_batch_size=512,eta=0.0,lr=5,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 28179
  time_since_restore: 690.1514139175415
  time_this_iter_s: 690.1514139175415
  time_total_s: 690.1514139175415
  timestamp: 1616157560
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00119
  
[2m[36m(pid=8106)[0m time to fit was 941.7003564834595
Result for _inner_e8deb_00061:
  auc: 0.9073005557060242
  date: 2021-03-19_13-39-24
  done: false
  experiment_id: 91e0e016dab24310b7e89635f2538d08
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8106
  time_since_restore: 4718.317294836044
  time_this_iter_s: 4718.317294836044
  time_total_s: 4718.317294836044
  timestamp: 1616157564
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00061
  
[2m[36m(pid=8106)[0m Finished run with seed 0 - lr 0.001 - sec_lr 0.1 - bs 64 - mean val auc: 0.9073005557060242
Result for _inner_e8deb_00061:
  auc: 0.9073005557060242
  date: 2021-03-19_13-39-24
  done: true
  experiment_id: 91e0e016dab24310b7e89635f2538d08
  experiment_tag: 61_batch_size=64,eta=0.0,lr=0.001,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8106
  time_since_restore: 4718.317294836044
  time_this_iter_s: 4718.317294836044
  time_total_s: 4718.317294836044
  timestamp: 1616157564
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00061
  
[2m[36m(pid=26617)[0m time to fit was 519.8556249141693
[2m[36m(pid=26617)[0m GPU available: False, used: False
[2m[36m(pid=26617)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26617)[0m 
[2m[36m(pid=26617)[0m   | Name      | Type              | Params
[2m[36m(pid=26617)[0m ------------------------------------------------
[2m[36m(pid=26617)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26617)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26617)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26617)[0m ------------------------------------------------
[2m[36m(pid=26617)[0m 8.7 K     Trainable params
[2m[36m(pid=26617)[0m 0         Non-trainable params
[2m[36m(pid=26617)[0m 8.7 K     Total params
[2m[36m(pid=8415)[0m time to fit was 244.2455189228058
Result for _inner_e8deb_00106:
  auc: 0.9079979181289672
  date: 2021-03-19_13-39-29
  done: false
  experiment_id: 98681e6c70ec474080940ae49a9ebd4a
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8415
  time_since_restore: 1378.2413539886475
  time_this_iter_s: 1378.2413539886475
  time_total_s: 1378.2413539886475
  timestamp: 1616157569
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00106
  
[2m[36m(pid=8415)[0m Finished run with seed 0 - lr 1 - sec_lr 1 - bs 64 - mean val auc: 0.9079979181289672
== Status ==
Memory usage on this node: 9.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 129/180 (1 PENDING, 26 RUNNING, 102 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00092 | RUNNING    |       |          128 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00095 | RUNNING    |       |           32 |     0 | 0.01  |    1     |        |                  |          |
| _inner_e8deb_00100 | RUNNING    |       |           32 |     0 | 0.1   |    1     |        |                  |          |
| _inner_e8deb_00128 | PENDING    |       |          256 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 109 more trials not shown (16 RUNNING, 92 TERMINATED)


Result for _inner_e8deb_00106:
  auc: 0.9079979181289672
  date: 2021-03-19_13-39-29
  done: true
  experiment_id: 98681e6c70ec474080940ae49a9ebd4a
  experiment_tag: 106_batch_size=64,eta=0.0,lr=1,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8415
  time_since_restore: 1378.2413539886475
  time_this_iter_s: 1378.2413539886475
  time_total_s: 1378.2413539886475
  timestamp: 1616157569
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00106
  
[2m[36m(pid=48812)[0m Starting run with seed 0 - lr 0.01 - sec_lr 2 - bs 64
[2m[36m(pid=48812)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=48812)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=48812)[0m GPU available: False, used: False
[2m[36m(pid=48812)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48812)[0m 
[2m[36m(pid=48812)[0m   | Name      | Type              | Params
[2m[36m(pid=48812)[0m ------------------------------------------------
[2m[36m(pid=48812)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48812)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48812)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48812)[0m ------------------------------------------------
[2m[36m(pid=48812)[0m 8.7 K     Trainable params
[2m[36m(pid=48812)[0m 0         Non-trainable params
[2m[36m(pid=48812)[0m 8.7 K     Total params
[2m[36m(pid=48812)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=48812)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48812)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=48812)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48812)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=48812)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48915)[0m Starting run with seed 0 - lr 0.01 - sec_lr 2 - bs 128
[2m[36m(pid=48915)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=48915)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=48915)[0m GPU available: False, used: False
[2m[36m(pid=48915)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48915)[0m 
[2m[36m(pid=48915)[0m   | Name      | Type              | Params
[2m[36m(pid=48915)[0m ------------------------------------------------
[2m[36m(pid=48915)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48915)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48915)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48915)[0m ------------------------------------------------
[2m[36m(pid=48915)[0m 8.7 K     Trainable params
[2m[36m(pid=48915)[0m 0         Non-trainable params
[2m[36m(pid=48915)[0m 8.7 K     Total params
[2m[36m(pid=48915)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=48915)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48915)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=48915)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48915)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=48915)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=27924)[0m time to fit was 66.73794651031494
Result for _inner_e8deb_00118:
  auc: 0.6110753774642944
  date: 2021-03-19_13-39-36
  done: false
  experiment_id: a85a32f1c7c54080af2649ab299ef85b
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 27924
  time_since_restore: 713.6007852554321
  time_this_iter_s: 713.6007852554321
  time_total_s: 713.6007852554321
  timestamp: 1616157576
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00118
  
[2m[36m(pid=27924)[0m Finished run with seed 0 - lr 5 - sec_lr 1 - bs 256 - mean val auc: 0.6110753774642944
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 130/180 (1 PENDING, 26 RUNNING, 103 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    |       |           64 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00092 | RUNNING    |       |          128 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00095 | RUNNING    |       |           32 |     0 | 0.01  |    1     |        |                  |          |
| _inner_e8deb_00100 | RUNNING    |       |           32 |     0 | 0.1   |    1     |        |                  |          |
| _inner_e8deb_00129 | PENDING    |       |          512 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 110 more trials not shown (16 RUNNING, 93 TERMINATED)


Result for _inner_e8deb_00118:
  auc: 0.6110753774642944
  date: 2021-03-19_13-39-36
  done: true
  experiment_id: a85a32f1c7c54080af2649ab299ef85b
  experiment_tag: 118_batch_size=256,eta=0.0,lr=5,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 27924
  time_since_restore: 713.6007852554321
  time_this_iter_s: 713.6007852554321
  time_total_s: 713.6007852554321
  timestamp: 1616157576
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00118
  
[2m[36m(pid=49065)[0m Starting run with seed 0 - lr 0.01 - sec_lr 2 - bs 256
[2m[36m(pid=49065)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=49065)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=49065)[0m GPU available: False, used: False
[2m[36m(pid=49065)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49065)[0m 
[2m[36m(pid=49065)[0m   | Name      | Type              | Params
[2m[36m(pid=49065)[0m ------------------------------------------------
[2m[36m(pid=49065)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49065)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49065)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49065)[0m ------------------------------------------------
[2m[36m(pid=49065)[0m 8.7 K     Trainable params
[2m[36m(pid=49065)[0m 0         Non-trainable params
[2m[36m(pid=49065)[0m 8.7 K     Total params
[2m[36m(pid=49065)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=49065)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49065)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=49065)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49065)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=49065)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49112)[0m Starting run with seed 0 - lr 0.01 - sec_lr 2 - bs 512
[2m[36m(pid=49112)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=49112)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=49112)[0m GPU available: False, used: False
[2m[36m(pid=49112)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49112)[0m 
[2m[36m(pid=49112)[0m   | Name      | Type              | Params
[2m[36m(pid=49112)[0m ------------------------------------------------
[2m[36m(pid=49112)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49112)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49112)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49112)[0m ------------------------------------------------
[2m[36m(pid=49112)[0m 8.7 K     Trainable params
[2m[36m(pid=49112)[0m 0         Non-trainable params
[2m[36m(pid=49112)[0m 8.7 K     Total params
[2m[36m(pid=49112)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=49112)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49112)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=49112)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49112)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=49112)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=34577)[0m time to fit was 516.4652280807495
[2m[36m(pid=34577)[0m GPU available: False, used: False
[2m[36m(pid=34577)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=34577)[0m 
[2m[36m(pid=34577)[0m   | Name      | Type              | Params
[2m[36m(pid=34577)[0m ------------------------------------------------
[2m[36m(pid=34577)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=34577)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=34577)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=34577)[0m ------------------------------------------------
[2m[36m(pid=34577)[0m 8.7 K     Trainable params
[2m[36m(pid=34577)[0m 0         Non-trainable params
[2m[36m(pid=34577)[0m 8.7 K     Total params
[2m[36m(pid=44479)[0m time to fit was 175.6163947582245
[2m[36m(pid=44479)[0m GPU available: False, used: False
[2m[36m(pid=44479)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=44479)[0m 
[2m[36m(pid=44479)[0m   | Name      | Type              | Params
[2m[36m(pid=44479)[0m ------------------------------------------------
[2m[36m(pid=44479)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=44479)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=44479)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=44479)[0m ------------------------------------------------
[2m[36m(pid=44479)[0m 8.7 K     Trainable params
[2m[36m(pid=44479)[0m 0         Non-trainable params
[2m[36m(pid=44479)[0m 8.7 K     Total params
[2m[36m(pid=40014)[0m time to fit was 269.34152245521545
[2m[36m(pid=40014)[0m GPU available: False, used: False
[2m[36m(pid=40014)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40014)[0m 
[2m[36m(pid=40014)[0m   | Name      | Type              | Params
[2m[36m(pid=40014)[0m ------------------------------------------------
[2m[36m(pid=40014)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40014)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40014)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40014)[0m ------------------------------------------------
[2m[36m(pid=40014)[0m 8.7 K     Trainable params
[2m[36m(pid=40014)[0m 0         Non-trainable params
[2m[36m(pid=40014)[0m 8.7 K     Total params
[2m[36m(pid=16527)[0m time to fit was 636.9214181900024
Result for _inner_e8deb_00086:
  auc: 0.7897416591644287
  date: 2021-03-19_13-41-27
  done: false
  experiment_id: d083ddbce9234711a5e9bee8938167a3
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 16527
  time_since_restore: 2946.808854341507
  time_this_iter_s: 2946.808854341507
  time_total_s: 2946.808854341507
  timestamp: 1616157687
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00086
  
[2m[36m(pid=16527)[0m Finished run with seed 0 - lr 5 - sec_lr 0.1 - bs 64 - mean val auc: 0.7897416591644287
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 131/180 (1 PENDING, 26 RUNNING, 104 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |                     |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00086 | RUNNING    | 145.101.32.82:16527 |           64 |     0 | 5     |    0.1   |      1 |          2946.81 | 0.789742 |
| _inner_e8deb_00090 | RUNNING    |                     |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |                     |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00092 | RUNNING    |                     |          128 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00095 | RUNNING    |                     |           32 |     0 | 0.01  |    1     |        |                  |          |
| _inner_e8deb_00100 | RUNNING    |                     |           32 |     0 | 0.1   |    1     |        |                  |          |
| _inner_e8deb_00130 | PENDING    |                     |           32 |     0 | 0.1   |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 111 more trials not shown (16 RUNNING, 94 TERMINATED)


Result for _inner_e8deb_00086:
  auc: 0.7897416591644287
  date: 2021-03-19_13-41-27
  done: true
  experiment_id: d083ddbce9234711a5e9bee8938167a3
  experiment_tag: 86_batch_size=64,eta=0.0,lr=5,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 16527
  time_since_restore: 2946.808854341507
  time_this_iter_s: 2946.808854341507
  time_total_s: 2946.808854341507
  timestamp: 1616157687
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00086
  
[2m[36m(pid=44479)[0m time to fit was 87.42090916633606
[2m[36m(pid=44479)[0m GPU available: False, used: False
[2m[36m(pid=44479)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=44479)[0m 
[2m[36m(pid=44479)[0m   | Name      | Type              | Params
[2m[36m(pid=44479)[0m ------------------------------------------------
[2m[36m(pid=44479)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=44479)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=44479)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=44479)[0m ------------------------------------------------
[2m[36m(pid=44479)[0m 8.7 K     Trainable params
[2m[36m(pid=44479)[0m 0         Non-trainable params
[2m[36m(pid=44479)[0m 8.7 K     Total params
[2m[36m(pid=26617)[0m time to fit was 124.93540358543396
[2m[36m(pid=26617)[0m GPU available: False, used: False
[2m[36m(pid=26617)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26617)[0m 
[2m[36m(pid=26617)[0m   | Name      | Type              | Params
[2m[36m(pid=26617)[0m ------------------------------------------------
[2m[36m(pid=26617)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26617)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26617)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26617)[0m ------------------------------------------------
[2m[36m(pid=26617)[0m 8.7 K     Trainable params
[2m[36m(pid=26617)[0m 0         Non-trainable params
[2m[36m(pid=26617)[0m 8.7 K     Total params
[2m[36m(pid=49065)[0m time to fit was 116.26587319374084
[2m[36m(pid=49065)[0m GPU available: False, used: False
[2m[36m(pid=49065)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49065)[0m 
[2m[36m(pid=49065)[0m   | Name      | Type              | Params
[2m[36m(pid=49065)[0m ------------------------------------------------
[2m[36m(pid=49065)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49065)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49065)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49065)[0m ------------------------------------------------
[2m[36m(pid=49065)[0m 8.7 K     Trainable params
[2m[36m(pid=49065)[0m 0         Non-trainable params
[2m[36m(pid=49065)[0m 8.7 K     Total params
[2m[36m(pid=52747)[0m Starting run with seed 0 - lr 0.1 - sec_lr 2 - bs 32
[2m[36m(pid=52747)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=52747)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=52747)[0m GPU available: False, used: False
[2m[36m(pid=52747)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=52747)[0m 
[2m[36m(pid=52747)[0m   | Name      | Type              | Params
[2m[36m(pid=52747)[0m ------------------------------------------------
[2m[36m(pid=52747)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=52747)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=52747)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=52747)[0m ------------------------------------------------
[2m[36m(pid=52747)[0m 8.7 K     Trainable params
[2m[36m(pid=52747)[0m 0         Non-trainable params
[2m[36m(pid=52747)[0m 8.7 K     Total params
[2m[36m(pid=52747)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=52747)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=52747)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=52747)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=52747)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=52747)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=3723)[0m time to fit was 677.2784790992737
[2m[36m(pid=3723)[0m GPU available: False, used: False
[2m[36m(pid=3723)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=3723)[0m 
[2m[36m(pid=3723)[0m   | Name      | Type              | Params
[2m[36m(pid=3723)[0m ------------------------------------------------
[2m[36m(pid=3723)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=3723)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=3723)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=3723)[0m ------------------------------------------------
[2m[36m(pid=3723)[0m 8.7 K     Trainable params
[2m[36m(pid=3723)[0m 0         Non-trainable params
[2m[36m(pid=3723)[0m 8.7 K     Total params
[2m[36m(pid=27849)[0m time to fit was 526.6700308322906
Result for _inner_e8deb_00092:
  auc: 0.884829831123352
  date: 2021-03-19_13-42-04
  done: false
  experiment_id: 0c3be8822c6846188ddd6c7c1ad3d8bb
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 27849
  time_since_restore: 2570.9984455108643
  time_this_iter_s: 2570.9984455108643
  time_total_s: 2570.9984455108643
  timestamp: 1616157724
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00092
  
[2m[36m(pid=27849)[0m Finished run with seed 0 - lr 0.001 - sec_lr 1 - bs 128 - mean val auc: 0.884829831123352
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 132/180 (1 PENDING, 26 RUNNING, 105 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |                     |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |                     |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |                     |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00092 | RUNNING    | 145.101.32.82:27849 |          128 |     0 | 0.001 |    1     |      1 |          2571    | 0.88483  |
| _inner_e8deb_00095 | RUNNING    |                     |           32 |     0 | 0.01  |    1     |        |                  |          |
| _inner_e8deb_00100 | RUNNING    |                     |           32 |     0 | 0.1   |    1     |        |                  |          |
| _inner_e8deb_00105 | RUNNING    |                     |           32 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00131 | PENDING    |                     |           64 |     0 | 0.1   |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 112 more trials not shown (16 RUNNING, 95 TERMINATED)


Result for _inner_e8deb_00092:
  auc: 0.884829831123352
  date: 2021-03-19_13-42-04
  done: true
  experiment_id: 0c3be8822c6846188ddd6c7c1ad3d8bb
  experiment_tag: 92_batch_size=128,eta=0.0,lr=0.001,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 27849
  time_since_restore: 2570.9984455108643
  time_this_iter_s: 2570.9984455108643
  time_total_s: 2570.9984455108643
  timestamp: 1616157724
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00092
  
2021-03-19 13:42:05,374	WARNING worker.py:1034 -- The actor or task with ID fffffffffffffffff878b75201000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=49112)[0m time to fit was 143.62657475471497
[2m[36m(pid=49112)[0m GPU available: False, used: False
[2m[36m(pid=49112)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49112)[0m 
[2m[36m(pid=49112)[0m   | Name      | Type              | Params
[2m[36m(pid=49112)[0m ------------------------------------------------
[2m[36m(pid=49112)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49112)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49112)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49112)[0m ------------------------------------------------
[2m[36m(pid=49112)[0m 8.7 K     Trainable params
[2m[36m(pid=49112)[0m 0         Non-trainable params
[2m[36m(pid=49112)[0m 8.7 K     Total params
[2m[36m(pid=48915)[0m time to fit was 156.0652039051056
[2m[36m(pid=48915)[0m GPU available: False, used: False
[2m[36m(pid=48915)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48915)[0m 
[2m[36m(pid=48915)[0m   | Name      | Type              | Params
[2m[36m(pid=48915)[0m ------------------------------------------------
[2m[36m(pid=48915)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48915)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48915)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48915)[0m ------------------------------------------------
[2m[36m(pid=48915)[0m 8.7 K     Trainable params
[2m[36m(pid=48915)[0m 0         Non-trainable params
[2m[36m(pid=48915)[0m 8.7 K     Total params
[2m[36m(pid=1105)[0m Starting run with seed 0 - lr 0.1 - sec_lr 2 - bs 64
[2m[36m(pid=1105)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=1105)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=1105)[0m GPU available: False, used: False
[2m[36m(pid=1105)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=1105)[0m 
[2m[36m(pid=1105)[0m   | Name      | Type              | Params
[2m[36m(pid=1105)[0m ------------------------------------------------
[2m[36m(pid=1105)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=1105)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=1105)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=1105)[0m ------------------------------------------------
[2m[36m(pid=1105)[0m 8.7 K     Trainable params
[2m[36m(pid=1105)[0m 0         Non-trainable params
[2m[36m(pid=1105)[0m 8.7 K     Total params
[2m[36m(pid=1105)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=1105)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=1105)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=1105)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=1105)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=1105)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=26407)[0m time to fit was 360.71938252449036
[2m[36m(pid=26407)[0m GPU available: False, used: False
[2m[36m(pid=26407)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26407)[0m 
[2m[36m(pid=26407)[0m   | Name      | Type              | Params
[2m[36m(pid=26407)[0m ------------------------------------------------
[2m[36m(pid=26407)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26407)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26407)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26407)[0m ------------------------------------------------
[2m[36m(pid=26407)[0m 8.7 K     Trainable params
[2m[36m(pid=26407)[0m 0         Non-trainable params
[2m[36m(pid=26407)[0m 8.7 K     Total params
[2m[36m(pid=44479)[0m time to fit was 85.63380980491638
[2m[36m(pid=44479)[0m GPU available: False, used: False
[2m[36m(pid=44479)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=44479)[0m 
[2m[36m(pid=44479)[0m   | Name      | Type              | Params
[2m[36m(pid=44479)[0m ------------------------------------------------
[2m[36m(pid=44479)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=44479)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=44479)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=44479)[0m ------------------------------------------------
[2m[36m(pid=44479)[0m 8.7 K     Trainable params
[2m[36m(pid=44479)[0m 0         Non-trainable params
[2m[36m(pid=44479)[0m 8.7 K     Total params
[2m[36m(pid=8460)[0m time to fit was 248.6205496788025
Result for _inner_e8deb_00111:
  auc: 0.8431290745735168
  date: 2021-03-19_13-42-58
  done: false
  experiment_id: 91c525c777504cc2bc7809b26f39ef4b
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8460
  time_since_restore: 1444.3701756000519
  time_this_iter_s: 1444.3701756000519
  time_total_s: 1444.3701756000519
  timestamp: 1616157778
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00111
  
[2m[36m(pid=8460)[0m Finished run with seed 0 - lr 2 - sec_lr 1 - bs 64 - mean val auc: 0.8431290745735168
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 133/180 (1 PENDING, 26 RUNNING, 106 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00095 | RUNNING    |       |           32 |     0 | 0.01  |    1     |        |                  |          |
| _inner_e8deb_00100 | RUNNING    |       |           32 |     0 | 0.1   |    1     |        |                  |          |
| _inner_e8deb_00105 | RUNNING    |       |           32 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00132 | PENDING    |       |          128 |     0 | 0.1   |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 113 more trials not shown (16 RUNNING, 96 TERMINATED)


Result for _inner_e8deb_00111:
  auc: 0.8431290745735168
  date: 2021-03-19_13-42-58
  done: true
  experiment_id: 91c525c777504cc2bc7809b26f39ef4b
  experiment_tag: 111_batch_size=64,eta=0.0,lr=2,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8460
  time_since_restore: 1444.3701756000519
  time_this_iter_s: 1444.3701756000519
  time_total_s: 1444.3701756000519
  timestamp: 1616157778
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00111
  
[2m[36m(pid=2917)[0m Starting run with seed 0 - lr 0.1 - sec_lr 2 - bs 128
[2m[36m(pid=2917)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=2917)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=2917)[0m GPU available: False, used: False
[2m[36m(pid=2917)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=2917)[0m 
[2m[36m(pid=2917)[0m   | Name      | Type              | Params
[2m[36m(pid=2917)[0m ------------------------------------------------
[2m[36m(pid=2917)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=2917)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=2917)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=2917)[0m ------------------------------------------------
[2m[36m(pid=2917)[0m 8.7 K     Trainable params
[2m[36m(pid=2917)[0m 0         Non-trainable params
[2m[36m(pid=2917)[0m 8.7 K     Total params
[2m[36m(pid=2917)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=2917)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=2917)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=2917)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=2917)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=2917)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31106)[0m time to fit was 854.6521828174591
[2m[36m(pid=31106)[0m GPU available: False, used: False
[2m[36m(pid=31106)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31106)[0m 
[2m[36m(pid=31106)[0m   | Name      | Type              | Params
[2m[36m(pid=31106)[0m ------------------------------------------------
[2m[36m(pid=31106)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31106)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31106)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31106)[0m ------------------------------------------------
[2m[36m(pid=31106)[0m 8.7 K     Trainable params
[2m[36m(pid=31106)[0m 0         Non-trainable params
[2m[36m(pid=31106)[0m 8.7 K     Total params
[2m[36m(pid=26617)[0m time to fit was 126.55103468894958
Result for _inner_e8deb_00117:
  auc: 0.6341785550117492
  date: 2021-03-19_13-43-38
  done: false
  experiment_id: 48e99430e9074567b24db7ca7f50b195
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 26617
  time_since_restore: 1007.7845118045807
  time_this_iter_s: 1007.7845118045807
  time_total_s: 1007.7845118045807
  timestamp: 1616157818
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00117
  
[2m[36m(pid=26617)[0m Finished run with seed 0 - lr 5 - sec_lr 1 - bs 128 - mean val auc: 0.6341785550117492
== Status ==
Memory usage on this node: 9.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 134/180 (1 PENDING, 26 RUNNING, 107 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00095 | RUNNING    |       |           32 |     0 | 0.01  |    1     |        |                  |          |
| _inner_e8deb_00100 | RUNNING    |       |           32 |     0 | 0.1   |    1     |        |                  |          |
| _inner_e8deb_00105 | RUNNING    |       |           32 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00133 | PENDING    |       |          256 |     0 | 0.1   |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 114 more trials not shown (16 RUNNING, 97 TERMINATED)


Result for _inner_e8deb_00117:
  auc: 0.6341785550117492
  date: 2021-03-19_13-43-38
  done: true
  experiment_id: 48e99430e9074567b24db7ca7f50b195
  experiment_tag: 117_batch_size=128,eta=0.0,lr=5,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 26617
  time_since_restore: 1007.7845118045807
  time_this_iter_s: 1007.7845118045807
  time_total_s: 1007.7845118045807
  timestamp: 1616157818
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00117
  
[2m[36m(pid=4193)[0m Starting run with seed 0 - lr 0.1 - sec_lr 2 - bs 256
[2m[36m(pid=4193)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=4193)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=4193)[0m GPU available: False, used: False
[2m[36m(pid=4193)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=4193)[0m 
[2m[36m(pid=4193)[0m   | Name      | Type              | Params
[2m[36m(pid=4193)[0m ------------------------------------------------
[2m[36m(pid=4193)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=4193)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=4193)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=4193)[0m ------------------------------------------------
[2m[36m(pid=4193)[0m 8.7 K     Trainable params
[2m[36m(pid=4193)[0m 0         Non-trainable params
[2m[36m(pid=4193)[0m 8.7 K     Total params
[2m[36m(pid=4193)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=4193)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=4193)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=4193)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=4193)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=4193)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=40014)[0m time to fit was 243.1637613773346
[2m[36m(pid=40014)[0m GPU available: False, used: False
[2m[36m(pid=40014)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40014)[0m 
[2m[36m(pid=40014)[0m   | Name      | Type              | Params
[2m[36m(pid=40014)[0m ------------------------------------------------
[2m[36m(pid=40014)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40014)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40014)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40014)[0m ------------------------------------------------
[2m[36m(pid=40014)[0m 8.7 K     Trainable params
[2m[36m(pid=40014)[0m 0         Non-trainable params
[2m[36m(pid=40014)[0m 8.7 K     Total params
[2m[36m(pid=8457)[0m time to fit was 534.9706017971039
[2m[36m(pid=8457)[0m GPU available: False, used: False
[2m[36m(pid=8457)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8457)[0m 
[2m[36m(pid=8457)[0m   | Name      | Type              | Params
[2m[36m(pid=8457)[0m ------------------------------------------------
[2m[36m(pid=8457)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8457)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8457)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8457)[0m ------------------------------------------------
[2m[36m(pid=8457)[0m 8.7 K     Trainable params
[2m[36m(pid=8457)[0m 0         Non-trainable params
[2m[36m(pid=8457)[0m 8.7 K     Total params
[2m[36m(pid=49065)[0m time to fit was 191.23627305030823
[2m[36m(pid=49065)[0m GPU available: False, used: False
[2m[36m(pid=49065)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49065)[0m 
[2m[36m(pid=49065)[0m   | Name      | Type              | Params
[2m[36m(pid=49065)[0m ------------------------------------------------
[2m[36m(pid=49065)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49065)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49065)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49065)[0m ------------------------------------------------
[2m[36m(pid=49065)[0m 8.7 K     Trainable params
[2m[36m(pid=49065)[0m 0         Non-trainable params
[2m[36m(pid=49065)[0m 8.7 K     Total params
[2m[36m(pid=4193)[0m time to fit was 72.64948320388794
[2m[36m(pid=4193)[0m GPU available: False, used: False
[2m[36m(pid=4193)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=4193)[0m 
[2m[36m(pid=4193)[0m   | Name      | Type              | Params
[2m[36m(pid=4193)[0m ------------------------------------------------
[2m[36m(pid=4193)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=4193)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=4193)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=4193)[0m ------------------------------------------------
[2m[36m(pid=4193)[0m 8.7 K     Trainable params
[2m[36m(pid=4193)[0m 0         Non-trainable params
[2m[36m(pid=4193)[0m 8.7 K     Total params
[2m[36m(pid=11272)[0m time to fit was 1650.2546062469482
[2m[36m(pid=11272)[0m GPU available: False, used: False
[2m[36m(pid=11272)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=11272)[0m 
[2m[36m(pid=11272)[0m   | Name      | Type              | Params
[2m[36m(pid=11272)[0m ------------------------------------------------
[2m[36m(pid=11272)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=11272)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=11272)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=11272)[0m ------------------------------------------------
[2m[36m(pid=11272)[0m 8.7 K     Trainable params
[2m[36m(pid=11272)[0m 0         Non-trainable params
[2m[36m(pid=11272)[0m 8.7 K     Total params
[2m[36m(pid=49112)[0m time to fit was 188.74983310699463
[2m[36m(pid=49112)[0m GPU available: False, used: False
[2m[36m(pid=49112)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49112)[0m 
[2m[36m(pid=49112)[0m   | Name      | Type              | Params
[2m[36m(pid=49112)[0m ------------------------------------------------
[2m[36m(pid=49112)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49112)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49112)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49112)[0m ------------------------------------------------
[2m[36m(pid=49112)[0m 8.7 K     Trainable params
[2m[36m(pid=49112)[0m 0         Non-trainable params
[2m[36m(pid=49112)[0m 8.7 K     Total params
[2m[36m(pid=2917)[0m time to fit was 136.2272720336914
[2m[36m(pid=48812)[0m time to fit was 354.5504586696625
[2m[36m(pid=2917)[0m GPU available: False, used: False
[2m[36m(pid=2917)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=2917)[0m 
[2m[36m(pid=2917)[0m   | Name      | Type              | Params
[2m[36m(pid=2917)[0m ------------------------------------------------
[2m[36m(pid=2917)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=2917)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=2917)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=2917)[0m ------------------------------------------------
[2m[36m(pid=2917)[0m 8.7 K     Trainable params
[2m[36m(pid=2917)[0m 0         Non-trainable params
[2m[36m(pid=2917)[0m 8.7 K     Total params
[2m[36m(pid=48812)[0m GPU available: False, used: False
[2m[36m(pid=48812)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48812)[0m 
[2m[36m(pid=48812)[0m   | Name      | Type              | Params
[2m[36m(pid=48812)[0m ------------------------------------------------
[2m[36m(pid=48812)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48812)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48812)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48812)[0m ------------------------------------------------
[2m[36m(pid=48812)[0m 8.7 K     Trainable params
[2m[36m(pid=48812)[0m 0         Non-trainable params
[2m[36m(pid=48812)[0m 8.7 K     Total params
[2m[36m(pid=29675)[0m time to fit was 600.3086538314819
Result for _inner_e8deb_00095:
  auc: 0.9102319836616516
  date: 2021-03-19_13-45-41
  done: false
  experiment_id: 7c389fee964543569fc22357c0097b04
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 29675
  time_since_restore: 2729.002959728241
  time_this_iter_s: 2729.002959728241
  time_total_s: 2729.002959728241
  timestamp: 1616157941
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00095
  
[2m[36m(pid=29675)[0m Finished run with seed 0 - lr 0.01 - sec_lr 1 - bs 32 - mean val auc: 0.9102319836616516
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 135/180 (1 PENDING, 26 RUNNING, 108 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |                     |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |                     |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |                     |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00095 | RUNNING    | 145.101.32.82:29675 |           32 |     0 | 0.01  |    1     |      1 |          2729    | 0.910232 |
| _inner_e8deb_00100 | RUNNING    |                     |           32 |     0 | 0.1   |    1     |        |                  |          |
| _inner_e8deb_00105 | RUNNING    |                     |           32 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |                     |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00134 | PENDING    |                     |          512 |     0 | 0.1   |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 115 more trials not shown (16 RUNNING, 98 TERMINATED)


Result for _inner_e8deb_00095:
  auc: 0.9102319836616516
  date: 2021-03-19_13-45-41
  done: true
  experiment_id: 7c389fee964543569fc22357c0097b04
  experiment_tag: 95_batch_size=32,eta=0.0,lr=0.01,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 29675
  time_since_restore: 2729.002959728241
  time_this_iter_s: 2729.002959728241
  time_total_s: 2729.002959728241
  timestamp: 1616157941
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00095
  
[2m[36m(pid=44479)[0m time to fit was 171.731032371521
[2m[36m(pid=44479)[0m GPU available: False, used: False
[2m[36m(pid=44479)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=44479)[0m 
[2m[36m(pid=44479)[0m   | Name      | Type              | Params
[2m[36m(pid=44479)[0m ------------------------------------------------
[2m[36m(pid=44479)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=44479)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=44479)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=44479)[0m ------------------------------------------------
[2m[36m(pid=44479)[0m 8.7 K     Trainable params
[2m[36m(pid=44479)[0m 0         Non-trainable params
[2m[36m(pid=44479)[0m 8.7 K     Total params
[2m[36m(pid=8169)[0m Starting run with seed 0 - lr 0.1 - sec_lr 2 - bs 512
[2m[36m(pid=8169)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=8169)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=8169)[0m GPU available: False, used: False
[2m[36m(pid=8169)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=41401)[0m time to fit was 465.83681535720825
[2m[36m(pid=8169)[0m 
[2m[36m(pid=8169)[0m   | Name      | Type              | Params
[2m[36m(pid=8169)[0m ------------------------------------------------
[2m[36m(pid=8169)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8169)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8169)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8169)[0m ------------------------------------------------
[2m[36m(pid=8169)[0m 8.7 K     Trainable params
[2m[36m(pid=8169)[0m 0         Non-trainable params
[2m[36m(pid=8169)[0m 8.7 K     Total params
[2m[36m(pid=8169)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8169)[0m   warnings.warn(*args, **kwargs)
Result for _inner_e8deb_00100:
  auc: 0.9089077830314636
  date: 2021-03-19_13-45-54
  done: false
  experiment_id: 7d88c3332e02462d83f61a25ae8e6c0a
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 41401
  time_since_restore: 2380.593409061432
  time_this_iter_s: 2380.593409061432
  time_total_s: 2380.593409061432
  timestamp: 1616157954
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00100
  
[2m[36m(pid=41401)[0m Finished run with seed 0 - lr 0.1 - sec_lr 1 - bs 32 - mean val auc: 0.9089077830314636
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 136/180 (1 PENDING, 26 RUNNING, 109 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |                     |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |                     |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |                     |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |                     |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00100 | RUNNING    | 145.101.32.82:41401 |           32 |     0 | 0.1   |    1     |      1 |          2380.59 | 0.908908 |
| _inner_e8deb_00105 | RUNNING    |                     |           32 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |                     |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |                     |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00135 | PENDING    |                     |           32 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 116 more trials not shown (16 RUNNING, 99 TERMINATED)


Result for _inner_e8deb_00100:
  auc: 0.9089077830314636
  date: 2021-03-19_13-45-54
  done: true
  experiment_id: 7d88c3332e02462d83f61a25ae8e6c0a
  experiment_tag: 100_batch_size=32,eta=0.0,lr=0.1,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 41401
  time_since_restore: 2380.593409061432
  time_this_iter_s: 2380.593409061432
  time_total_s: 2380.593409061432
  timestamp: 1616157954
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00100
  
[2m[36m(pid=8169)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8169)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8169)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=8169)[0m   warnings.warn(*args, **kwargs)
2021-03-19 13:45:55,966	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffffb36476e701000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=1105)[0m time to fit was 224.0629906654358
[2m[36m(pid=1105)[0m GPU available: False, used: False
[2m[36m(pid=1105)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=1105)[0m 
[2m[36m(pid=1105)[0m   | Name      | Type              | Params
[2m[36m(pid=1105)[0m ------------------------------------------------
[2m[36m(pid=1105)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=1105)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=1105)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=1105)[0m ------------------------------------------------
[2m[36m(pid=1105)[0m 8.7 K     Trainable params
[2m[36m(pid=1105)[0m 0         Non-trainable params
[2m[36m(pid=1105)[0m 8.7 K     Total params
[2m[36m(pid=8639)[0m Starting run with seed 0 - lr 1 - sec_lr 2 - bs 32
[2m[36m(pid=8639)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=8639)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=8639)[0m GPU available: False, used: False
[2m[36m(pid=8639)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8639)[0m 
[2m[36m(pid=8639)[0m   | Name      | Type              | Params
[2m[36m(pid=8639)[0m ------------------------------------------------
[2m[36m(pid=8639)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8639)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8639)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8639)[0m ------------------------------------------------
[2m[36m(pid=8639)[0m 8.7 K     Trainable params
[2m[36m(pid=8639)[0m 0         Non-trainable params
[2m[36m(pid=8639)[0m 8.7 K     Total params
[2m[36m(pid=8639)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8639)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8639)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8639)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8639)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=8639)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48915)[0m time to fit was 242.1064488887787
[2m[36m(pid=48915)[0m GPU available: False, used: False
[2m[36m(pid=48915)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48915)[0m 
[2m[36m(pid=48915)[0m   | Name      | Type              | Params
[2m[36m(pid=48915)[0m ------------------------------------------------
[2m[36m(pid=48915)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48915)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48915)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48915)[0m ------------------------------------------------
[2m[36m(pid=48915)[0m 8.7 K     Trainable params
[2m[36m(pid=48915)[0m 0         Non-trainable params
[2m[36m(pid=48915)[0m 8.7 K     Total params
[2m[36m(pid=25581)[0m time to fit was 800.9922335147858
[2m[36m(pid=25581)[0m GPU available: False, used: False
[2m[36m(pid=25581)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=25581)[0m 
[2m[36m(pid=25581)[0m   | Name      | Type              | Params
[2m[36m(pid=25581)[0m ------------------------------------------------
[2m[36m(pid=25581)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=25581)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=25581)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=25581)[0m ------------------------------------------------
[2m[36m(pid=25581)[0m 8.7 K     Trainable params
[2m[36m(pid=25581)[0m 0         Non-trainable params
[2m[36m(pid=25581)[0m 8.7 K     Total params
[2m[36m(pid=4193)[0m time to fit was 85.67935633659363
[2m[36m(pid=4193)[0m GPU available: False, used: False
[2m[36m(pid=4193)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=4193)[0m 
[2m[36m(pid=4193)[0m   | Name      | Type              | Params
[2m[36m(pid=4193)[0m ------------------------------------------------
[2m[36m(pid=4193)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=4193)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=4193)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=4193)[0m ------------------------------------------------
[2m[36m(pid=4193)[0m 8.7 K     Trainable params
[2m[36m(pid=4193)[0m 0         Non-trainable params
[2m[36m(pid=4193)[0m 8.7 K     Total params
[2m[36m(pid=40014)[0m time to fit was 122.25653457641602
[2m[36m(pid=40014)[0m GPU available: False, used: False
[2m[36m(pid=40014)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=40014)[0m 
[2m[36m(pid=40014)[0m   | Name      | Type              | Params
[2m[36m(pid=40014)[0m ------------------------------------------------
[2m[36m(pid=40014)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=40014)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=40014)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=40014)[0m ------------------------------------------------
[2m[36m(pid=40014)[0m 8.7 K     Trainable params
[2m[36m(pid=40014)[0m 0         Non-trainable params
[2m[36m(pid=40014)[0m 8.7 K     Total params
[2m[36m(pid=47886)[0m time to fit was 460.41974687576294
[2m[36m(pid=47886)[0m GPU available: False, used: False
[2m[36m(pid=47886)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=47886)[0m 
[2m[36m(pid=47886)[0m   | Name      | Type              | Params
[2m[36m(pid=47886)[0m ------------------------------------------------
[2m[36m(pid=47886)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=47886)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=47886)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=47886)[0m ------------------------------------------------
[2m[36m(pid=47886)[0m 8.7 K     Trainable params
[2m[36m(pid=47886)[0m 0         Non-trainable params
[2m[36m(pid=47886)[0m 8.7 K     Total params
[2m[36m(pid=8169)[0m time to fit was 56.935405015945435
[2m[36m(pid=8169)[0m GPU available: False, used: False
[2m[36m(pid=8169)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8169)[0m 
[2m[36m(pid=8169)[0m   | Name      | Type              | Params
[2m[36m(pid=8169)[0m ------------------------------------------------
[2m[36m(pid=8169)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8169)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8169)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8169)[0m ------------------------------------------------
[2m[36m(pid=8169)[0m 8.7 K     Trainable params
[2m[36m(pid=8169)[0m 0         Non-trainable params
[2m[36m(pid=8169)[0m 8.7 K     Total params
[2m[36m(pid=2917)[0m time to fit was 138.5916028022766
[2m[36m(pid=2917)[0m GPU available: False, used: False
[2m[36m(pid=2917)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=2917)[0m 
[2m[36m(pid=2917)[0m   | Name      | Type              | Params
[2m[36m(pid=2917)[0m ------------------------------------------------
[2m[36m(pid=2917)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=2917)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=2917)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=2917)[0m ------------------------------------------------
[2m[36m(pid=2917)[0m 8.7 K     Trainable params
[2m[36m(pid=2917)[0m 0         Non-trainable params
[2m[36m(pid=2917)[0m 8.7 K     Total params
[2m[36m(pid=4193)[0m time to fit was 103.35910773277283
[2m[36m(pid=4193)[0m GPU available: False, used: False
[2m[36m(pid=4193)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=4193)[0m 
[2m[36m(pid=4193)[0m   | Name      | Type              | Params
[2m[36m(pid=4193)[0m ------------------------------------------------
[2m[36m(pid=4193)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=4193)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=4193)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=4193)[0m ------------------------------------------------
[2m[36m(pid=4193)[0m 8.7 K     Trainable params
[2m[36m(pid=4193)[0m 0         Non-trainable params
[2m[36m(pid=4193)[0m 8.7 K     Total params
[2m[36m(pid=52747)[0m time to fit was 393.1456081867218
[2m[36m(pid=52747)[0m GPU available: False, used: False
[2m[36m(pid=52747)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=52747)[0m 
[2m[36m(pid=52747)[0m   | Name      | Type              | Params
[2m[36m(pid=52747)[0m ------------------------------------------------
[2m[36m(pid=52747)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=52747)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=52747)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=52747)[0m ------------------------------------------------
[2m[36m(pid=52747)[0m 8.7 K     Trainable params
[2m[36m(pid=52747)[0m 0         Non-trainable params
[2m[36m(pid=52747)[0m 8.7 K     Total params
[2m[36m(pid=8169)[0m time to fit was 88.90763235092163
[2m[36m(pid=8169)[0m GPU available: False, used: False
[2m[36m(pid=8169)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8169)[0m 
[2m[36m(pid=8169)[0m   | Name      | Type              | Params
[2m[36m(pid=8169)[0m ------------------------------------------------
[2m[36m(pid=8169)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8169)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8169)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8169)[0m ------------------------------------------------
[2m[36m(pid=8169)[0m 8.7 K     Trainable params
[2m[36m(pid=8169)[0m 0         Non-trainable params
[2m[36m(pid=8169)[0m 8.7 K     Total params
[2m[36m(pid=6688)[0m time to fit was 1771.307118177414
[2m[36m(pid=6688)[0m GPU available: False, used: False
[2m[36m(pid=6688)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=6688)[0m 
[2m[36m(pid=6688)[0m   | Name      | Type              | Params
[2m[36m(pid=6688)[0m ------------------------------------------------
[2m[36m(pid=6688)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=6688)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=6688)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=6688)[0m ------------------------------------------------
[2m[36m(pid=6688)[0m 8.7 K     Trainable params
[2m[36m(pid=6688)[0m 0         Non-trainable params
[2m[36m(pid=6688)[0m 8.7 K     Total params
[2m[36m(pid=34577)[0m time to fit was 532.64080286026
[2m[36m(pid=34577)[0m GPU available: False, used: False
[2m[36m(pid=34577)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=34577)[0m 
[2m[36m(pid=34577)[0m   | Name      | Type              | Params
[2m[36m(pid=34577)[0m ------------------------------------------------
[2m[36m(pid=34577)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=34577)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=34577)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=34577)[0m ------------------------------------------------
[2m[36m(pid=34577)[0m 8.7 K     Trainable params
[2m[36m(pid=34577)[0m 0         Non-trainable params
[2m[36m(pid=34577)[0m 8.7 K     Total params
[2m[36m(pid=44479)[0m time to fit was 193.01311898231506
[2m[36m(pid=49112)[0m time to fit was 224.67788887023926
Result for _inner_e8deb_00124:
  auc: 0.8568771839141845
  date: 2021-03-19_13-49-02
  done: false
  experiment_id: 2600c6eb50e44555b216f96639829fbc
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 44479
  time_since_restore: 714.5975730419159
  time_this_iter_s: 714.5975730419159
  time_total_s: 714.5975730419159
  timestamp: 1616158142
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00124
  
[2m[36m(pid=44479)[0m Finished run with seed 0 - lr 0.001 - sec_lr 2 - bs 512 - mean val auc: 0.8568771839141845
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 137/180 (1 PENDING, 26 RUNNING, 110 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00105 | RUNNING    |       |           32 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00116 | RUNNING    |       |           64 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00136 | PENDING    |       |           64 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 117 more trials not shown (16 RUNNING, 100 TERMINATED)


Result for _inner_e8deb_00124:
  auc: 0.8568771839141845
  date: 2021-03-19_13-49-02
  done: true
  experiment_id: 2600c6eb50e44555b216f96639829fbc
  experiment_tag: 124_batch_size=512,eta=0.0,lr=0.001,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 44479
  time_since_restore: 714.5975730419159
  time_this_iter_s: 714.5975730419159
  time_total_s: 714.5975730419159
  timestamp: 1616158142
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00124
  
[2m[36m(pid=49112)[0m GPU available: False, used: False
[2m[36m(pid=49112)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49112)[0m 
[2m[36m(pid=49112)[0m   | Name      | Type              | Params
[2m[36m(pid=49112)[0m ------------------------------------------------
[2m[36m(pid=49112)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49112)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49112)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49112)[0m ------------------------------------------------
[2m[36m(pid=49112)[0m 8.7 K     Trainable params
[2m[36m(pid=49112)[0m 0         Non-trainable params
[2m[36m(pid=49112)[0m 8.7 K     Total params
[2m[36m(pid=40014)[0m time to fit was 147.84854578971863
Result for _inner_e8deb_00123:
  auc: 0.8563506245613098
  date: 2021-03-19_13-49-05
  done: false
  experiment_id: 15620e1cec5b470187a65b691e8d3390
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 40014
  time_since_restore: 866.0469052791595
  time_this_iter_s: 866.0469052791595
  time_total_s: 866.0469052791595
  timestamp: 1616158145
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00123
  
[2m[36m(pid=40014)[0m Finished run with seed 0 - lr 0.001 - sec_lr 2 - bs 256 - mean val auc: 0.8563506245613098
Result for _inner_e8deb_00123:
  auc: 0.8563506245613098
  date: 2021-03-19_13-49-05
  done: true
  experiment_id: 15620e1cec5b470187a65b691e8d3390
  experiment_tag: 123_batch_size=256,eta=0.0,lr=0.001,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 40014
  time_since_restore: 866.0469052791595
  time_this_iter_s: 866.0469052791595
  time_total_s: 866.0469052791595
  timestamp: 1616158145
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00123
  
[2m[36m(pid=15253)[0m Starting run with seed 0 - lr 1 - sec_lr 2 - bs 64
[2m[36m(pid=15253)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15253)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=15253)[0m GPU available: False, used: False
[2m[36m(pid=15253)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=15253)[0m 
[2m[36m(pid=15253)[0m   | Name      | Type              | Params
[2m[36m(pid=15253)[0m ------------------------------------------------
[2m[36m(pid=15253)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=15253)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=15253)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=15253)[0m ------------------------------------------------
[2m[36m(pid=15253)[0m 8.7 K     Trainable params
[2m[36m(pid=15253)[0m 0         Non-trainable params
[2m[36m(pid=15253)[0m 8.7 K     Total params
[2m[36m(pid=15253)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=15253)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=15253)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=15253)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49065)[0m time to fit was 267.94898438453674
[2m[36m(pid=15253)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=15253)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49065)[0m GPU available: False, used: False
[2m[36m(pid=49065)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49065)[0m 
[2m[36m(pid=49065)[0m   | Name      | Type              | Params
[2m[36m(pid=49065)[0m ------------------------------------------------
[2m[36m(pid=49065)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49065)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49065)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49065)[0m ------------------------------------------------
[2m[36m(pid=49065)[0m 8.7 K     Trainable params
[2m[36m(pid=49065)[0m 0         Non-trainable params
[2m[36m(pid=49065)[0m 8.7 K     Total params
[2m[36m(pid=15400)[0m Starting run with seed 0 - lr 1 - sec_lr 2 - bs 128
[2m[36m(pid=15400)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15400)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=15400)[0m GPU available: False, used: False
[2m[36m(pid=15400)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=15400)[0m 
[2m[36m(pid=15400)[0m   | Name      | Type              | Params
[2m[36m(pid=15400)[0m ------------------------------------------------
[2m[36m(pid=15400)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=15400)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=15400)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=15400)[0m ------------------------------------------------
[2m[36m(pid=15400)[0m 8.7 K     Trainable params
[2m[36m(pid=15400)[0m 0         Non-trainable params
[2m[36m(pid=15400)[0m 8.7 K     Total params
[2m[36m(pid=15400)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=15400)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=15400)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=15400)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=15400)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=15400)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8169)[0m time to fit was 60.011372089385986
[2m[36m(pid=8169)[0m GPU available: False, used: False
[2m[36m(pid=8169)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8169)[0m 
[2m[36m(pid=8169)[0m   | Name      | Type              | Params
[2m[36m(pid=8169)[0m ------------------------------------------------
[2m[36m(pid=8169)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8169)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8169)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8169)[0m ------------------------------------------------
[2m[36m(pid=8169)[0m 8.7 K     Trainable params
[2m[36m(pid=8169)[0m 0         Non-trainable params
[2m[36m(pid=8169)[0m 8.7 K     Total params
[2m[36m(pid=23636)[0m time to fit was 1073.7290184497833
[2m[36m(pid=23636)[0m GPU available: False, used: False
[2m[36m(pid=23636)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23636)[0m 
[2m[36m(pid=23636)[0m   | Name      | Type              | Params
[2m[36m(pid=23636)[0m ------------------------------------------------
[2m[36m(pid=23636)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23636)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23636)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23636)[0m ------------------------------------------------
[2m[36m(pid=23636)[0m 8.7 K     Trainable params
[2m[36m(pid=23636)[0m 0         Non-trainable params
[2m[36m(pid=23636)[0m 8.7 K     Total params
[2m[36m(pid=4193)[0m time to fit was 77.65463376045227
[2m[36m(pid=4193)[0m GPU available: False, used: False
[2m[36m(pid=4193)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=4193)[0m 
[2m[36m(pid=4193)[0m   | Name      | Type              | Params
[2m[36m(pid=4193)[0m ------------------------------------------------
[2m[36m(pid=4193)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=4193)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=4193)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=4193)[0m ------------------------------------------------
[2m[36m(pid=4193)[0m 8.7 K     Trainable params
[2m[36m(pid=4193)[0m 0         Non-trainable params
[2m[36m(pid=4193)[0m 8.7 K     Total params
[2m[36m(pid=25581)[0m time to fit was 211.65075182914734
[2m[36m(pid=25581)[0m GPU available: False, used: False
[2m[36m(pid=25581)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=25581)[0m 
[2m[36m(pid=25581)[0m   | Name      | Type              | Params
[2m[36m(pid=25581)[0m ------------------------------------------------
[2m[36m(pid=25581)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=25581)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=25581)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=25581)[0m ------------------------------------------------
[2m[36m(pid=25581)[0m 8.7 K     Trainable params
[2m[36m(pid=25581)[0m 0         Non-trainable params
[2m[36m(pid=25581)[0m 8.7 K     Total params
[2m[36m(pid=1105)[0m time to fit was 243.65803384780884
[2m[36m(pid=1105)[0m GPU available: False, used: False
[2m[36m(pid=1105)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=1105)[0m 
[2m[36m(pid=1105)[0m   | Name      | Type              | Params
[2m[36m(pid=1105)[0m ------------------------------------------------
[2m[36m(pid=1105)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=1105)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=1105)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=1105)[0m ------------------------------------------------
[2m[36m(pid=1105)[0m 8.7 K     Trainable params
[2m[36m(pid=1105)[0m 0         Non-trainable params
[2m[36m(pid=1105)[0m 8.7 K     Total params
[2m[36m(pid=8169)[0m time to fit was 58.343215465545654
[2m[36m(pid=8169)[0m GPU available: False, used: False
[2m[36m(pid=8169)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8169)[0m 
[2m[36m(pid=8169)[0m   | Name      | Type              | Params
[2m[36m(pid=8169)[0m ------------------------------------------------
[2m[36m(pid=8169)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8169)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8169)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8169)[0m ------------------------------------------------
[2m[36m(pid=8169)[0m 8.7 K     Trainable params
[2m[36m(pid=8169)[0m 0         Non-trainable params
[2m[36m(pid=8169)[0m 8.7 K     Total params
[2m[36m(pid=3723)[0m time to fit was 507.2514297962189
[2m[36m(pid=3723)[0m GPU available: False, used: False
[2m[36m(pid=3723)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=3723)[0m 
[2m[36m(pid=3723)[0m   | Name      | Type              | Params
[2m[36m(pid=3723)[0m ------------------------------------------------
[2m[36m(pid=3723)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=3723)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=3723)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=3723)[0m ------------------------------------------------
[2m[36m(pid=3723)[0m 8.7 K     Trainable params
[2m[36m(pid=3723)[0m 0         Non-trainable params
[2m[36m(pid=3723)[0m 8.7 K     Total params
[2m[36m(pid=2917)[0m time to fit was 179.11815977096558
[2m[36m(pid=2917)[0m GPU available: False, used: False
[2m[36m(pid=2917)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=2917)[0m 
[2m[36m(pid=2917)[0m   | Name      | Type              | Params
[2m[36m(pid=2917)[0m ------------------------------------------------
[2m[36m(pid=2917)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=2917)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=2917)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=2917)[0m ------------------------------------------------
[2m[36m(pid=2917)[0m 8.7 K     Trainable params
[2m[36m(pid=2917)[0m 0         Non-trainable params
[2m[36m(pid=2917)[0m 8.7 K     Total params
[2m[36m(pid=4193)[0m time to fit was 77.4373049736023
Result for _inner_e8deb_00133:
  auc: 0.9110842704772949
  date: 2021-03-19_13-50-47
  done: false
  experiment_id: 85030719966a4b8d88fa41b79fb49380
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 4193
  time_since_restore: 418.0597941875458
  time_this_iter_s: 418.0597941875458
  time_total_s: 418.0597941875458
  timestamp: 1616158247
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00133
  
[2m[36m(pid=4193)[0m Finished run with seed 0 - lr 0.1 - sec_lr 2 - bs 256 - mean val auc: 0.9110842704772949
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 139/180 (1 PENDING, 26 RUNNING, 112 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00105 | RUNNING    |       |           32 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00116 | RUNNING    |       |           64 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00138 | PENDING    |       |          256 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 119 more trials not shown (16 RUNNING, 102 TERMINATED)


Result for _inner_e8deb_00133:
  auc: 0.9110842704772949
  date: 2021-03-19_13-50-47
  done: true
  experiment_id: 85030719966a4b8d88fa41b79fb49380
  experiment_tag: 133_batch_size=256,eta=0.0,lr=0.1,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 4193
  time_since_restore: 418.0597941875458
  time_this_iter_s: 418.0597941875458
  time_total_s: 418.0597941875458
  timestamp: 1616158247
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00133
  
[2m[36m(pid=18929)[0m Starting run with seed 0 - lr 1 - sec_lr 2 - bs 256
[2m[36m(pid=18929)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=18929)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=18929)[0m GPU available: False, used: False
[2m[36m(pid=18929)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=18929)[0m 
[2m[36m(pid=18929)[0m   | Name      | Type              | Params
[2m[36m(pid=18929)[0m ------------------------------------------------
[2m[36m(pid=18929)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=18929)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=18929)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=18929)[0m ------------------------------------------------
[2m[36m(pid=18929)[0m 8.7 K     Trainable params
[2m[36m(pid=18929)[0m 0         Non-trainable params
[2m[36m(pid=18929)[0m 8.7 K     Total params
[2m[36m(pid=18929)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=18929)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=18929)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=18929)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=18929)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=18929)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49065)[0m time to fit was 112.03518438339233
[2m[36m(pid=49065)[0m GPU available: False, used: False
[2m[36m(pid=49065)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49065)[0m 
[2m[36m(pid=49065)[0m   | Name      | Type              | Params
[2m[36m(pid=49065)[0m ------------------------------------------------
[2m[36m(pid=49065)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49065)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49065)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49065)[0m ------------------------------------------------
[2m[36m(pid=49065)[0m 8.7 K     Trainable params
[2m[36m(pid=49065)[0m 0         Non-trainable params
[2m[36m(pid=49065)[0m 8.7 K     Total params
[2m[36m(pid=49112)[0m time to fit was 126.71689248085022
[2m[36m(pid=49112)[0m GPU available: False, used: False
[2m[36m(pid=49112)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49112)[0m 
[2m[36m(pid=49112)[0m   | Name      | Type              | Params
[2m[36m(pid=49112)[0m ------------------------------------------------
[2m[36m(pid=49112)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49112)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49112)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49112)[0m ------------------------------------------------
[2m[36m(pid=49112)[0m 8.7 K     Trainable params
[2m[36m(pid=49112)[0m 0         Non-trainable params
[2m[36m(pid=49112)[0m 8.7 K     Total params
[2m[36m(pid=8169)[0m time to fit was 56.7173433303833
Result for _inner_e8deb_00134:
  auc: 0.9119826316833496
  date: 2021-03-19_13-51-16
  done: false
  experiment_id: 84dee81840e64d2999ab6438c96af542
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8169
  time_since_restore: 322.2226040363312
  time_this_iter_s: 322.2226040363312
  time_total_s: 322.2226040363312
  timestamp: 1616158276
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00134
  
[2m[36m(pid=8169)[0m Finished run with seed 0 - lr 0.1 - sec_lr 2 - bs 512 - mean val auc: 0.9119826316833496
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 140/180 (1 PENDING, 26 RUNNING, 113 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    |       |           32 |     0 | 0.001 |    0.001 |        |                  |          |
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00105 | RUNNING    |       |           32 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00116 | RUNNING    |       |           64 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00139 | PENDING    |       |          512 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 120 more trials not shown (16 RUNNING, 103 TERMINATED)


Result for _inner_e8deb_00134:
  auc: 0.9119826316833496
  date: 2021-03-19_13-51-16
  done: true
  experiment_id: 84dee81840e64d2999ab6438c96af542
  experiment_tag: 134_batch_size=512,eta=0.0,lr=0.1,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8169
  time_since_restore: 322.2226040363312
  time_this_iter_s: 322.2226040363312
  time_total_s: 322.2226040363312
  timestamp: 1616158276
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00134
  
2021-03-19 13:51:16,835	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff3bf25fe301000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=19780)[0m Starting run with seed 0 - lr 1 - sec_lr 2 - bs 512
[2m[36m(pid=19780)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=19780)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=19780)[0m GPU available: False, used: False
[2m[36m(pid=19780)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=19780)[0m 
[2m[36m(pid=19780)[0m   | Name      | Type              | Params
[2m[36m(pid=19780)[0m ------------------------------------------------
[2m[36m(pid=19780)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=19780)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=19780)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=19780)[0m ------------------------------------------------
[2m[36m(pid=19780)[0m 8.7 K     Trainable params
[2m[36m(pid=19780)[0m 0         Non-trainable params
[2m[36m(pid=19780)[0m 8.7 K     Total params
[2m[36m(pid=19780)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=19780)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=19780)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=19780)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=19780)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=19780)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48915)[0m time to fit was 317.19940161705017
[2m[36m(pid=48915)[0m GPU available: False, used: False
[2m[36m(pid=48915)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48915)[0m 
[2m[36m(pid=48915)[0m   | Name      | Type              | Params
[2m[36m(pid=48915)[0m ------------------------------------------------
[2m[36m(pid=48915)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48915)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48915)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48915)[0m ------------------------------------------------
[2m[36m(pid=48915)[0m 8.7 K     Trainable params
[2m[36m(pid=48915)[0m 0         Non-trainable params
[2m[36m(pid=48915)[0m 8.7 K     Total params
[2m[36m(pid=15400)[0m time to fit was 144.32682466506958
[2m[36m(pid=15400)[0m GPU available: False, used: False
[2m[36m(pid=15400)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=15400)[0m 
[2m[36m(pid=15400)[0m   | Name      | Type              | Params
[2m[36m(pid=15400)[0m ------------------------------------------------
[2m[36m(pid=15400)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=15400)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=15400)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=15400)[0m ------------------------------------------------
[2m[36m(pid=15400)[0m 8.7 K     Trainable params
[2m[36m(pid=15400)[0m 0         Non-trainable params
[2m[36m(pid=15400)[0m 8.7 K     Total params
[2m[36m(pid=18929)[0m time to fit was 78.20442724227905
[2m[36m(pid=18929)[0m GPU available: False, used: False
[2m[36m(pid=18929)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=18929)[0m 
[2m[36m(pid=18929)[0m   | Name      | Type              | Params
[2m[36m(pid=18929)[0m ------------------------------------------------
[2m[36m(pid=18929)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=18929)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=18929)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=18929)[0m ------------------------------------------------
[2m[36m(pid=18929)[0m 8.7 K     Trainable params
[2m[36m(pid=18929)[0m 0         Non-trainable params
[2m[36m(pid=18929)[0m 8.7 K     Total params
[2m[36m(pid=19780)[0m time to fit was 55.31062722206116
[2m[36m(pid=19780)[0m GPU available: False, used: False
[2m[36m(pid=19780)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=19780)[0m 
[2m[36m(pid=19780)[0m   | Name      | Type              | Params
[2m[36m(pid=19780)[0m ------------------------------------------------
[2m[36m(pid=19780)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=19780)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=19780)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=19780)[0m ------------------------------------------------
[2m[36m(pid=19780)[0m 8.7 K     Trainable params
[2m[36m(pid=19780)[0m 0         Non-trainable params
[2m[36m(pid=19780)[0m 8.7 K     Total params
[2m[36m(pid=35861)[0m time to fit was 1780.781361579895
Result for _inner_e8deb_00000:
  auc: 0.9087842702865601
  date: 2021-03-19_13-52-44
  done: false
  experiment_id: 6634d506cfe24106a15c4c9d9b666685
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35861
  time_since_restore: 9052.732978582382
  time_this_iter_s: 9052.732978582382
  time_total_s: 9052.732978582382
  timestamp: 1616158364
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00000
  
[2m[36m(pid=35861)[0m Finished run with seed 0 - lr 0.001 - sec_lr 0.001 - bs 32 - mean val auc: 0.9087842702865601
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 141/180 (1 PENDING, 26 RUNNING, 114 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | RUNNING    | 145.101.32.82:35861 |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |                     |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |                     |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |                     |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00105 | RUNNING    |                     |           32 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |                     |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |                     |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00116 | RUNNING    |                     |           64 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00140 | PENDING    |                     |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 121 more trials not shown (16 RUNNING, 104 TERMINATED)


Result for _inner_e8deb_00000:
  auc: 0.9087842702865601
  date: 2021-03-19_13-52-44
  done: true
  experiment_id: 6634d506cfe24106a15c4c9d9b666685
  experiment_tag: 0_batch_size=32,eta=0.0,lr=0.001,sec_lr=0.001
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35861
  time_since_restore: 9052.732978582382
  time_this_iter_s: 9052.732978582382
  time_total_s: 9052.732978582382
  timestamp: 1616158364
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00000
  
[2m[36m(pid=2917)[0m time to fit was 126.68787336349487
[2m[36m(pid=2917)[0m GPU available: False, used: False
[2m[36m(pid=2917)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=2917)[0m 
[2m[36m(pid=2917)[0m   | Name      | Type              | Params
[2m[36m(pid=2917)[0m ------------------------------------------------
[2m[36m(pid=2917)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=2917)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=2917)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=2917)[0m ------------------------------------------------
[2m[36m(pid=2917)[0m 8.7 K     Trainable params
[2m[36m(pid=2917)[0m 0         Non-trainable params
[2m[36m(pid=2917)[0m 8.7 K     Total params
[2m[36m(pid=22604)[0m Starting run with seed 0 - lr 2 - sec_lr 2 - bs 32
[2m[36m(pid=22604)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=22604)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=22604)[0m GPU available: False, used: False
[2m[36m(pid=22604)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=22604)[0m 
[2m[36m(pid=22604)[0m   | Name      | Type              | Params
[2m[36m(pid=22604)[0m ------------------------------------------------
[2m[36m(pid=22604)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=22604)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=22604)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=22604)[0m ------------------------------------------------
[2m[36m(pid=22604)[0m 8.7 K     Trainable params
[2m[36m(pid=22604)[0m 0         Non-trainable params
[2m[36m(pid=22604)[0m 8.7 K     Total params
[2m[36m(pid=22604)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=22604)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=22604)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=22604)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=22604)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=22604)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48812)[0m time to fit was 451.662335395813
[2m[36m(pid=48812)[0m GPU available: False, used: False
[2m[36m(pid=48812)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48812)[0m 
[2m[36m(pid=48812)[0m   | Name      | Type              | Params
[2m[36m(pid=48812)[0m ------------------------------------------------
[2m[36m(pid=48812)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48812)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48812)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48812)[0m ------------------------------------------------
[2m[36m(pid=48812)[0m 8.7 K     Trainable params
[2m[36m(pid=48812)[0m 0         Non-trainable params
[2m[36m(pid=48812)[0m 8.7 K     Total params
[2m[36m(pid=49065)[0m time to fit was 111.28716349601746
[2m[36m(pid=49065)[0m Finished run with seed 0 - lr 0.01 - sec_lr 2 - bs 256 - mean val auc: 0.9089795351028442
Result for _inner_e8deb_00128:
  auc: 0.9089795351028442
  date: 2021-03-19_13-52-59
  done: false
  experiment_id: ddd4811805574373a55e165ea064bb84
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 49065
  time_since_restore: 800.1103510856628
  time_this_iter_s: 800.1103510856628
  time_total_s: 800.1103510856628
  timestamp: 1616158379
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00128
  
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 142/180 (1 PENDING, 26 RUNNING, 115 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00105 | RUNNING    |       |           32 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00116 | RUNNING    |       |           64 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00141 | PENDING    |       |           64 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 122 more trials not shown (16 RUNNING, 105 TERMINATED)


Result for _inner_e8deb_00128:
  auc: 0.9089795351028442
  date: 2021-03-19_13-52-59
  done: true
  experiment_id: ddd4811805574373a55e165ea064bb84
  experiment_tag: 128_batch_size=256,eta=0.0,lr=0.01,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 49065
  time_since_restore: 800.1103510856628
  time_this_iter_s: 800.1103510856628
  time_total_s: 800.1103510856628
  timestamp: 1616158379
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00128
  
[2m[36m(pid=49112)[0m time to fit was 114.51319336891174
Result for _inner_e8deb_00129:
  auc: 0.9090599417686462
  date: 2021-03-19_13-53-04
  done: false
  experiment_id: 56119ae82ad348e48596a4cefc68a233
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 49112
  time_since_restore: 799.4762272834778
  time_this_iter_s: 799.4762272834778
  time_total_s: 799.4762272834778
  timestamp: 1616158384
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00129
  
[2m[36m(pid=49112)[0m Finished run with seed 0 - lr 0.01 - sec_lr 2 - bs 512 - mean val auc: 0.9090599417686462
Result for _inner_e8deb_00129:
  auc: 0.9090599417686462
  date: 2021-03-19_13-53-04
  done: true
  experiment_id: 56119ae82ad348e48596a4cefc68a233
  experiment_tag: 129_batch_size=512,eta=0.0,lr=0.01,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 49112
  time_since_restore: 799.4762272834778
  time_this_iter_s: 799.4762272834778
  time_total_s: 799.4762272834778
  timestamp: 1616158384
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00129
  
[2m[36m(pid=23089)[0m Starting run with seed 0 - lr 2 - sec_lr 2 - bs 64
[2m[36m(pid=23089)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=23089)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=23089)[0m GPU available: False, used: False
[2m[36m(pid=23089)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23089)[0m 
[2m[36m(pid=23089)[0m   | Name      | Type              | Params
[2m[36m(pid=23089)[0m ------------------------------------------------
[2m[36m(pid=23089)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23089)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23089)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23089)[0m ------------------------------------------------
[2m[36m(pid=23089)[0m 8.7 K     Trainable params
[2m[36m(pid=23089)[0m 0         Non-trainable params
[2m[36m(pid=23089)[0m 8.7 K     Total params
[2m[36m(pid=23089)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=23089)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8639)[0m time to fit was 425.9249527454376
[2m[36m(pid=8639)[0m GPU available: False, used: False
[2m[36m(pid=8639)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8639)[0m 
[2m[36m(pid=8639)[0m   | Name      | Type              | Params
[2m[36m(pid=8639)[0m ------------------------------------------------
[2m[36m(pid=8639)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8639)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8639)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8639)[0m ------------------------------------------------
[2m[36m(pid=8639)[0m 8.7 K     Trainable params
[2m[36m(pid=8639)[0m 0         Non-trainable params
[2m[36m(pid=8639)[0m 8.7 K     Total params
[2m[36m(pid=23089)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=23089)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23089)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=23089)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23225)[0m Starting run with seed 0 - lr 2 - sec_lr 2 - bs 128
[2m[36m(pid=23225)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=23225)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=23225)[0m GPU available: False, used: False
[2m[36m(pid=23225)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23225)[0m 
[2m[36m(pid=23225)[0m   | Name      | Type              | Params
[2m[36m(pid=23225)[0m ------------------------------------------------
[2m[36m(pid=23225)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23225)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23225)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23225)[0m ------------------------------------------------
[2m[36m(pid=23225)[0m 8.7 K     Trainable params
[2m[36m(pid=23225)[0m 0         Non-trainable params
[2m[36m(pid=23225)[0m 8.7 K     Total params
[2m[36m(pid=23225)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=23225)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23225)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=23225)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23225)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=23225)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=19780)[0m time to fit was 60.94313931465149
[2m[36m(pid=19780)[0m GPU available: False, used: False
[2m[36m(pid=19780)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=19780)[0m 
[2m[36m(pid=19780)[0m   | Name      | Type              | Params
[2m[36m(pid=19780)[0m ------------------------------------------------
[2m[36m(pid=19780)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=19780)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=19780)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=19780)[0m ------------------------------------------------
[2m[36m(pid=19780)[0m 8.7 K     Trainable params
[2m[36m(pid=19780)[0m 0         Non-trainable params
[2m[36m(pid=19780)[0m 8.7 K     Total params
[2m[36m(pid=15253)[0m time to fit was 259.7408835887909
[2m[36m(pid=15253)[0m GPU available: False, used: False
[2m[36m(pid=15253)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=15253)[0m 
[2m[36m(pid=15253)[0m   | Name      | Type              | Params
[2m[36m(pid=15253)[0m ------------------------------------------------
[2m[36m(pid=15253)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=15253)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=15253)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=15253)[0m ------------------------------------------------
[2m[36m(pid=15253)[0m 8.7 K     Trainable params
[2m[36m(pid=15253)[0m 0         Non-trainable params
[2m[36m(pid=15253)[0m 8.7 K     Total params
[2m[36m(pid=8457)[0m time to fit was 548.5176568031311
[2m[36m(pid=8457)[0m GPU available: False, used: False
[2m[36m(pid=8457)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8457)[0m 
[2m[36m(pid=8457)[0m   | Name      | Type              | Params
[2m[36m(pid=8457)[0m ------------------------------------------------
[2m[36m(pid=8457)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8457)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8457)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8457)[0m ------------------------------------------------
[2m[36m(pid=8457)[0m 8.7 K     Trainable params
[2m[36m(pid=8457)[0m 0         Non-trainable params
[2m[36m(pid=8457)[0m 8.7 K     Total params
[2m[36m(pid=25102)[0m time to fit was 1462.569714307785
[2m[36m(pid=25102)[0m GPU available: False, used: False
[2m[36m(pid=25102)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=25102)[0m 
[2m[36m(pid=25102)[0m   | Name      | Type              | Params
[2m[36m(pid=25102)[0m ------------------------------------------------
[2m[36m(pid=25102)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=25102)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=25102)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=25102)[0m ------------------------------------------------
[2m[36m(pid=25102)[0m 8.7 K     Trainable params
[2m[36m(pid=25102)[0m 0         Non-trainable params
[2m[36m(pid=25102)[0m 8.7 K     Total params
[2m[36m(pid=15400)[0m time to fit was 142.65377354621887
[2m[36m(pid=15400)[0m GPU available: False, used: False
[2m[36m(pid=15400)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=15400)[0m 
[2m[36m(pid=15400)[0m   | Name      | Type              | Params
[2m[36m(pid=15400)[0m ------------------------------------------------
[2m[36m(pid=15400)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=15400)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=15400)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=15400)[0m ------------------------------------------------
[2m[36m(pid=15400)[0m 8.7 K     Trainable params
[2m[36m(pid=15400)[0m 0         Non-trainable params
[2m[36m(pid=15400)[0m 8.7 K     Total params
[2m[36m(pid=1105)[0m time to fit was 243.50819730758667
[2m[36m(pid=1105)[0m GPU available: False, used: False
[2m[36m(pid=1105)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=1105)[0m 
[2m[36m(pid=1105)[0m   | Name      | Type              | Params
[2m[36m(pid=1105)[0m ------------------------------------------------
[2m[36m(pid=1105)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=1105)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=1105)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=1105)[0m ------------------------------------------------
[2m[36m(pid=1105)[0m 8.7 K     Trainable params
[2m[36m(pid=1105)[0m 0         Non-trainable params
[2m[36m(pid=1105)[0m 8.7 K     Total params
[2m[36m(pid=18929)[0m time to fit was 113.46233797073364
[2m[36m(pid=18929)[0m GPU available: False, used: False
[2m[36m(pid=18929)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=18929)[0m 
[2m[36m(pid=18929)[0m   | Name      | Type              | Params
[2m[36m(pid=18929)[0m ------------------------------------------------
[2m[36m(pid=18929)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=18929)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=18929)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=18929)[0m ------------------------------------------------
[2m[36m(pid=18929)[0m 8.7 K     Trainable params
[2m[36m(pid=18929)[0m 0         Non-trainable params
[2m[36m(pid=18929)[0m 8.7 K     Total params
[2m[36m(pid=48915)[0m time to fit was 164.49536275863647
[2m[36m(pid=48915)[0m GPU available: False, used: False
[2m[36m(pid=48915)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48915)[0m 
[2m[36m(pid=48915)[0m   | Name      | Type              | Params
[2m[36m(pid=48915)[0m ------------------------------------------------
[2m[36m(pid=48915)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48915)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48915)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48915)[0m ------------------------------------------------
[2m[36m(pid=48915)[0m 8.7 K     Trainable params
[2m[36m(pid=48915)[0m 0         Non-trainable params
[2m[36m(pid=48915)[0m 8.7 K     Total params
[2m[36m(pid=19780)[0m time to fit was 53.257296085357666
[2m[36m(pid=19780)[0m GPU available: False, used: False
[2m[36m(pid=19780)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=19780)[0m 
[2m[36m(pid=19780)[0m   | Name      | Type              | Params
[2m[36m(pid=19780)[0m ------------------------------------------------
[2m[36m(pid=19780)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=19780)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=19780)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=19780)[0m ------------------------------------------------
[2m[36m(pid=19780)[0m 8.7 K     Trainable params
[2m[36m(pid=19780)[0m 0         Non-trainable params
[2m[36m(pid=19780)[0m 8.7 K     Total params
[2m[36m(pid=2917)[0m time to fit was 114.01440691947937
Result for _inner_e8deb_00132:
  auc: 0.9101737380027771
  date: 2021-03-19_13-54-45
  done: false
  experiment_id: efe6f8698b654bc19ec72b35566fb340
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 2917
  time_since_restore: 695.8894255161285
  time_this_iter_s: 695.8894255161285
  time_total_s: 695.8894255161285
  timestamp: 1616158485
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00132
  
[2m[36m(pid=2917)[0m Finished run with seed 0 - lr 0.1 - sec_lr 2 - bs 128 - mean val auc: 0.9101737380027771
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 144/180 (1 PENDING, 26 RUNNING, 117 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00105 | RUNNING    |       |           32 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00116 | RUNNING    |       |           64 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00143 | PENDING    |       |          256 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 124 more trials not shown (16 RUNNING, 107 TERMINATED)


Result for _inner_e8deb_00132:
  auc: 0.9101737380027771
  date: 2021-03-19_13-54-45
  done: true
  experiment_id: efe6f8698b654bc19ec72b35566fb340
  experiment_tag: 132_batch_size=128,eta=0.0,lr=0.1,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 2917
  time_since_restore: 695.8894255161285
  time_this_iter_s: 695.8894255161285
  time_total_s: 695.8894255161285
  timestamp: 1616158485
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00132
  
2021-03-19 13:54:47,363	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff2ebd345001000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=26127)[0m Starting run with seed 0 - lr 2 - sec_lr 2 - bs 256
[2m[36m(pid=26127)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=26127)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=26127)[0m GPU available: False, used: False
[2m[36m(pid=26127)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26127)[0m 
[2m[36m(pid=26127)[0m   | Name      | Type              | Params
[2m[36m(pid=26127)[0m ------------------------------------------------
[2m[36m(pid=26127)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26127)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26127)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26127)[0m ------------------------------------------------
[2m[36m(pid=26127)[0m 8.7 K     Trainable params
[2m[36m(pid=26127)[0m 0         Non-trainable params
[2m[36m(pid=26127)[0m 8.7 K     Total params
[2m[36m(pid=26127)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=26127)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=26127)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=26127)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=26127)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=26127)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=52747)[0m time to fit was 425.48078417778015
[2m[36m(pid=52747)[0m GPU available: False, used: False
[2m[36m(pid=52747)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=52747)[0m 
[2m[36m(pid=52747)[0m   | Name      | Type              | Params
[2m[36m(pid=52747)[0m ------------------------------------------------
[2m[36m(pid=52747)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=52747)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=52747)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=52747)[0m ------------------------------------------------
[2m[36m(pid=52747)[0m 8.7 K     Trainable params
[2m[36m(pid=52747)[0m 0         Non-trainable params
[2m[36m(pid=52747)[0m 8.7 K     Total params
[2m[36m(pid=25581)[0m time to fit was 341.3757071495056
Result for _inner_e8deb_00116:
  auc: 0.608674144744873
  date: 2021-03-19_13-55-37
  done: false
  experiment_id: d406260724484293b093bed73c456356
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 25581
  time_since_restore: 1764.874766588211
  time_this_iter_s: 1764.874766588211
  time_total_s: 1764.874766588211
  timestamp: 1616158537
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00116
  
[2m[36m(pid=25581)[0m Finished run with seed 0 - lr 5 - sec_lr 1 - bs 64 - mean val auc: 0.608674144744873
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 145/180 (1 PENDING, 26 RUNNING, 118 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |                     |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |                     |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |                     |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00105 | RUNNING    |                     |           32 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |                     |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |                     |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00116 | RUNNING    | 145.101.32.82:25581 |           64 |     0 | 5     |    1     |      1 |          1764.87 | 0.608674 |
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00144 | PENDING    |                     |          512 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 125 more trials not shown (16 RUNNING, 108 TERMINATED)


Result for _inner_e8deb_00116:
  auc: 0.608674144744873
  date: 2021-03-19_13-55-37
  done: true
  experiment_id: d406260724484293b093bed73c456356
  experiment_tag: 116_batch_size=64,eta=0.0,lr=5,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 25581
  time_since_restore: 1764.874766588211
  time_this_iter_s: 1764.874766588211
  time_total_s: 1764.874766588211
  timestamp: 1616158537
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00116
  
[2m[36m(pid=23225)[0m time to fit was 143.08840608596802
[2m[36m(pid=23225)[0m GPU available: False, used: False
[2m[36m(pid=23225)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23225)[0m 
[2m[36m(pid=23225)[0m   | Name      | Type              | Params
[2m[36m(pid=23225)[0m ------------------------------------------------
[2m[36m(pid=23225)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23225)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23225)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23225)[0m ------------------------------------------------
[2m[36m(pid=23225)[0m 8.7 K     Trainable params
[2m[36m(pid=23225)[0m 0         Non-trainable params
[2m[36m(pid=23225)[0m 8.7 K     Total params
[2m[36m(pid=19780)[0m time to fit was 82.73015832901001
[2m[36m(pid=19780)[0m GPU available: False, used: False
[2m[36m(pid=19780)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=19780)[0m 
[2m[36m(pid=19780)[0m   | Name      | Type              | Params
[2m[36m(pid=19780)[0m ------------------------------------------------
[2m[36m(pid=19780)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=19780)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=19780)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=19780)[0m ------------------------------------------------
[2m[36m(pid=19780)[0m 8.7 K     Trainable params
[2m[36m(pid=19780)[0m 0         Non-trainable params
[2m[36m(pid=19780)[0m 8.7 K     Total params
[2m[36m(pid=18929)[0m time to fit was 90.75694847106934
[2m[36m(pid=18929)[0m GPU available: False, used: False
[2m[36m(pid=18929)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=18929)[0m 
[2m[36m(pid=18929)[0m   | Name      | Type              | Params
[2m[36m(pid=18929)[0m ------------------------------------------------
[2m[36m(pid=18929)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=18929)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=18929)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=18929)[0m ------------------------------------------------
[2m[36m(pid=18929)[0m 8.7 K     Trainable params
[2m[36m(pid=18929)[0m 0         Non-trainable params
[2m[36m(pid=18929)[0m 8.7 K     Total params
[2m[36m(pid=27628)[0m Starting run with seed 0 - lr 2 - sec_lr 2 - bs 512
[2m[36m(pid=27628)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=27628)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=27628)[0m GPU available: False, used: False
[2m[36m(pid=27628)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27628)[0m 
[2m[36m(pid=27628)[0m   | Name      | Type              | Params
[2m[36m(pid=27628)[0m ------------------------------------------------
[2m[36m(pid=27628)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27628)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27628)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27628)[0m ------------------------------------------------
[2m[36m(pid=27628)[0m 8.7 K     Trainable params
[2m[36m(pid=27628)[0m 0         Non-trainable params
[2m[36m(pid=27628)[0m 8.7 K     Total params
[2m[36m(pid=27628)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=27628)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=27628)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=27628)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=27628)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=27628)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=47886)[0m time to fit was 565.8739371299744
[2m[36m(pid=47886)[0m GPU available: False, used: False
[2m[36m(pid=47886)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=47886)[0m 
[2m[36m(pid=47886)[0m   | Name      | Type              | Params
[2m[36m(pid=47886)[0m ------------------------------------------------
[2m[36m(pid=47886)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=47886)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=47886)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=47886)[0m ------------------------------------------------
[2m[36m(pid=47886)[0m 8.7 K     Trainable params
[2m[36m(pid=47886)[0m 0         Non-trainable params
[2m[36m(pid=47886)[0m 8.7 K     Total params
[2m[36m(pid=26127)[0m time to fit was 90.51601004600525
[2m[36m(pid=26127)[0m GPU available: False, used: False
[2m[36m(pid=26127)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26127)[0m 
[2m[36m(pid=26127)[0m   | Name      | Type              | Params
[2m[36m(pid=26127)[0m ------------------------------------------------
[2m[36m(pid=26127)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26127)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26127)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26127)[0m ------------------------------------------------
[2m[36m(pid=26127)[0m 8.7 K     Trainable params
[2m[36m(pid=26127)[0m 0         Non-trainable params
[2m[36m(pid=26127)[0m 8.7 K     Total params
[2m[36m(pid=19780)[0m time to fit was 55.03943395614624
Result for _inner_e8deb_00139:
  auc: 0.9114540815353394
  date: 2021-03-19_13-56-35
  done: false
  experiment_id: 82143ced46014fbd858b606c8466dab4
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 19780
  time_since_restore: 308.47974967956543
  time_this_iter_s: 308.47974967956543
  time_total_s: 308.47974967956543
  timestamp: 1616158595
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00139
  
[2m[36m(pid=19780)[0m Finished run with seed 0 - lr 1 - sec_lr 2 - bs 512 - mean val auc: 0.9114540815353394
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 146/180 (1 PENDING, 26 RUNNING, 119 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00105 | RUNNING    |       |           32 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00145 | PENDING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 126 more trials not shown (16 RUNNING, 109 TERMINATED)


Result for _inner_e8deb_00139:
  auc: 0.9114540815353394
  date: 2021-03-19_13-56-35
  done: true
  experiment_id: 82143ced46014fbd858b606c8466dab4
  experiment_tag: 139_batch_size=512,eta=0.0,lr=1,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 19780
  time_since_restore: 308.47974967956543
  time_this_iter_s: 308.47974967956543
  time_total_s: 308.47974967956543
  timestamp: 1616158595
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00139
  
[2m[36m(pid=29455)[0m Starting run with seed 0 - lr 5 - sec_lr 2 - bs 32
[2m[36m(pid=29455)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=29455)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=29455)[0m GPU available: False, used: False
[2m[36m(pid=29455)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29455)[0m 
[2m[36m(pid=29455)[0m   | Name      | Type              | Params
[2m[36m(pid=29455)[0m ------------------------------------------------
[2m[36m(pid=29455)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29455)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29455)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29455)[0m ------------------------------------------------
[2m[36m(pid=29455)[0m 8.7 K     Trainable params
[2m[36m(pid=29455)[0m 0         Non-trainable params
[2m[36m(pid=29455)[0m 8.7 K     Total params
[2m[36m(pid=29455)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=29455)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=29455)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=29455)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=29455)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=29455)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48915)[0m time to fit was 154.49614310264587
Result for _inner_e8deb_00127:
  auc: 0.9094670295715332
  date: 2021-03-19_13-56-49
  done: false
  experiment_id: 4068a9732e5d4c90a26b80c848993b01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 48915
  time_since_restore: 1035.69220495224
  time_this_iter_s: 1035.69220495224
  time_total_s: 1035.69220495224
  timestamp: 1616158609
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00127
  
[2m[36m(pid=48915)[0m Finished run with seed 0 - lr 0.01 - sec_lr 2 - bs 128 - mean val auc: 0.9094670295715332
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 147/180 (1 PENDING, 26 RUNNING, 120 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    |       |           64 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00105 | RUNNING    |       |           32 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00146 | PENDING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 127 more trials not shown (16 RUNNING, 110 TERMINATED)


Result for _inner_e8deb_00127:
  auc: 0.9094670295715332
  date: 2021-03-19_13-56-49
  done: true
  experiment_id: 4068a9732e5d4c90a26b80c848993b01
  experiment_tag: 127_batch_size=128,eta=0.0,lr=0.01,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 48915
  time_since_restore: 1035.69220495224
  time_this_iter_s: 1035.69220495224
  time_total_s: 1035.69220495224
  timestamp: 1616158609
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00127
  
[2m[36m(pid=27628)[0m time to fit was 69.02825212478638
[2m[36m(pid=27628)[0m GPU available: False, used: False
[2m[36m(pid=27628)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27628)[0m 
[2m[36m(pid=27628)[0m   | Name      | Type              | Params
[2m[36m(pid=27628)[0m ------------------------------------------------
[2m[36m(pid=27628)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27628)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27628)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27628)[0m ------------------------------------------------
[2m[36m(pid=27628)[0m 8.7 K     Trainable params
[2m[36m(pid=27628)[0m 0         Non-trainable params
[2m[36m(pid=27628)[0m 8.7 K     Total params
[2m[36m(pid=29932)[0m Starting run with seed 0 - lr 5 - sec_lr 2 - bs 64
[2m[36m(pid=29932)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=29932)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=29932)[0m GPU available: False, used: False
[2m[36m(pid=29932)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29932)[0m 
[2m[36m(pid=29932)[0m   | Name      | Type              | Params
[2m[36m(pid=29932)[0m ------------------------------------------------
[2m[36m(pid=29932)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29932)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29932)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29932)[0m ------------------------------------------------
[2m[36m(pid=29932)[0m 8.7 K     Trainable params
[2m[36m(pid=29932)[0m 0         Non-trainable params
[2m[36m(pid=29932)[0m 8.7 K     Total params
[2m[36m(pid=29932)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=29932)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=29932)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=29932)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=29932)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=29932)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23089)[0m time to fit was 240.93662309646606
[2m[36m(pid=23089)[0m GPU available: False, used: False
[2m[36m(pid=23089)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23089)[0m 
[2m[36m(pid=23089)[0m   | Name      | Type              | Params
[2m[36m(pid=23089)[0m ------------------------------------------------
[2m[36m(pid=23089)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23089)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23089)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23089)[0m ------------------------------------------------
[2m[36m(pid=23089)[0m 8.7 K     Trainable params
[2m[36m(pid=23089)[0m 0         Non-trainable params
[2m[36m(pid=23089)[0m 8.7 K     Total params
[2m[36m(pid=31106)[0m time to fit was 819.5447111129761
[2m[36m(pid=31106)[0m GPU available: False, used: False
[2m[36m(pid=31106)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31106)[0m 
[2m[36m(pid=31106)[0m   | Name      | Type              | Params
[2m[36m(pid=31106)[0m ------------------------------------------------
[2m[36m(pid=31106)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31106)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31106)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31106)[0m ------------------------------------------------
[2m[36m(pid=31106)[0m 8.7 K     Trainable params
[2m[36m(pid=31106)[0m 0         Non-trainable params
[2m[36m(pid=31106)[0m 8.7 K     Total params
[2m[36m(pid=15400)[0m time to fit was 194.08737635612488
[2m[36m(pid=15400)[0m GPU available: False, used: False
[2m[36m(pid=15400)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=15400)[0m 
[2m[36m(pid=15400)[0m   | Name      | Type              | Params
[2m[36m(pid=15400)[0m ------------------------------------------------
[2m[36m(pid=15400)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=15400)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=15400)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=15400)[0m ------------------------------------------------
[2m[36m(pid=15400)[0m 8.7 K     Trainable params
[2m[36m(pid=15400)[0m 0         Non-trainable params
[2m[36m(pid=15400)[0m 8.7 K     Total params
[2m[36m(pid=26407)[0m time to fit was 872.5232293605804
Result for _inner_e8deb_00091:
  auc: 0.8916362285614013
  date: 2021-03-19_13-57-25
  done: false
  experiment_id: 795c4c99fd254e8a941f0efe6d166743
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 26407
  time_since_restore: 3545.393042087555
  time_this_iter_s: 3545.393042087555
  time_total_s: 3545.393042087555
  timestamp: 1616158645
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00091
  
[2m[36m(pid=26407)[0m Finished run with seed 0 - lr 0.001 - sec_lr 1 - bs 64 - mean val auc: 0.8916362285614013
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 148/180 (1 PENDING, 26 RUNNING, 121 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00030 | RUNNING    |                     |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |                     |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |                     |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00091 | RUNNING    | 145.101.32.82:26407 |           64 |     0 | 0.001 |    1     |      1 |          3545.39 | 0.891636 |
| _inner_e8deb_00105 | RUNNING    |                     |           32 |     0 | 1     |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |                     |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |                     |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |                     |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00147 | PENDING    |                     |          128 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 128 more trials not shown (16 RUNNING, 111 TERMINATED)


Result for _inner_e8deb_00091:
  auc: 0.8916362285614013
  date: 2021-03-19_13-57-25
  done: true
  experiment_id: 795c4c99fd254e8a941f0efe6d166743
  experiment_tag: 91_batch_size=64,eta=0.0,lr=0.001,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 26407
  time_since_restore: 3545.393042087555
  time_this_iter_s: 3545.393042087555
  time_total_s: 3545.393042087555
  timestamp: 1616158645
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00091
  
[2m[36m(pid=3723)[0m time to fit was 426.2822413444519
Result for _inner_e8deb_00105:
  auc: 0.9080480337142944
  date: 2021-03-19_13-57-27
  done: false
  experiment_id: bdc4469abca04e969a3e6d8d0852109c
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 3723
  time_since_restore: 2599.6863408088684
  time_this_iter_s: 2599.6863408088684
  time_total_s: 2599.6863408088684
  timestamp: 1616158647
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00105
  
[2m[36m(pid=3723)[0m Finished run with seed 0 - lr 1 - sec_lr 1 - bs 32 - mean val auc: 0.9080480337142944
Result for _inner_e8deb_00105:
  auc: 0.9080480337142944
  date: 2021-03-19_13-57-27
  done: true
  experiment_id: bdc4469abca04e969a3e6d8d0852109c
  experiment_tag: 105_batch_size=32,eta=0.0,lr=1,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 3723
  time_since_restore: 2599.6863408088684
  time_this_iter_s: 2599.6863408088684
  time_total_s: 2599.6863408088684
  timestamp: 1616158647
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00105
  
[2m[36m(pid=23225)[0m time to fit was 111.40185236930847
[2m[36m(pid=23225)[0m GPU available: False, used: False
[2m[36m(pid=23225)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23225)[0m 
[2m[36m(pid=23225)[0m   | Name      | Type              | Params
[2m[36m(pid=23225)[0m ------------------------------------------------
[2m[36m(pid=23225)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23225)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23225)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23225)[0m ------------------------------------------------
[2m[36m(pid=23225)[0m 8.7 K     Trainable params
[2m[36m(pid=23225)[0m 0         Non-trainable params
[2m[36m(pid=23225)[0m 8.7 K     Total params
[2m[36m(pid=30947)[0m Starting run with seed 0 - lr 5 - sec_lr 2 - bs 128
[2m[36m(pid=30947)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30947)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=30947)[0m GPU available: False, used: False
[2m[36m(pid=30947)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=30947)[0m 
[2m[36m(pid=30947)[0m   | Name      | Type              | Params
[2m[36m(pid=30947)[0m ------------------------------------------------
[2m[36m(pid=30947)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=30947)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=30947)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=30947)[0m ------------------------------------------------
[2m[36m(pid=30947)[0m 8.7 K     Trainable params
[2m[36m(pid=30947)[0m 0         Non-trainable params
[2m[36m(pid=30947)[0m 8.7 K     Total params
[2m[36m(pid=30947)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=30947)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=30947)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=30947)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=30947)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=30947)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=18929)[0m time to fit was 115.05754113197327
[2m[36m(pid=18929)[0m GPU available: False, used: False
[2m[36m(pid=18929)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=18929)[0m 
[2m[36m(pid=18929)[0m   | Name      | Type              | Params
[2m[36m(pid=18929)[0m ------------------------------------------------
[2m[36m(pid=18929)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=18929)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=18929)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=18929)[0m ------------------------------------------------
[2m[36m(pid=18929)[0m 8.7 K     Trainable params
[2m[36m(pid=18929)[0m 0         Non-trainable params
[2m[36m(pid=18929)[0m 8.7 K     Total params
[2m[36m(pid=31015)[0m Starting run with seed 0 - lr 5 - sec_lr 2 - bs 256
[2m[36m(pid=31015)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=31015)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=31015)[0m GPU available: False, used: False
[2m[36m(pid=31015)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31015)[0m 
[2m[36m(pid=31015)[0m   | Name      | Type              | Params
[2m[36m(pid=31015)[0m ------------------------------------------------
[2m[36m(pid=31015)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31015)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31015)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31015)[0m ------------------------------------------------
[2m[36m(pid=31015)[0m 8.7 K     Trainable params
[2m[36m(pid=31015)[0m 0         Non-trainable params
[2m[36m(pid=31015)[0m 8.7 K     Total params
[2m[36m(pid=31015)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=31015)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=34577)[0m time to fit was 524.4836270809174
[2m[36m(pid=34577)[0m GPU available: False, used: False
[2m[36m(pid=34577)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=34577)[0m 
[2m[36m(pid=34577)[0m   | Name      | Type              | Params
[2m[36m(pid=34577)[0m ------------------------------------------------
[2m[36m(pid=34577)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=34577)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=34577)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=34577)[0m ------------------------------------------------
[2m[36m(pid=34577)[0m 8.7 K     Trainable params
[2m[36m(pid=34577)[0m 0         Non-trainable params
[2m[36m(pid=34577)[0m 8.7 K     Total params
[2m[36m(pid=31015)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=31015)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31015)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=31015)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=26127)[0m time to fit was 77.85094666481018
[2m[36m(pid=26127)[0m GPU available: False, used: False
[2m[36m(pid=26127)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26127)[0m 
[2m[36m(pid=26127)[0m   | Name      | Type              | Params
[2m[36m(pid=26127)[0m ------------------------------------------------
[2m[36m(pid=26127)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26127)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26127)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26127)[0m ------------------------------------------------
[2m[36m(pid=26127)[0m 8.7 K     Trainable params
[2m[36m(pid=26127)[0m 0         Non-trainable params
[2m[36m(pid=26127)[0m 8.7 K     Total params
[2m[36m(pid=15253)[0m time to fit was 298.21075415611267
[2m[36m(pid=15253)[0m GPU available: False, used: False
[2m[36m(pid=15253)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=15253)[0m 
[2m[36m(pid=15253)[0m   | Name      | Type              | Params
[2m[36m(pid=15253)[0m ------------------------------------------------
[2m[36m(pid=15253)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=15253)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=15253)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=15253)[0m ------------------------------------------------
[2m[36m(pid=15253)[0m 8.7 K     Trainable params
[2m[36m(pid=15253)[0m 0         Non-trainable params
[2m[36m(pid=15253)[0m 8.7 K     Total params
[2m[36m(pid=31015)[0m time to fit was 66.40151238441467
[2m[36m(pid=31015)[0m GPU available: False, used: False
[2m[36m(pid=31015)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31015)[0m 
[2m[36m(pid=31015)[0m   | Name      | Type              | Params
[2m[36m(pid=31015)[0m ------------------------------------------------
[2m[36m(pid=31015)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31015)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31015)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31015)[0m ------------------------------------------------
[2m[36m(pid=31015)[0m 8.7 K     Trainable params
[2m[36m(pid=31015)[0m 0         Non-trainable params
[2m[36m(pid=31015)[0m 8.7 K     Total params
[2m[36m(pid=1105)[0m time to fit was 281.16493344306946
[2m[36m(pid=1105)[0m GPU available: False, used: False
[2m[36m(pid=1105)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=1105)[0m 
[2m[36m(pid=1105)[0m   | Name      | Type              | Params
[2m[36m(pid=1105)[0m ------------------------------------------------
[2m[36m(pid=1105)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=1105)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=1105)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=1105)[0m ------------------------------------------------
[2m[36m(pid=1105)[0m 8.7 K     Trainable params
[2m[36m(pid=1105)[0m 0         Non-trainable params
[2m[36m(pid=1105)[0m 8.7 K     Total params
[2m[36m(pid=27628)[0m time to fit was 112.50704264640808
[2m[36m(pid=27628)[0m GPU available: False, used: False
[2m[36m(pid=27628)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27628)[0m 
[2m[36m(pid=27628)[0m   | Name      | Type              | Params
[2m[36m(pid=27628)[0m ------------------------------------------------
[2m[36m(pid=27628)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27628)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27628)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27628)[0m ------------------------------------------------
[2m[36m(pid=27628)[0m 8.7 K     Trainable params
[2m[36m(pid=27628)[0m 0         Non-trainable params
[2m[36m(pid=27628)[0m 8.7 K     Total params
[2m[36m(pid=18929)[0m time to fit was 85.2091932296753
Result for _inner_e8deb_00138:
  auc: 0.9108158946037292
  date: 2021-03-19_13-59-02
  done: false
  experiment_id: 6b209adaffa84cedb488721f1c79747e
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 18929
  time_since_restore: 484.02789068222046
  time_this_iter_s: 484.02789068222046
  time_total_s: 484.02789068222046
  timestamp: 1616158742
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00138
  
[2m[36m(pid=18929)[0m Finished run with seed 0 - lr 1 - sec_lr 2 - bs 256 - mean val auc: 0.9108158946037292
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 150/180 (1 PENDING, 26 RUNNING, 123 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00122 | RUNNING    |       |          128 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00149 | PENDING    |       |          512 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 130 more trials not shown (16 RUNNING, 113 TERMINATED)


Result for _inner_e8deb_00138:
  auc: 0.9108158946037292
  date: 2021-03-19_13-59-02
  done: true
  experiment_id: 6b209adaffa84cedb488721f1c79747e
  experiment_tag: 138_batch_size=256,eta=0.0,lr=1,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 18929
  time_since_restore: 484.02789068222046
  time_this_iter_s: 484.02789068222046
  time_total_s: 484.02789068222046
  timestamp: 1616158742
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00138
  
[2m[36m(pid=31029)[0m Starting run with seed 0 - lr 5 - sec_lr 2 - bs 512
[2m[36m(pid=31029)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=31029)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=31029)[0m GPU available: False, used: False
[2m[36m(pid=31029)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31029)[0m 
[2m[36m(pid=31029)[0m   | Name      | Type              | Params
[2m[36m(pid=31029)[0m ------------------------------------------------
[2m[36m(pid=31029)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31029)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31029)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31029)[0m ------------------------------------------------
[2m[36m(pid=31029)[0m 8.7 K     Trainable params
[2m[36m(pid=31029)[0m 0         Non-trainable params
[2m[36m(pid=31029)[0m 8.7 K     Total params
[2m[36m(pid=31029)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=31029)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31029)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=31029)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31029)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=31029)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=15400)[0m time to fit was 116.11153841018677
[2m[36m(pid=15400)[0m GPU available: False, used: False
[2m[36m(pid=15400)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=15400)[0m 
[2m[36m(pid=15400)[0m   | Name      | Type              | Params
[2m[36m(pid=15400)[0m ------------------------------------------------
[2m[36m(pid=15400)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=15400)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=15400)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=15400)[0m ------------------------------------------------
[2m[36m(pid=15400)[0m 8.7 K     Trainable params
[2m[36m(pid=15400)[0m 0         Non-trainable params
[2m[36m(pid=15400)[0m 8.7 K     Total params
[2m[36m(pid=30947)[0m time to fit was 112.86698293685913
[2m[36m(pid=30947)[0m GPU available: False, used: False
[2m[36m(pid=30947)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=30947)[0m 
[2m[36m(pid=30947)[0m   | Name      | Type              | Params
[2m[36m(pid=30947)[0m ------------------------------------------------
[2m[36m(pid=30947)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=30947)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=30947)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=30947)[0m ------------------------------------------------
[2m[36m(pid=30947)[0m 8.7 K     Trainable params
[2m[36m(pid=30947)[0m 0         Non-trainable params
[2m[36m(pid=30947)[0m 8.7 K     Total params
[2m[36m(pid=31015)[0m time to fit was 67.58214616775513
[2m[36m(pid=31015)[0m GPU available: False, used: False
[2m[36m(pid=31015)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31015)[0m 
[2m[36m(pid=31015)[0m   | Name      | Type              | Params
[2m[36m(pid=31015)[0m ------------------------------------------------
[2m[36m(pid=31015)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31015)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31015)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31015)[0m ------------------------------------------------
[2m[36m(pid=31015)[0m 8.7 K     Trainable params
[2m[36m(pid=31015)[0m 0         Non-trainable params
[2m[36m(pid=31015)[0m 8.7 K     Total params
[2m[36m(pid=27628)[0m time to fit was 71.12111496925354
[2m[36m(pid=27628)[0m GPU available: False, used: False
[2m[36m(pid=27628)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27628)[0m 
[2m[36m(pid=27628)[0m   | Name      | Type              | Params
[2m[36m(pid=27628)[0m ------------------------------------------------
[2m[36m(pid=27628)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27628)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27628)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27628)[0m ------------------------------------------------
[2m[36m(pid=27628)[0m 8.7 K     Trainable params
[2m[36m(pid=27628)[0m 0         Non-trainable params
[2m[36m(pid=27628)[0m 8.7 K     Total params
[2m[36m(pid=26127)[0m time to fit was 148.57864546775818
[2m[36m(pid=26127)[0m GPU available: False, used: False
[2m[36m(pid=26127)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26127)[0m 
[2m[36m(pid=26127)[0m   | Name      | Type              | Params
[2m[36m(pid=26127)[0m ------------------------------------------------
[2m[36m(pid=26127)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26127)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26127)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26127)[0m ------------------------------------------------
[2m[36m(pid=26127)[0m 8.7 K     Trainable params
[2m[36m(pid=26127)[0m 0         Non-trainable params
[2m[36m(pid=26127)[0m 8.7 K     Total params
[2m[36m(pid=29801)[0m time to fit was 1411.3558974266052
[2m[36m(pid=29801)[0m GPU available: False, used: False
[2m[36m(pid=29801)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29801)[0m 
[2m[36m(pid=29801)[0m   | Name      | Type              | Params
[2m[36m(pid=29801)[0m ------------------------------------------------
[2m[36m(pid=29801)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29801)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29801)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29801)[0m ------------------------------------------------
[2m[36m(pid=29801)[0m 8.7 K     Trainable params
[2m[36m(pid=29801)[0m 0         Non-trainable params
[2m[36m(pid=29801)[0m 8.7 K     Total params
[2m[36m(pid=48812)[0m time to fit was 452.80866742134094
[2m[36m(pid=48812)[0m GPU available: False, used: False
[2m[36m(pid=48812)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48812)[0m 
[2m[36m(pid=48812)[0m   | Name      | Type              | Params
[2m[36m(pid=48812)[0m ------------------------------------------------
[2m[36m(pid=48812)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48812)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48812)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48812)[0m ------------------------------------------------
[2m[36m(pid=48812)[0m 8.7 K     Trainable params
[2m[36m(pid=48812)[0m 0         Non-trainable params
[2m[36m(pid=48812)[0m 8.7 K     Total params
[2m[36m(pid=8639)[0m time to fit was 460.61716079711914
[2m[36m(pid=8639)[0m GPU available: False, used: False
[2m[36m(pid=8639)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8639)[0m 
[2m[36m(pid=8639)[0m   | Name      | Type              | Params
[2m[36m(pid=8639)[0m ------------------------------------------------
[2m[36m(pid=8639)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8639)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8639)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8639)[0m ------------------------------------------------
[2m[36m(pid=8639)[0m 8.7 K     Trainable params
[2m[36m(pid=8639)[0m 0         Non-trainable params
[2m[36m(pid=8639)[0m 8.7 K     Total params
[2m[36m(pid=31015)[0m time to fit was 66.67665481567383
[2m[36m(pid=31015)[0m GPU available: False, used: False
[2m[36m(pid=31015)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31015)[0m 
[2m[36m(pid=31015)[0m   | Name      | Type              | Params
[2m[36m(pid=31015)[0m ------------------------------------------------
[2m[36m(pid=31015)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31015)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31015)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31015)[0m ------------------------------------------------
[2m[36m(pid=31015)[0m 8.7 K     Trainable params
[2m[36m(pid=31015)[0m 0         Non-trainable params
[2m[36m(pid=31015)[0m 8.7 K     Total params
[2m[36m(pid=27628)[0m time to fit was 60.28923010826111
[2m[36m(pid=27628)[0m GPU available: False, used: False
[2m[36m(pid=27628)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27628)[0m 
[2m[36m(pid=27628)[0m   | Name      | Type              | Params
[2m[36m(pid=27628)[0m ------------------------------------------------
[2m[36m(pid=27628)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27628)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27628)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27628)[0m ------------------------------------------------
[2m[36m(pid=27628)[0m 8.7 K     Trainable params
[2m[36m(pid=27628)[0m 0         Non-trainable params
[2m[36m(pid=27628)[0m 8.7 K     Total params
[2m[36m(pid=23225)[0m time to fit was 220.61530804634094
[2m[36m(pid=23225)[0m GPU available: False, used: False
[2m[36m(pid=23225)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23225)[0m 
[2m[36m(pid=23225)[0m   | Name      | Type              | Params
[2m[36m(pid=23225)[0m ------------------------------------------------
[2m[36m(pid=23225)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23225)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23225)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23225)[0m ------------------------------------------------
[2m[36m(pid=23225)[0m 8.7 K     Trainable params
[2m[36m(pid=23225)[0m 0         Non-trainable params
[2m[36m(pid=23225)[0m 8.7 K     Total params
[2m[36m(pid=27628)[0m time to fit was 42.06003212928772
[2m[36m(pid=30947)[0m time to fit was 135.31642842292786
Result for _inner_e8deb_00144:
  auc: 0.8266684770584106
  date: 2021-03-19_14-01-44
  done: false
  experiment_id: f0c92e844754408dbbc388b300dba21b
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 27628
  time_since_restore: 356.2916955947876
  time_this_iter_s: 356.2916955947876
  time_total_s: 356.2916955947876
  timestamp: 1616158904
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00144
  
[2m[36m(pid=27628)[0m Finished run with seed 0 - lr 2 - sec_lr 2 - bs 512 - mean val auc: 0.8266684770584106
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 151/180 (1 PENDING, 26 RUNNING, 124 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00122 | RUNNING    |       |          128 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00150 | PENDING    |       |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 131 more trials not shown (16 RUNNING, 114 TERMINATED)


Result for _inner_e8deb_00144:
  auc: 0.8266684770584106
  date: 2021-03-19_14-01-44
  done: true
  experiment_id: f0c92e844754408dbbc388b300dba21b
  experiment_tag: 144_batch_size=512,eta=0.0,lr=2,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 27628
  time_since_restore: 356.2916955947876
  time_this_iter_s: 356.2916955947876
  time_total_s: 356.2916955947876
  timestamp: 1616158904
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00144
  
[2m[36m(pid=30947)[0m GPU available: False, used: False
[2m[36m(pid=30947)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=30947)[0m 
[2m[36m(pid=30947)[0m   | Name      | Type              | Params
[2m[36m(pid=30947)[0m ------------------------------------------------
[2m[36m(pid=30947)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=30947)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=30947)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=30947)[0m ------------------------------------------------
[2m[36m(pid=30947)[0m 8.7 K     Trainable params
[2m[36m(pid=30947)[0m 0         Non-trainable params
[2m[36m(pid=30947)[0m 8.7 K     Total params
[2m[36m(pid=31019)[0m Starting run with seed 0 - lr 0.001 - sec_lr 5 - bs 32
[2m[36m(pid=31019)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=31019)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=31019)[0m GPU available: False, used: False
[2m[36m(pid=31019)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31019)[0m 
[2m[36m(pid=31019)[0m   | Name      | Type              | Params
[2m[36m(pid=31019)[0m ------------------------------------------------
[2m[36m(pid=31019)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31019)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31019)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31019)[0m ------------------------------------------------
[2m[36m(pid=31019)[0m 8.7 K     Trainable params
[2m[36m(pid=31019)[0m 0         Non-trainable params
[2m[36m(pid=31019)[0m 8.7 K     Total params
[2m[36m(pid=31019)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=31019)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31019)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=31019)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31019)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=31019)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=15400)[0m time to fit was 165.59345817565918
Result for _inner_e8deb_00137:
  auc: 0.9105301260948181
  date: 2021-03-19_14-02-00
  done: false
  experiment_id: f5726a9cb2ce40de877970aec9ae8a81
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 15400
  time_since_restore: 764.1839487552643
  time_this_iter_s: 764.1839487552643
  time_total_s: 764.1839487552643
  timestamp: 1616158920
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00137
  
[2m[36m(pid=15400)[0m Finished run with seed 0 - lr 1 - sec_lr 2 - bs 128 - mean val auc: 0.9105301260948181
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 152/180 (1 PENDING, 26 RUNNING, 125 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00122 | RUNNING    |       |          128 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00151 | PENDING    |       |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 132 more trials not shown (16 RUNNING, 115 TERMINATED)


Result for _inner_e8deb_00137:
  auc: 0.9105301260948181
  date: 2021-03-19_14-02-00
  done: true
  experiment_id: f5726a9cb2ce40de877970aec9ae8a81
  experiment_tag: 137_batch_size=128,eta=0.0,lr=1,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 15400
  time_since_restore: 764.1839487552643
  time_this_iter_s: 764.1839487552643
  time_total_s: 764.1839487552643
  timestamp: 1616158920
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00137
  
[2m[36m(pid=31006)[0m Starting run with seed 0 - lr 0.001 - sec_lr 5 - bs 64
[2m[36m(pid=31006)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=31006)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=31006)[0m GPU available: False, used: False
[2m[36m(pid=31006)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31006)[0m 
[2m[36m(pid=31006)[0m   | Name      | Type              | Params
[2m[36m(pid=31006)[0m ------------------------------------------------
[2m[36m(pid=31006)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31006)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31006)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31006)[0m ------------------------------------------------
[2m[36m(pid=31006)[0m 8.7 K     Trainable params
[2m[36m(pid=31006)[0m 0         Non-trainable params
[2m[36m(pid=31006)[0m 8.7 K     Total params
[2m[36m(pid=31006)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=31006)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31006)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=31006)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31006)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=31006)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31029)[0m time to fit was 180.1354203224182
[2m[36m(pid=31029)[0m GPU available: False, used: False
[2m[36m(pid=31029)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31029)[0m 
[2m[36m(pid=31029)[0m   | Name      | Type              | Params
[2m[36m(pid=31029)[0m ------------------------------------------------
[2m[36m(pid=31029)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31029)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31029)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31029)[0m ------------------------------------------------
[2m[36m(pid=31029)[0m 8.7 K     Trainable params
[2m[36m(pid=31029)[0m 0         Non-trainable params
[2m[36m(pid=31029)[0m 8.7 K     Total params
[2m[36m(pid=26127)[0m time to fit was 130.9018406867981
[2m[36m(pid=26127)[0m GPU available: False, used: False
[2m[36m(pid=26127)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26127)[0m 
[2m[36m(pid=26127)[0m   | Name      | Type              | Params
[2m[36m(pid=26127)[0m ------------------------------------------------
[2m[36m(pid=26127)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26127)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26127)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26127)[0m ------------------------------------------------
[2m[36m(pid=26127)[0m 8.7 K     Trainable params
[2m[36m(pid=26127)[0m 0         Non-trainable params
[2m[36m(pid=26127)[0m 8.7 K     Total params
[2m[36m(pid=22604)[0m time to fit was 572.0941035747528
[2m[36m(pid=22604)[0m GPU available: False, used: False
[2m[36m(pid=22604)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=22604)[0m 
[2m[36m(pid=22604)[0m   | Name      | Type              | Params
[2m[36m(pid=22604)[0m ------------------------------------------------
[2m[36m(pid=22604)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=22604)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=22604)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=22604)[0m ------------------------------------------------
[2m[36m(pid=22604)[0m 8.7 K     Trainable params
[2m[36m(pid=22604)[0m 0         Non-trainable params
[2m[36m(pid=22604)[0m 8.7 K     Total params
[2m[36m(pid=1105)[0m time to fit was 243.99678826332092
Result for _inner_e8deb_00131:
  auc: 0.9094928145408631
  date: 2021-03-19_14-02-53
  done: false
  experiment_id: ab0d6a0e466845559bcea255793a2bdf
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 1105
  time_since_restore: 1237.659516096115
  time_this_iter_s: 1237.659516096115
  time_total_s: 1237.659516096115
  timestamp: 1616158973
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00131
  
[2m[36m(pid=1105)[0m Finished run with seed 0 - lr 0.1 - sec_lr 2 - bs 64 - mean val auc: 0.9094928145408631
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 153/180 (1 PENDING, 26 RUNNING, 126 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00030 | RUNNING    |       |           32 |     0 | 0.001 |    0.01  |        |                  |          |
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00122 | RUNNING    |       |          128 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00152 | PENDING    |       |          128 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 133 more trials not shown (16 RUNNING, 116 TERMINATED)


Result for _inner_e8deb_00131:
  auc: 0.9094928145408631
  date: 2021-03-19_14-02-53
  done: true
  experiment_id: ab0d6a0e466845559bcea255793a2bdf
  experiment_tag: 131_batch_size=64,eta=0.0,lr=0.1,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 1105
  time_since_restore: 1237.659516096115
  time_this_iter_s: 1237.659516096115
  time_total_s: 1237.659516096115
  timestamp: 1616158973
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00131
  
[2m[36m(pid=31016)[0m Starting run with seed 0 - lr 0.001 - sec_lr 5 - bs 128
[2m[36m(pid=31016)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=31016)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=31016)[0m GPU available: False, used: False
[2m[36m(pid=31016)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31016)[0m 
[2m[36m(pid=31016)[0m   | Name      | Type              | Params
[2m[36m(pid=31016)[0m ------------------------------------------------
[2m[36m(pid=31016)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31016)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31016)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31016)[0m ------------------------------------------------
[2m[36m(pid=31016)[0m 8.7 K     Trainable params
[2m[36m(pid=31016)[0m 0         Non-trainable params
[2m[36m(pid=31016)[0m 8.7 K     Total params
[2m[36m(pid=31016)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=31016)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=15253)[0m time to fit was 270.0359721183777
[2m[36m(pid=15253)[0m GPU available: False, used: False
[2m[36m(pid=15253)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=15253)[0m 
[2m[36m(pid=15253)[0m   | Name      | Type              | Params
[2m[36m(pid=15253)[0m ------------------------------------------------
[2m[36m(pid=15253)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=15253)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=15253)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=15253)[0m ------------------------------------------------
[2m[36m(pid=15253)[0m 8.7 K     Trainable params
[2m[36m(pid=15253)[0m 0         Non-trainable params
[2m[36m(pid=15253)[0m 8.7 K     Total params
[2m[36m(pid=31016)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=31016)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31016)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=31016)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23089)[0m time to fit was 375.3993275165558
[2m[36m(pid=23225)[0m time to fit was 136.7956669330597
[2m[36m(pid=23089)[0m GPU available: False, used: False
[2m[36m(pid=23089)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23089)[0m 
[2m[36m(pid=23089)[0m   | Name      | Type              | Params
[2m[36m(pid=23089)[0m ------------------------------------------------
[2m[36m(pid=23089)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23089)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23089)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23089)[0m ------------------------------------------------
[2m[36m(pid=23089)[0m 8.7 K     Trainable params
[2m[36m(pid=23089)[0m 0         Non-trainable params
[2m[36m(pid=23089)[0m 8.7 K     Total params
[2m[36m(pid=23225)[0m GPU available: False, used: False
[2m[36m(pid=23225)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23225)[0m 
[2m[36m(pid=23225)[0m   | Name      | Type              | Params
[2m[36m(pid=23225)[0m ------------------------------------------------
[2m[36m(pid=23225)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23225)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23225)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23225)[0m ------------------------------------------------
[2m[36m(pid=23225)[0m 8.7 K     Trainable params
[2m[36m(pid=23225)[0m 0         Non-trainable params
[2m[36m(pid=23225)[0m 8.7 K     Total params
[2m[36m(pid=35838)[0m time to fit was 1776.2250425815582
Result for _inner_e8deb_00030:
  auc: 0.9084881544113159
  date: 2021-03-19_14-04-01
  done: false
  experiment_id: c3aa36d7fd074aed9e9c354a65a14bd9
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35838
  time_since_restore: 8927.744658946991
  time_this_iter_s: 8927.744658946991
  time_total_s: 8927.744658946991
  timestamp: 1616159041
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00030
  
[2m[36m(pid=35838)[0m Finished run with seed 0 - lr 0.001 - sec_lr 0.01 - bs 32 - mean val auc: 0.9084881544113159
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 154/180 (1 PENDING, 26 RUNNING, 127 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00030 | RUNNING    | 145.101.32.82:35838 |           32 |     0 | 0.001 |    0.01  |      1 |          8927.74 | 0.908488 |
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |                     |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |                     |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |                     |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |                     |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |                     |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00122 | RUNNING    |                     |          128 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |                     |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00153 | PENDING    |                     |          256 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 134 more trials not shown (16 RUNNING, 117 TERMINATED)


Result for _inner_e8deb_00030:
  auc: 0.9084881544113159
  date: 2021-03-19_14-04-01
  done: true
  experiment_id: c3aa36d7fd074aed9e9c354a65a14bd9
  experiment_tag: 30_batch_size=32,eta=0.0,lr=0.001,sec_lr=0.01
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35838
  time_since_restore: 8927.744658946991
  time_this_iter_s: 8927.744658946991
  time_total_s: 8927.744658946991
  timestamp: 1616159041
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00030
  
[2m[36m(pid=42969)[0m Starting run with seed 0 - lr 0.001 - sec_lr 5 - bs 256
[2m[36m(pid=42969)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=42969)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=42969)[0m GPU available: False, used: False
[2m[36m(pid=42969)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=42969)[0m 
[2m[36m(pid=42969)[0m   | Name      | Type              | Params
[2m[36m(pid=42969)[0m ------------------------------------------------
[2m[36m(pid=42969)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=42969)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=42969)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=42969)[0m ------------------------------------------------
[2m[36m(pid=42969)[0m 8.7 K     Trainable params
[2m[36m(pid=42969)[0m 0         Non-trainable params
[2m[36m(pid=42969)[0m 8.7 K     Total params
[2m[36m(pid=42969)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=42969)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=42969)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=42969)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=42969)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=42969)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31029)[0m time to fit was 137.87939286231995
[2m[36m(pid=31029)[0m GPU available: False, used: False
[2m[36m(pid=31029)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31029)[0m 
[2m[36m(pid=31029)[0m   | Name      | Type              | Params
[2m[36m(pid=31029)[0m ------------------------------------------------
[2m[36m(pid=31029)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31029)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31029)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31029)[0m ------------------------------------------------
[2m[36m(pid=31029)[0m 8.7 K     Trainable params
[2m[36m(pid=31029)[0m 0         Non-trainable params
[2m[36m(pid=31029)[0m 8.7 K     Total params
[2m[36m(pid=8457)[0m time to fit was 646.0144124031067
[2m[36m(pid=8457)[0m GPU available: False, used: False
[2m[36m(pid=8457)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8457)[0m 
[2m[36m(pid=8457)[0m   | Name      | Type              | Params
[2m[36m(pid=8457)[0m ------------------------------------------------
[2m[36m(pid=8457)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8457)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8457)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8457)[0m ------------------------------------------------
[2m[36m(pid=8457)[0m 8.7 K     Trainable params
[2m[36m(pid=8457)[0m 0         Non-trainable params
[2m[36m(pid=8457)[0m 8.7 K     Total params
[2m[36m(pid=26127)[0m time to fit was 127.982017993927
[2m[36m(pid=26127)[0m Finished run with seed 0 - lr 2 - sec_lr 2 - bs 256 - mean val auc: 0.8851775407791138
Result for _inner_e8deb_00143:
  auc: 0.8851775407791138
  date: 2021-03-19_14-04-33
  done: false
  experiment_id: 5a44cc3e1356493282e740ca613dea97
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 26127
  time_since_restore: 577.1764993667603
  time_this_iter_s: 577.1764993667603
  time_total_s: 577.1764993667603
  timestamp: 1616159073
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00143
  
[2m[36m(pid=48812)[0m time to fit was 242.76725029945374
== Status ==
Memory usage on this node: 9.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 155/180 (1 PENDING, 26 RUNNING, 128 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00122 | RUNNING    |       |          128 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00126 | RUNNING    |       |           64 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00154 | PENDING    |       |          512 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 135 more trials not shown (16 RUNNING, 118 TERMINATED)


Result for _inner_e8deb_00143:
  auc: 0.8851775407791138
  date: 2021-03-19_14-04-33
  done: true
  experiment_id: 5a44cc3e1356493282e740ca613dea97
  experiment_tag: 143_batch_size=256,eta=0.0,lr=2,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 26127
  time_since_restore: 577.1764993667603
  time_this_iter_s: 577.1764993667603
  time_total_s: 577.1764993667603
  timestamp: 1616159073
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00143
  
[2m[36m(pid=48812)[0m GPU available: False, used: False
[2m[36m(pid=48812)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=48812)[0m 
[2m[36m(pid=48812)[0m   | Name      | Type              | Params
[2m[36m(pid=48812)[0m ------------------------------------------------
[2m[36m(pid=48812)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=48812)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=48812)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=48812)[0m ------------------------------------------------
[2m[36m(pid=48812)[0m 8.7 K     Trainable params
[2m[36m(pid=48812)[0m 0         Non-trainable params
[2m[36m(pid=48812)[0m 8.7 K     Total params
[2m[36m(pid=44387)[0m Starting run with seed 0 - lr 0.001 - sec_lr 5 - bs 512
[2m[36m(pid=44387)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=44387)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=44387)[0m GPU available: False, used: False
[2m[36m(pid=44387)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=44387)[0m 
[2m[36m(pid=44387)[0m   | Name      | Type              | Params
[2m[36m(pid=44387)[0m ------------------------------------------------
[2m[36m(pid=44387)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=44387)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=44387)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=44387)[0m ------------------------------------------------
[2m[36m(pid=44387)[0m 8.7 K     Trainable params
[2m[36m(pid=44387)[0m 0         Non-trainable params
[2m[36m(pid=44387)[0m 8.7 K     Total params
[2m[36m(pid=44387)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=44387)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=44387)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=44387)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=44387)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=44387)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23225)[0m time to fit was 113.68533945083618
Result for _inner_e8deb_00142:
  auc: 0.7456849455833435
  date: 2021-03-19_14-05-21
  done: false
  experiment_id: 73afb4c989bb4acd8b9f6cd0d8bc0475
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 23225
  time_since_restore: 726.8828823566437
  time_this_iter_s: 726.8828823566437
  time_total_s: 726.8828823566437
  timestamp: 1616159121
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00142
  
[2m[36m(pid=23225)[0m Finished run with seed 0 - lr 2 - sec_lr 2 - bs 128 - mean val auc: 0.7456849455833435
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 156/180 (1 PENDING, 26 RUNNING, 129 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00122 | RUNNING    |       |          128 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00126 | RUNNING    |       |           64 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00155 | PENDING    |       |           32 |     0 | 0.01  |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 136 more trials not shown (16 RUNNING, 119 TERMINATED)


Result for _inner_e8deb_00142:
  auc: 0.7456849455833435
  date: 2021-03-19_14-05-21
  done: true
  experiment_id: 73afb4c989bb4acd8b9f6cd0d8bc0475
  experiment_tag: 142_batch_size=128,eta=0.0,lr=2,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 23225
  time_since_restore: 726.8828823566437
  time_this_iter_s: 726.8828823566437
  time_total_s: 726.8828823566437
  timestamp: 1616159121
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00142
  
[2m[36m(pid=45882)[0m Starting run with seed 0 - lr 0.01 - sec_lr 5 - bs 32
[2m[36m(pid=45882)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=45882)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=45882)[0m GPU available: False, used: False
[2m[36m(pid=45882)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=45882)[0m 
[2m[36m(pid=45882)[0m   | Name      | Type              | Params
[2m[36m(pid=45882)[0m ------------------------------------------------
[2m[36m(pid=45882)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=45882)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=45882)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=45882)[0m ------------------------------------------------
[2m[36m(pid=45882)[0m 8.7 K     Trainable params
[2m[36m(pid=45882)[0m 0         Non-trainable params
[2m[36m(pid=45882)[0m 8.7 K     Total params
[2m[36m(pid=45882)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=45882)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=45882)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=45882)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=45882)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=45882)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=44387)[0m time to fit was 75.53574085235596
[2m[36m(pid=44387)[0m GPU available: False, used: False
[2m[36m(pid=44387)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=44387)[0m 
[2m[36m(pid=44387)[0m   | Name      | Type              | Params
[2m[36m(pid=44387)[0m ------------------------------------------------
[2m[36m(pid=44387)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=44387)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=44387)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=44387)[0m ------------------------------------------------
[2m[36m(pid=44387)[0m 8.7 K     Trainable params
[2m[36m(pid=44387)[0m 0         Non-trainable params
[2m[36m(pid=44387)[0m 8.7 K     Total params
[2m[36m(pid=31015)[0m time to fit was 308.0140452384949
[2m[36m(pid=31015)[0m GPU available: False, used: False
[2m[36m(pid=31015)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31015)[0m 
[2m[36m(pid=31015)[0m   | Name      | Type              | Params
[2m[36m(pid=31015)[0m ------------------------------------------------
[2m[36m(pid=31015)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31015)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31015)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31015)[0m ------------------------------------------------
[2m[36m(pid=31015)[0m 8.7 K     Trainable params
[2m[36m(pid=31015)[0m 0         Non-trainable params
[2m[36m(pid=31015)[0m 8.7 K     Total params
[2m[36m(pid=34577)[0m time to fit was 527.4655690193176
[2m[36m(pid=34577)[0m GPU available: False, used: False
[2m[36m(pid=34577)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=34577)[0m 
[2m[36m(pid=34577)[0m   | Name      | Type              | Params
[2m[36m(pid=34577)[0m ------------------------------------------------
[2m[36m(pid=34577)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=34577)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=34577)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=34577)[0m ------------------------------------------------
[2m[36m(pid=34577)[0m 8.7 K     Trainable params
[2m[36m(pid=34577)[0m 0         Non-trainable params
[2m[36m(pid=34577)[0m 8.7 K     Total params
[2m[36m(pid=52747)[0m time to fit was 674.0259697437286
[2m[36m(pid=52747)[0m GPU available: False, used: False
[2m[36m(pid=52747)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=52747)[0m 
[2m[36m(pid=52747)[0m   | Name      | Type              | Params
[2m[36m(pid=52747)[0m ------------------------------------------------
[2m[36m(pid=52747)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=52747)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=52747)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=52747)[0m ------------------------------------------------
[2m[36m(pid=52747)[0m 8.7 K     Trainable params
[2m[36m(pid=52747)[0m 0         Non-trainable params
[2m[36m(pid=52747)[0m 8.7 K     Total params
[2m[36m(pid=15253)[0m time to fit was 248.05633735656738
[2m[36m(pid=15253)[0m GPU available: False, used: False
[2m[36m(pid=15253)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=15253)[0m 
[2m[36m(pid=15253)[0m   | Name      | Type              | Params
[2m[36m(pid=15253)[0m ------------------------------------------------
[2m[36m(pid=15253)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=15253)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=15253)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=15253)[0m ------------------------------------------------
[2m[36m(pid=15253)[0m 8.7 K     Trainable params
[2m[36m(pid=15253)[0m 0         Non-trainable params
[2m[36m(pid=15253)[0m 8.7 K     Total params
[2m[36m(pid=44387)[0m time to fit was 74.21568655967712
[2m[36m(pid=44387)[0m GPU available: False, used: False
[2m[36m(pid=44387)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31015)[0m time to fit was 67.22299194335938
[2m[36m(pid=44387)[0m 
[2m[36m(pid=44387)[0m   | Name      | Type              | Params
[2m[36m(pid=44387)[0m ------------------------------------------------
[2m[36m(pid=44387)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=44387)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=44387)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=44387)[0m ------------------------------------------------
[2m[36m(pid=44387)[0m 8.7 K     Trainable params
[2m[36m(pid=44387)[0m 0         Non-trainable params
[2m[36m(pid=44387)[0m 8.7 K     Total params
Result for _inner_e8deb_00148:
  auc: 0.5578674912452698
  date: 2021-03-19_14-07-14
  done: false
  experiment_id: 87c68ef510294bf78607b64e84eee07d
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 31015
  time_since_restore: 577.1539144515991
  time_this_iter_s: 577.1539144515991
  time_total_s: 577.1539144515991
  timestamp: 1616159234
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00148
  
[2m[36m(pid=31015)[0m Finished run with seed 0 - lr 5 - sec_lr 2 - bs 256 - mean val auc: 0.5578674912452698
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 157/180 (1 PENDING, 26 RUNNING, 130 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00122 | RUNNING    |       |          128 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00126 | RUNNING    |       |           64 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00156 | PENDING    |       |           64 |     0 | 0.01  |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 137 more trials not shown (16 RUNNING, 120 TERMINATED)


Result for _inner_e8deb_00148:
  auc: 0.5578674912452698
  date: 2021-03-19_14-07-14
  done: true
  experiment_id: 87c68ef510294bf78607b64e84eee07d
  experiment_tag: 148_batch_size=256,eta=0.0,lr=5,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 31015
  time_since_restore: 577.1539144515991
  time_this_iter_s: 577.1539144515991
  time_total_s: 577.1539144515991
  timestamp: 1616159234
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00148
  
[2m[36m(pid=49344)[0m Starting run with seed 0 - lr 0.01 - sec_lr 5 - bs 64
[2m[36m(pid=49344)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=49344)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=49344)[0m GPU available: False, used: False
[2m[36m(pid=49344)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49344)[0m 
[2m[36m(pid=49344)[0m   | Name      | Type              | Params
[2m[36m(pid=49344)[0m ------------------------------------------------
[2m[36m(pid=49344)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49344)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49344)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49344)[0m ------------------------------------------------
[2m[36m(pid=49344)[0m 8.7 K     Trainable params
[2m[36m(pid=49344)[0m 0         Non-trainable params
[2m[36m(pid=49344)[0m 8.7 K     Total params
[2m[36m(pid=49344)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=49344)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49344)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=49344)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=49344)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=49344)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31029)[0m time to fit was 218.05299353599548
[2m[36m(pid=31029)[0m GPU available: False, used: False
[2m[36m(pid=31029)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31029)[0m 
[2m[36m(pid=31029)[0m   | Name      | Type              | Params
[2m[36m(pid=31029)[0m ------------------------------------------------
[2m[36m(pid=31029)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31029)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31029)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31029)[0m ------------------------------------------------
[2m[36m(pid=31029)[0m 8.7 K     Trainable params
[2m[36m(pid=31029)[0m 0         Non-trainable params
[2m[36m(pid=31029)[0m 8.7 K     Total params
[2m[36m(pid=44387)[0m time to fit was 71.35493040084839
[2m[36m(pid=44387)[0m GPU available: False, used: False
[2m[36m(pid=44387)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=44387)[0m 
[2m[36m(pid=44387)[0m   | Name      | Type              | Params
[2m[36m(pid=44387)[0m ------------------------------------------------
[2m[36m(pid=44387)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=44387)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=44387)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=44387)[0m ------------------------------------------------
[2m[36m(pid=44387)[0m 8.7 K     Trainable params
[2m[36m(pid=44387)[0m 0         Non-trainable params
[2m[36m(pid=44387)[0m 8.7 K     Total params
[2m[36m(pid=30947)[0m time to fit was 417.91620993614197
[2m[36m(pid=30947)[0m GPU available: False, used: False
[2m[36m(pid=30947)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=30947)[0m 
[2m[36m(pid=30947)[0m   | Name      | Type              | Params
[2m[36m(pid=30947)[0m ------------------------------------------------
[2m[36m(pid=30947)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=30947)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=30947)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=30947)[0m ------------------------------------------------
[2m[36m(pid=30947)[0m 8.7 K     Trainable params
[2m[36m(pid=30947)[0m 0         Non-trainable params
[2m[36m(pid=30947)[0m 8.7 K     Total params
[2m[36m(pid=31029)[0m time to fit was 46.42361497879028
[2m[36m(pid=31029)[0m GPU available: False, used: False
[2m[36m(pid=31029)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31029)[0m 
[2m[36m(pid=31029)[0m   | Name      | Type              | Params
[2m[36m(pid=31029)[0m ------------------------------------------------
[2m[36m(pid=31029)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31029)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31029)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31029)[0m ------------------------------------------------
[2m[36m(pid=31029)[0m 8.7 K     Trainable params
[2m[36m(pid=31029)[0m 0         Non-trainable params
[2m[36m(pid=31029)[0m 8.7 K     Total params
[2m[36m(pid=8639)[0m time to fit was 500.4538938999176
[2m[36m(pid=8639)[0m GPU available: False, used: False
[2m[36m(pid=8639)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8639)[0m 
[2m[36m(pid=8639)[0m   | Name      | Type              | Params
[2m[36m(pid=8639)[0m ------------------------------------------------
[2m[36m(pid=8639)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8639)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8639)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8639)[0m ------------------------------------------------
[2m[36m(pid=8639)[0m 8.7 K     Trainable params
[2m[36m(pid=8639)[0m 0         Non-trainable params
[2m[36m(pid=8639)[0m 8.7 K     Total params
[2m[36m(pid=47886)[0m time to fit was 795.0816986560822
[2m[36m(pid=47886)[0m GPU available: False, used: False
[2m[36m(pid=47886)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=47886)[0m 
[2m[36m(pid=47886)[0m   | Name      | Type              | Params
[2m[36m(pid=47886)[0m ------------------------------------------------
[2m[36m(pid=47886)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=47886)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=47886)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=47886)[0m ------------------------------------------------
[2m[36m(pid=47886)[0m 8.7 K     Trainable params
[2m[36m(pid=47886)[0m 0         Non-trainable params
[2m[36m(pid=47886)[0m 8.7 K     Total params
[2m[36m(pid=42969)[0m time to fit was 315.28502368927
[2m[36m(pid=42969)[0m GPU available: False, used: False
[2m[36m(pid=42969)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=42969)[0m 
[2m[36m(pid=42969)[0m   | Name      | Type              | Params
[2m[36m(pid=42969)[0m ------------------------------------------------
[2m[36m(pid=42969)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=42969)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=42969)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=42969)[0m ------------------------------------------------
[2m[36m(pid=42969)[0m 8.7 K     Trainable params
[2m[36m(pid=42969)[0m 0         Non-trainable params
[2m[36m(pid=42969)[0m 8.7 K     Total params
[2m[36m(pid=31029)[0m time to fit was 41.48985552787781
[2m[36m(pid=31029)[0m Finished run with seed 0 - lr 5 - sec_lr 2 - bs 512 - mean val auc: 0.736411714553833
Result for _inner_e8deb_00149:
  auc: 0.736411714553833
  date: 2021-03-19_14-09-36
  done: false
  experiment_id: 6b19af8a322e4584bfbe718376558e16
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 31029
  time_since_restore: 625.2517783641815
  time_this_iter_s: 625.2517783641815
  time_total_s: 625.2517783641815
  timestamp: 1616159376
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00149
  
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 158/180 (1 PENDING, 26 RUNNING, 131 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00122 | RUNNING    |       |          128 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00126 | RUNNING    |       |           64 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00157 | PENDING    |       |          128 |     0 | 0.01  |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 138 more trials not shown (16 RUNNING, 121 TERMINATED)


Result for _inner_e8deb_00149:
  auc: 0.736411714553833
  date: 2021-03-19_14-09-36
  done: true
  experiment_id: 6b19af8a322e4584bfbe718376558e16
  experiment_tag: 149_batch_size=512,eta=0.0,lr=5,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 31029
  time_since_restore: 625.2517783641815
  time_this_iter_s: 625.2517783641815
  time_total_s: 625.2517783641815
  timestamp: 1616159376
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00149
  
[2m[36m(pid=44387)[0m time to fit was 77.60834836959839
[2m[36m(pid=44387)[0m GPU available: False, used: False
[2m[36m(pid=44387)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=44387)[0m 
[2m[36m(pid=44387)[0m   | Name      | Type              | Params
[2m[36m(pid=44387)[0m ------------------------------------------------
[2m[36m(pid=44387)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=44387)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=44387)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=44387)[0m ------------------------------------------------
[2m[36m(pid=44387)[0m 8.7 K     Trainable params
[2m[36m(pid=44387)[0m 0         Non-trainable params
[2m[36m(pid=44387)[0m 8.7 K     Total params
[2m[36m(pid=564)[0m Starting run with seed 0 - lr 0.01 - sec_lr 5 - bs 128
[2m[36m(pid=564)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=564)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=564)[0m GPU available: False, used: False
[2m[36m(pid=564)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=564)[0m 
[2m[36m(pid=564)[0m   | Name      | Type              | Params
[2m[36m(pid=564)[0m ------------------------------------------------
[2m[36m(pid=564)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=564)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=564)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=564)[0m ------------------------------------------------
[2m[36m(pid=564)[0m 8.7 K     Trainable params
[2m[36m(pid=564)[0m 0         Non-trainable params
[2m[36m(pid=564)[0m 8.7 K     Total params
[2m[36m(pid=564)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=564)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=564)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=564)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=564)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=564)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=48812)[0m time to fit was 321.65419602394104
Result for _inner_e8deb_00126:
  auc: 0.9106584072113038
  date: 2021-03-19_14-09-55
  done: false
  experiment_id: 559f9f0aac8c4363b5073e81221b3d2b
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 48812
  time_since_restore: 1824.799207687378
  time_this_iter_s: 1824.799207687378
  time_total_s: 1824.799207687378
  timestamp: 1616159395
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00126
  
[2m[36m(pid=48812)[0m Finished run with seed 0 - lr 0.01 - sec_lr 2 - bs 64 - mean val auc: 0.9106584072113038
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 159/180 (1 PENDING, 26 RUNNING, 132 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |                     |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |                     |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |                     |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |                     |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |                     |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00122 | RUNNING    |                     |          128 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |                     |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00126 | RUNNING    | 145.101.32.82:48812 |           64 |     0 | 0.01  |    2     |      1 |          1824.8  | 0.910658 |
| _inner_e8deb_00158 | PENDING    |                     |          256 |     0 | 0.01  |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 139 more trials not shown (16 RUNNING, 122 TERMINATED)


Result for _inner_e8deb_00126:
  auc: 0.9106584072113038
  date: 2021-03-19_14-09-55
  done: true
  experiment_id: 559f9f0aac8c4363b5073e81221b3d2b
  experiment_tag: 126_batch_size=64,eta=0.0,lr=0.01,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 48812
  time_since_restore: 1824.799207687378
  time_this_iter_s: 1824.799207687378
  time_total_s: 1824.799207687378
  timestamp: 1616159395
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00126
  
[2m[36m(pid=1110)[0m Starting run with seed 0 - lr 0.01 - sec_lr 5 - bs 256
[2m[36m(pid=1110)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=1110)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=1110)[0m GPU available: False, used: False
[2m[36m(pid=1110)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=1110)[0m 
[2m[36m(pid=1110)[0m   | Name      | Type              | Params
[2m[36m(pid=1110)[0m ------------------------------------------------
[2m[36m(pid=1110)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=1110)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=1110)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=1110)[0m ------------------------------------------------
[2m[36m(pid=1110)[0m 8.7 K     Trainable params
[2m[36m(pid=1110)[0m 0         Non-trainable params
[2m[36m(pid=1110)[0m 8.7 K     Total params
[2m[36m(pid=1110)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=1110)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=1110)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=1110)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=1110)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=1110)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=30947)[0m time to fit was 115.18153619766235
[2m[36m(pid=30947)[0m GPU available: False, used: False
[2m[36m(pid=30947)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=30947)[0m 
[2m[36m(pid=30947)[0m   | Name      | Type              | Params
[2m[36m(pid=30947)[0m ------------------------------------------------
[2m[36m(pid=30947)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=30947)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=30947)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=30947)[0m ------------------------------------------------
[2m[36m(pid=30947)[0m 8.7 K     Trainable params
[2m[36m(pid=30947)[0m 0         Non-trainable params
[2m[36m(pid=30947)[0m 8.7 K     Total params
[2m[36m(pid=15253)[0m time to fit was 207.7306935787201
Result for _inner_e8deb_00136:
  auc: 0.9083273410797119
  date: 2021-03-19_14-10-39
  done: false
  experiment_id: 0e13cf3d598c4e35af9a0dce535c830f
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 15253
  time_since_restore: 1285.127653837204
  time_this_iter_s: 1285.127653837204
  time_total_s: 1285.127653837204
  timestamp: 1616159439
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00136
  
[2m[36m(pid=15253)[0m Finished run with seed 0 - lr 1 - sec_lr 2 - bs 64 - mean val auc: 0.9083273410797119
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 160/180 (1 PENDING, 26 RUNNING, 133 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    |       |           32 |     0 | 2     |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00122 | RUNNING    |       |          128 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00130 | RUNNING    |       |           32 |     0 | 0.1   |    2     |        |                  |          |
| _inner_e8deb_00159 | PENDING    |       |          512 |     0 | 0.01  |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 140 more trials not shown (16 RUNNING, 123 TERMINATED)


Result for _inner_e8deb_00136:
  auc: 0.9083273410797119
  date: 2021-03-19_14-10-39
  done: true
  experiment_id: 0e13cf3d598c4e35af9a0dce535c830f
  experiment_tag: 136_batch_size=64,eta=0.0,lr=1,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 15253
  time_since_restore: 1285.127653837204
  time_this_iter_s: 1285.127653837204
  time_total_s: 1285.127653837204
  timestamp: 1616159439
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00136
  
2021-03-19 14:10:39,828	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff6dbbfced01000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=23089)[0m time to fit was 436.07572841644287
[2m[36m(pid=23089)[0m GPU available: False, used: False
[2m[36m(pid=23089)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23089)[0m 
[2m[36m(pid=23089)[0m   | Name      | Type              | Params
[2m[36m(pid=23089)[0m ------------------------------------------------
[2m[36m(pid=23089)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23089)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23089)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23089)[0m ------------------------------------------------
[2m[36m(pid=23089)[0m 8.7 K     Trainable params
[2m[36m(pid=23089)[0m 0         Non-trainable params
[2m[36m(pid=23089)[0m 8.7 K     Total params
[2m[36m(pid=2527)[0m Starting run with seed 0 - lr 0.01 - sec_lr 5 - bs 512
[2m[36m(pid=2527)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=2527)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=2527)[0m GPU available: False, used: False
[2m[36m(pid=2527)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=2527)[0m 
[2m[36m(pid=2527)[0m   | Name      | Type              | Params
[2m[36m(pid=2527)[0m ------------------------------------------------
[2m[36m(pid=2527)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=2527)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=2527)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=2527)[0m ------------------------------------------------
[2m[36m(pid=2527)[0m 8.7 K     Trainable params
[2m[36m(pid=2527)[0m 0         Non-trainable params
[2m[36m(pid=2527)[0m 8.7 K     Total params
[2m[36m(pid=2527)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=2527)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=2527)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=2527)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=2527)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=2527)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=42969)[0m time to fit was 85.243812084198
[2m[36m(pid=42969)[0m GPU available: False, used: False
[2m[36m(pid=42969)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=42969)[0m 
[2m[36m(pid=42969)[0m   | Name      | Type              | Params
[2m[36m(pid=42969)[0m ------------------------------------------------
[2m[36m(pid=42969)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=42969)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=42969)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=42969)[0m ------------------------------------------------
[2m[36m(pid=42969)[0m 8.7 K     Trainable params
[2m[36m(pid=42969)[0m 0         Non-trainable params
[2m[36m(pid=42969)[0m 8.7 K     Total params
[2m[36m(pid=8457)[0m time to fit was 399.9192750453949
Result for _inner_e8deb_00110:
  auc: 0.8208109974861145
  date: 2021-03-19_14-11-11
  done: false
  experiment_id: effa89bb5e734ae6978126cec0203e3a
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8457
  time_since_restore: 3152.058210372925
  time_this_iter_s: 3152.058210372925
  time_total_s: 3152.058210372925
  timestamp: 1616159471
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00110
  
[2m[36m(pid=8457)[0m Finished run with seed 0 - lr 2 - sec_lr 1 - bs 32 - mean val auc: 0.8208109974861145
== Status ==
Memory usage on this node: 9.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 161/180 (1 PENDING, 26 RUNNING, 134 TERMINATED)
+--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |                    |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |                    |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |                    |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00110 | RUNNING    | 145.101.32.82:8457 |           32 |     0 | 2     |    1     |      1 |          3152.06 | 0.820811 |
| _inner_e8deb_00115 | RUNNING    |                    |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |                    |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |                    |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00122 | RUNNING    |                    |          128 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |                    |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00130 | RUNNING    |                    |           32 |     0 | 0.1   |    2     |        |                  |          |
| _inner_e8deb_00160 | PENDING    |                    |           32 |     0 | 0.1   |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                    |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                    |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                    |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                    |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                    |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                    |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                    |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                    |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                    |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                    |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 141 more trials not shown (16 RUNNING, 124 TERMINATED)


Result for _inner_e8deb_00110:
  auc: 0.8208109974861145
  date: 2021-03-19_14-11-11
  done: true
  experiment_id: effa89bb5e734ae6978126cec0203e3a
  experiment_tag: 110_batch_size=32,eta=0.0,lr=2,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8457
  time_since_restore: 3152.058210372925
  time_this_iter_s: 3152.058210372925
  time_total_s: 3152.058210372925
  timestamp: 1616159471
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00110
  
[2m[36m(pid=3499)[0m Starting run with seed 0 - lr 0.1 - sec_lr 5 - bs 32
[2m[36m(pid=3499)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=3499)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=3499)[0m GPU available: False, used: False
[2m[36m(pid=3499)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=3499)[0m 
[2m[36m(pid=3499)[0m   | Name      | Type              | Params
[2m[36m(pid=3499)[0m ------------------------------------------------
[2m[36m(pid=3499)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=3499)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=3499)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=3499)[0m ------------------------------------------------
[2m[36m(pid=3499)[0m 8.7 K     Trainable params
[2m[36m(pid=3499)[0m 0         Non-trainable params
[2m[36m(pid=3499)[0m 8.7 K     Total params
[2m[36m(pid=3499)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=3499)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=3499)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=3499)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=3499)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=3499)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31016)[0m time to fit was 525.3420076370239
[2m[36m(pid=31016)[0m GPU available: False, used: False
[2m[36m(pid=31016)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31016)[0m 
[2m[36m(pid=31016)[0m   | Name      | Type              | Params
[2m[36m(pid=31016)[0m ------------------------------------------------
[2m[36m(pid=31016)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31016)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31016)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31016)[0m ------------------------------------------------
[2m[36m(pid=31016)[0m 8.7 K     Trainable params
[2m[36m(pid=31016)[0m 0         Non-trainable params
[2m[36m(pid=31016)[0m 8.7 K     Total params
[2m[36m(pid=1110)[0m time to fit was 104.28702449798584
[2m[36m(pid=1110)[0m GPU available: False, used: False
[2m[36m(pid=1110)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=1110)[0m 
[2m[36m(pid=1110)[0m   | Name      | Type              | Params
[2m[36m(pid=1110)[0m ------------------------------------------------
[2m[36m(pid=1110)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=1110)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=1110)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=1110)[0m ------------------------------------------------
[2m[36m(pid=1110)[0m 8.7 K     Trainable params
[2m[36m(pid=1110)[0m 0         Non-trainable params
[2m[36m(pid=1110)[0m 8.7 K     Total params
[2m[36m(pid=42969)[0m time to fit was 91.21023344993591
[2m[36m(pid=42969)[0m GPU available: False, used: False
[2m[36m(pid=42969)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=42969)[0m 
[2m[36m(pid=42969)[0m   | Name      | Type              | Params
[2m[36m(pid=42969)[0m ------------------------------------------------
[2m[36m(pid=42969)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=42969)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=42969)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=42969)[0m ------------------------------------------------
[2m[36m(pid=42969)[0m 8.7 K     Trainable params
[2m[36m(pid=42969)[0m 0         Non-trainable params
[2m[36m(pid=42969)[0m 8.7 K     Total params
[2m[36m(pid=564)[0m time to fit was 177.33664560317993
[2m[36m(pid=564)[0m GPU available: False, used: False
[2m[36m(pid=564)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=564)[0m 
[2m[36m(pid=564)[0m   | Name      | Type              | Params
[2m[36m(pid=564)[0m ------------------------------------------------
[2m[36m(pid=564)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=564)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=564)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=564)[0m ------------------------------------------------
[2m[36m(pid=564)[0m 8.7 K     Trainable params
[2m[36m(pid=564)[0m 0         Non-trainable params
[2m[36m(pid=564)[0m 8.7 K     Total params
[2m[36m(pid=29932)[0m time to fit was 947.0830700397491
[2m[36m(pid=29932)[0m GPU available: False, used: False
[2m[36m(pid=29932)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29932)[0m 
[2m[36m(pid=29932)[0m   | Name      | Type              | Params
[2m[36m(pid=29932)[0m ------------------------------------------------
[2m[36m(pid=29932)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29932)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29932)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29932)[0m ------------------------------------------------
[2m[36m(pid=29932)[0m 8.7 K     Trainable params
[2m[36m(pid=29932)[0m 0         Non-trainable params
[2m[36m(pid=29932)[0m 8.7 K     Total params
[2m[36m(pid=31106)[0m time to fit was 943.1073248386383
[2m[36m(pid=31106)[0m GPU available: False, used: False
[2m[36m(pid=31106)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31106)[0m 
[2m[36m(pid=31106)[0m   | Name      | Type              | Params
[2m[36m(pid=31106)[0m ------------------------------------------------
[2m[36m(pid=31106)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31106)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31106)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31106)[0m ------------------------------------------------
[2m[36m(pid=31106)[0m 8.7 K     Trainable params
[2m[36m(pid=31106)[0m 0         Non-trainable params
[2m[36m(pid=31106)[0m 8.7 K     Total params
[2m[36m(pid=22604)[0m time to fit was 644.3877475261688
[2m[36m(pid=22604)[0m GPU available: False, used: False
[2m[36m(pid=22604)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=22604)[0m 
[2m[36m(pid=22604)[0m   | Name      | Type              | Params
[2m[36m(pid=22604)[0m ------------------------------------------------
[2m[36m(pid=22604)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=22604)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=22604)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=22604)[0m ------------------------------------------------
[2m[36m(pid=22604)[0m 8.7 K     Trainable params
[2m[36m(pid=22604)[0m 0         Non-trainable params
[2m[36m(pid=22604)[0m 8.7 K     Total params
[2m[36m(pid=44387)[0m time to fit was 215.16115760803223
Result for _inner_e8deb_00154:
  auc: 0.8390738248825074
  date: 2021-03-19_14-13-19
  done: false
  experiment_id: ee3c0621f84544c1b6f092366ac6c87a
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 44387
  time_since_restore: 515.169281244278
  time_this_iter_s: 515.169281244278
  time_total_s: 515.169281244278
  timestamp: 1616159599
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00154
  
[2m[36m(pid=44387)[0m Finished run with seed 0 - lr 0.001 - sec_lr 5 - bs 512 - mean val auc: 0.8390738248825074
== Status ==
Memory usage on this node: 9.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 162/180 (1 PENDING, 26 RUNNING, 135 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00122 | RUNNING    |       |          128 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00130 | RUNNING    |       |           32 |     0 | 0.1   |    2     |        |                  |          |
| _inner_e8deb_00135 | RUNNING    |       |           32 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00161 | PENDING    |       |           64 |     0 | 0.1   |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 142 more trials not shown (16 RUNNING, 125 TERMINATED)


Result for _inner_e8deb_00154:
  auc: 0.8390738248825074
  date: 2021-03-19_14-13-19
  done: true
  experiment_id: ee3c0621f84544c1b6f092366ac6c87a
  experiment_tag: 154_batch_size=512,eta=0.0,lr=0.001,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 44387
  time_since_restore: 515.169281244278
  time_this_iter_s: 515.169281244278
  time_total_s: 515.169281244278
  timestamp: 1616159599
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00154
  
2021-03-19 14:13:20,265	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff0e828b6701000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=49344)[0m time to fit was 354.59802651405334
[2m[36m(pid=49344)[0m GPU available: False, used: False
[2m[36m(pid=49344)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49344)[0m 
[2m[36m(pid=49344)[0m   | Name      | Type              | Params
[2m[36m(pid=49344)[0m ------------------------------------------------
[2m[36m(pid=49344)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49344)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49344)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49344)[0m ------------------------------------------------
[2m[36m(pid=49344)[0m 8.7 K     Trainable params
[2m[36m(pid=49344)[0m 0         Non-trainable params
[2m[36m(pid=49344)[0m 8.7 K     Total params
[2m[36m(pid=7482)[0m Starting run with seed 0 - lr 0.1 - sec_lr 5 - bs 64
[2m[36m(pid=7482)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7482)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=7482)[0m GPU available: False, used: False
[2m[36m(pid=7482)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=7482)[0m 
[2m[36m(pid=7482)[0m   | Name      | Type              | Params
[2m[36m(pid=7482)[0m ------------------------------------------------
[2m[36m(pid=7482)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=7482)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=7482)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=7482)[0m ------------------------------------------------
[2m[36m(pid=7482)[0m 8.7 K     Trainable params
[2m[36m(pid=7482)[0m 0         Non-trainable params
[2m[36m(pid=7482)[0m 8.7 K     Total params
[2m[36m(pid=7482)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=7482)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=7482)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=7482)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=7482)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=7482)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=2527)[0m time to fit was 164.3780162334442
[2m[36m(pid=2527)[0m GPU available: False, used: False
[2m[36m(pid=2527)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=2527)[0m 
[2m[36m(pid=2527)[0m   | Name      | Type              | Params
[2m[36m(pid=2527)[0m ------------------------------------------------
[2m[36m(pid=2527)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=2527)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=2527)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=2527)[0m ------------------------------------------------
[2m[36m(pid=2527)[0m 8.7 K     Trainable params
[2m[36m(pid=2527)[0m 0         Non-trainable params
[2m[36m(pid=2527)[0m 8.7 K     Total params
[2m[36m(pid=11272)[0m time to fit was 1707.0179641246796
[2m[36m(pid=11272)[0m GPU available: False, used: False
[2m[36m(pid=11272)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=11272)[0m 
[2m[36m(pid=11272)[0m   | Name      | Type              | Params
[2m[36m(pid=11272)[0m ------------------------------------------------
[2m[36m(pid=11272)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=11272)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=11272)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=11272)[0m ------------------------------------------------
[2m[36m(pid=11272)[0m 8.7 K     Trainable params
[2m[36m(pid=11272)[0m 0         Non-trainable params
[2m[36m(pid=11272)[0m 8.7 K     Total params
[2m[36m(pid=30947)[0m time to fit was 199.83587193489075
Result for _inner_e8deb_00147:
  auc: 0.5700808525085449
  date: 2021-03-19_14-13-57
  done: false
  experiment_id: 3f0de03625984904b5d4f9afece21375
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 30947
  time_since_restore: 982.4435954093933
  time_this_iter_s: 982.4435954093933
  time_total_s: 982.4435954093933
  timestamp: 1616159637
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00147
  
[2m[36m(pid=30947)[0m Finished run with seed 0 - lr 5 - sec_lr 2 - bs 128 - mean val auc: 0.5700808525085449
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 163/180 (1 PENDING, 26 RUNNING, 136 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00122 | RUNNING    |       |          128 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00130 | RUNNING    |       |           32 |     0 | 0.1   |    2     |        |                  |          |
| _inner_e8deb_00135 | RUNNING    |       |           32 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00162 | PENDING    |       |          128 |     0 | 0.1   |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 143 more trials not shown (16 RUNNING, 126 TERMINATED)


Result for _inner_e8deb_00147:
  auc: 0.5700808525085449
  date: 2021-03-19_14-13-57
  done: true
  experiment_id: 3f0de03625984904b5d4f9afece21375
  experiment_tag: 147_batch_size=128,eta=0.0,lr=5,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 30947
  time_since_restore: 982.4435954093933
  time_this_iter_s: 982.4435954093933
  time_total_s: 982.4435954093933
  timestamp: 1616159637
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00147
  
[2m[36m(pid=8610)[0m Starting run with seed 0 - lr 0.1 - sec_lr 5 - bs 128
[2m[36m(pid=8610)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=8610)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=8610)[0m GPU available: False, used: False
[2m[36m(pid=8610)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8610)[0m 
[2m[36m(pid=8610)[0m   | Name      | Type              | Params
[2m[36m(pid=8610)[0m ------------------------------------------------
[2m[36m(pid=8610)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8610)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8610)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8610)[0m ------------------------------------------------
[2m[36m(pid=8610)[0m 8.7 K     Trainable params
[2m[36m(pid=8610)[0m 0         Non-trainable params
[2m[36m(pid=8610)[0m 8.7 K     Total params
[2m[36m(pid=8610)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8610)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8610)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=8610)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8610)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=8610)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=52747)[0m time to fit was 462.2257454395294
[2m[36m(pid=52747)[0m GPU available: False, used: False
[2m[36m(pid=52747)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=52747)[0m 
[2m[36m(pid=52747)[0m   | Name      | Type              | Params
[2m[36m(pid=52747)[0m ------------------------------------------------
[2m[36m(pid=52747)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=52747)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=52747)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=52747)[0m ------------------------------------------------
[2m[36m(pid=52747)[0m 8.7 K     Trainable params
[2m[36m(pid=52747)[0m 0         Non-trainable params
[2m[36m(pid=52747)[0m 8.7 K     Total params
[2m[36m(pid=1110)[0m time to fit was 152.70089387893677
[2m[36m(pid=1110)[0m GPU available: False, used: False
[2m[36m(pid=1110)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=1110)[0m 
[2m[36m(pid=1110)[0m   | Name      | Type              | Params
[2m[36m(pid=1110)[0m ------------------------------------------------
[2m[36m(pid=1110)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=1110)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=1110)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=1110)[0m ------------------------------------------------
[2m[36m(pid=1110)[0m 8.7 K     Trainable params
[2m[36m(pid=1110)[0m 0         Non-trainable params
[2m[36m(pid=1110)[0m 8.7 K     Total params
[2m[36m(pid=45882)[0m time to fit was 564.0609138011932
[2m[36m(pid=45882)[0m GPU available: False, used: False
[2m[36m(pid=45882)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=45882)[0m 
[2m[36m(pid=45882)[0m   | Name      | Type              | Params
[2m[36m(pid=45882)[0m ------------------------------------------------
[2m[36m(pid=45882)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=45882)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=45882)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=45882)[0m ------------------------------------------------
[2m[36m(pid=45882)[0m 8.7 K     Trainable params
[2m[36m(pid=45882)[0m 0         Non-trainable params
[2m[36m(pid=45882)[0m 8.7 K     Total params
[2m[36m(pid=34577)[0m time to fit was 525.8263309001923
Result for _inner_e8deb_00122:
  auc: 0.8881144046783447
  date: 2021-03-19_14-15-12
  done: false
  experiment_id: 1a04046955114047a022ac2198299314
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 34577
  time_since_restore: 2628.17236661911
  time_this_iter_s: 2628.17236661911
  time_total_s: 2628.17236661911
  timestamp: 1616159712
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00122
  
[2m[36m(pid=34577)[0m Finished run with seed 0 - lr 0.001 - sec_lr 2 - bs 128 - mean val auc: 0.8881144046783447
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 164/180 (1 PENDING, 26 RUNNING, 137 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |                     |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |                     |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |                     |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |                     |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00122 | RUNNING    | 145.101.32.82:34577 |          128 |     0 | 0.001 |    2     |      1 |          2628.17 | 0.888114 |
| _inner_e8deb_00125 | RUNNING    |                     |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00130 | RUNNING    |                     |           32 |     0 | 0.1   |    2     |        |                  |          |
| _inner_e8deb_00135 | RUNNING    |                     |           32 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00163 | PENDING    |                     |          256 |     0 | 0.1   |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 144 more trials not shown (16 RUNNING, 127 TERMINATED)


Result for _inner_e8deb_00122:
  auc: 0.8881144046783447
  date: 2021-03-19_14-15-12
  done: true
  experiment_id: 1a04046955114047a022ac2198299314
  experiment_tag: 122_batch_size=128,eta=0.0,lr=0.001,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 34577
  time_since_restore: 2628.17236661911
  time_this_iter_s: 2628.17236661911
  time_total_s: 2628.17236661911
  timestamp: 1616159712
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00122
  
[2m[36m(pid=10664)[0m Starting run with seed 0 - lr 0.1 - sec_lr 5 - bs 256
[2m[36m(pid=10664)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10664)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=10664)[0m GPU available: False, used: False
[2m[36m(pid=10664)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10664)[0m 
[2m[36m(pid=10664)[0m   | Name      | Type              | Params
[2m[36m(pid=10664)[0m ------------------------------------------------
[2m[36m(pid=10664)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10664)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10664)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10664)[0m ------------------------------------------------
[2m[36m(pid=10664)[0m 8.7 K     Trainable params
[2m[36m(pid=10664)[0m 0         Non-trainable params
[2m[36m(pid=10664)[0m 8.7 K     Total params
[2m[36m(pid=10664)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=10664)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=10664)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=10664)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=10664)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=10664)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23089)[0m time to fit was 322.3311233520508
[2m[36m(pid=23089)[0m GPU available: False, used: False
[2m[36m(pid=23089)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23089)[0m 
[2m[36m(pid=23089)[0m   | Name      | Type              | Params
[2m[36m(pid=23089)[0m ------------------------------------------------
[2m[36m(pid=23089)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23089)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23089)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23089)[0m ------------------------------------------------
[2m[36m(pid=23089)[0m 8.7 K     Trainable params
[2m[36m(pid=23089)[0m 0         Non-trainable params
[2m[36m(pid=23089)[0m 8.7 K     Total params
[2m[36m(pid=8610)[0m time to fit was 123.84977555274963
[2m[36m(pid=8610)[0m GPU available: False, used: False
[2m[36m(pid=8610)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8610)[0m 
[2m[36m(pid=8610)[0m   | Name      | Type              | Params
[2m[36m(pid=8610)[0m ------------------------------------------------
[2m[36m(pid=8610)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8610)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8610)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8610)[0m ------------------------------------------------
[2m[36m(pid=8610)[0m 8.7 K     Trainable params
[2m[36m(pid=8610)[0m 0         Non-trainable params
[2m[36m(pid=8610)[0m 8.7 K     Total params
[2m[36m(pid=564)[0m time to fit was 206.75057363510132
[2m[36m(pid=564)[0m GPU available: False, used: False
[2m[36m(pid=564)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=564)[0m 
[2m[36m(pid=564)[0m   | Name      | Type              | Params
[2m[36m(pid=564)[0m ------------------------------------------------
[2m[36m(pid=564)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=564)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=564)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=564)[0m ------------------------------------------------
[2m[36m(pid=564)[0m 8.7 K     Trainable params
[2m[36m(pid=564)[0m 0         Non-trainable params
[2m[36m(pid=564)[0m 8.7 K     Total params
[2m[36m(pid=29932)[0m time to fit was 207.42681574821472
[2m[36m(pid=29932)[0m GPU available: False, used: False
[2m[36m(pid=29932)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29932)[0m 
[2m[36m(pid=29932)[0m   | Name      | Type              | Params
[2m[36m(pid=29932)[0m ------------------------------------------------
[2m[36m(pid=29932)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29932)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29932)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29932)[0m ------------------------------------------------
[2m[36m(pid=29932)[0m 8.7 K     Trainable params
[2m[36m(pid=29932)[0m 0         Non-trainable params
[2m[36m(pid=29932)[0m 8.7 K     Total params
[2m[36m(pid=10664)[0m time to fit was 71.74578905105591
[2m[36m(pid=10664)[0m GPU available: False, used: False
[2m[36m(pid=10664)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10664)[0m 
[2m[36m(pid=10664)[0m   | Name      | Type              | Params
[2m[36m(pid=10664)[0m ------------------------------------------------
[2m[36m(pid=10664)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10664)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10664)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10664)[0m ------------------------------------------------
[2m[36m(pid=10664)[0m 8.7 K     Trainable params
[2m[36m(pid=10664)[0m 0         Non-trainable params
[2m[36m(pid=10664)[0m 8.7 K     Total params
[2m[36m(pid=1110)[0m time to fit was 140.04798126220703
[2m[36m(pid=1110)[0m GPU available: False, used: False
[2m[36m(pid=1110)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=1110)[0m 
[2m[36m(pid=1110)[0m   | Name      | Type              | Params
[2m[36m(pid=1110)[0m ------------------------------------------------
[2m[36m(pid=1110)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=1110)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=1110)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=1110)[0m ------------------------------------------------
[2m[36m(pid=1110)[0m 8.7 K     Trainable params
[2m[36m(pid=1110)[0m 0         Non-trainable params
[2m[36m(pid=1110)[0m 8.7 K     Total params
[2m[36m(pid=2527)[0m time to fit was 209.80069947242737
[2m[36m(pid=2527)[0m GPU available: False, used: False
[2m[36m(pid=2527)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=2527)[0m 
[2m[36m(pid=2527)[0m   | Name      | Type              | Params
[2m[36m(pid=2527)[0m ------------------------------------------------
[2m[36m(pid=2527)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=2527)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=2527)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=2527)[0m ------------------------------------------------
[2m[36m(pid=2527)[0m 8.7 K     Trainable params
[2m[36m(pid=2527)[0m 0         Non-trainable params
[2m[36m(pid=2527)[0m 8.7 K     Total params
[2m[36m(pid=31006)[0m time to fit was 924.2075850963593
[2m[36m(pid=31006)[0m GPU available: False, used: False
[2m[36m(pid=31006)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31006)[0m 
[2m[36m(pid=31006)[0m   | Name      | Type              | Params
[2m[36m(pid=31006)[0m ------------------------------------------------
[2m[36m(pid=31006)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31006)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31006)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31006)[0m ------------------------------------------------
[2m[36m(pid=31006)[0m 8.7 K     Trainable params
[2m[36m(pid=31006)[0m 0         Non-trainable params
[2m[36m(pid=31006)[0m 8.7 K     Total params
[2m[36m(pid=7482)[0m time to fit was 243.15328907966614
[2m[36m(pid=7482)[0m GPU available: False, used: False
[2m[36m(pid=7482)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=7482)[0m 
[2m[36m(pid=7482)[0m   | Name      | Type              | Params
[2m[36m(pid=7482)[0m ------------------------------------------------
[2m[36m(pid=7482)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=7482)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=7482)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=7482)[0m ------------------------------------------------
[2m[36m(pid=7482)[0m 8.7 K     Trainable params
[2m[36m(pid=7482)[0m 0         Non-trainable params
[2m[36m(pid=7482)[0m 8.7 K     Total params
[2m[36m(pid=42969)[0m time to fit was 318.7730212211609
[2m[36m(pid=42969)[0m GPU available: False, used: False
[2m[36m(pid=42969)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=42969)[0m 
[2m[36m(pid=42969)[0m   | Name      | Type              | Params
[2m[36m(pid=42969)[0m ------------------------------------------------
[2m[36m(pid=42969)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=42969)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=42969)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=42969)[0m ------------------------------------------------
[2m[36m(pid=42969)[0m 8.7 K     Trainable params
[2m[36m(pid=42969)[0m 0         Non-trainable params
[2m[36m(pid=42969)[0m 8.7 K     Total params
[2m[36m(pid=6688)[0m time to fit was 1776.326468706131
[2m[36m(pid=6688)[0m GPU available: False, used: False
[2m[36m(pid=6688)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=6688)[0m 
[2m[36m(pid=6688)[0m   | Name      | Type              | Params
[2m[36m(pid=6688)[0m ------------------------------------------------
[2m[36m(pid=6688)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=6688)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=6688)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=6688)[0m ------------------------------------------------
[2m[36m(pid=6688)[0m 8.7 K     Trainable params
[2m[36m(pid=6688)[0m 0         Non-trainable params
[2m[36m(pid=6688)[0m 8.7 K     Total params
[2m[36m(pid=10664)[0m time to fit was 92.87308859825134
[2m[36m(pid=10664)[0m GPU available: False, used: False
[2m[36m(pid=10664)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10664)[0m 
[2m[36m(pid=10664)[0m   | Name      | Type              | Params
[2m[36m(pid=10664)[0m ------------------------------------------------
[2m[36m(pid=10664)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10664)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10664)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10664)[0m ------------------------------------------------
[2m[36m(pid=10664)[0m 8.7 K     Trainable params
[2m[36m(pid=10664)[0m 0         Non-trainable params
[2m[36m(pid=10664)[0m 8.7 K     Total params
[2m[36m(pid=3499)[0m time to fit was 423.5764527320862
[2m[36m(pid=3499)[0m GPU available: False, used: False
[2m[36m(pid=3499)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=3499)[0m 
[2m[36m(pid=3499)[0m   | Name      | Type              | Params
[2m[36m(pid=3499)[0m ------------------------------------------------
[2m[36m(pid=3499)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=3499)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=3499)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=3499)[0m ------------------------------------------------
[2m[36m(pid=3499)[0m 8.7 K     Trainable params
[2m[36m(pid=3499)[0m 0         Non-trainable params
[2m[36m(pid=3499)[0m 8.7 K     Total params
[2m[36m(pid=8610)[0m time to fit was 144.20847964286804
[2m[36m(pid=8610)[0m GPU available: False, used: False
[2m[36m(pid=8610)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8610)[0m 
[2m[36m(pid=8610)[0m   | Name      | Type              | Params
[2m[36m(pid=8610)[0m ------------------------------------------------
[2m[36m(pid=8610)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8610)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8610)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8610)[0m ------------------------------------------------
[2m[36m(pid=8610)[0m 8.7 K     Trainable params
[2m[36m(pid=8610)[0m 0         Non-trainable params
[2m[36m(pid=8610)[0m 8.7 K     Total params
[2m[36m(pid=8639)[0m time to fit was 568.4847028255463
[2m[36m(pid=8639)[0m GPU available: False, used: False
[2m[36m(pid=8639)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8639)[0m 
[2m[36m(pid=8639)[0m   | Name      | Type              | Params
[2m[36m(pid=8639)[0m ------------------------------------------------
[2m[36m(pid=8639)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8639)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8639)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8639)[0m ------------------------------------------------
[2m[36m(pid=8639)[0m 8.7 K     Trainable params
[2m[36m(pid=8639)[0m 0         Non-trainable params
[2m[36m(pid=8639)[0m 8.7 K     Total params
[2m[36m(pid=23636)[0m time to fit was 1779.8128321170807
[2m[36m(pid=23636)[0m GPU available: False, used: False
[2m[36m(pid=23636)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23636)[0m 
[2m[36m(pid=23636)[0m   | Name      | Type              | Params
[2m[36m(pid=23636)[0m ------------------------------------------------
[2m[36m(pid=23636)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23636)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23636)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23636)[0m ------------------------------------------------
[2m[36m(pid=23636)[0m 8.7 K     Trainable params
[2m[36m(pid=23636)[0m 0         Non-trainable params
[2m[36m(pid=23636)[0m 8.7 K     Total params
[2m[36m(pid=31016)[0m time to fit was 466.7593936920166
[2m[36m(pid=31016)[0m GPU available: False, used: False
[2m[36m(pid=31016)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31016)[0m 
[2m[36m(pid=31016)[0m   | Name      | Type              | Params
[2m[36m(pid=31016)[0m ------------------------------------------------
[2m[36m(pid=31016)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31016)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31016)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31016)[0m ------------------------------------------------
[2m[36m(pid=31016)[0m 8.7 K     Trainable params
[2m[36m(pid=31016)[0m 0         Non-trainable params
[2m[36m(pid=31016)[0m 8.7 K     Total params
[2m[36m(pid=10664)[0m time to fit was 96.11300873756409
[2m[36m(pid=10664)[0m GPU available: False, used: False
[2m[36m(pid=10664)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10664)[0m 
[2m[36m(pid=10664)[0m   | Name      | Type              | Params
[2m[36m(pid=10664)[0m ------------------------------------------------
[2m[36m(pid=10664)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10664)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10664)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10664)[0m ------------------------------------------------
[2m[36m(pid=10664)[0m 8.7 K     Trainable params
[2m[36m(pid=10664)[0m 0         Non-trainable params
[2m[36m(pid=10664)[0m 8.7 K     Total params
[2m[36m(pid=23089)[0m time to fit was 226.84353184700012
[2m[36m(pid=23089)[0m Finished run with seed 0 - lr 2 - sec_lr 2 - bs 64 - mean val auc: 0.7432752311229706
Result for _inner_e8deb_00141:
  auc: 0.7432752311229706
  date: 2021-03-19_14-19-53
  done: false
  experiment_id: 1971084ecc67441a8510583feba8135e
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 23089
  time_since_restore: 1602.9726037979126
  time_this_iter_s: 1602.9726037979126
  time_total_s: 1602.9726037979126
  timestamp: 1616159993
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00141
  
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 165/180 (1 PENDING, 26 RUNNING, 138 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    |       |           32 |     0 | 5     |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00130 | RUNNING    |       |           32 |     0 | 0.1   |    2     |        |                  |          |
| _inner_e8deb_00135 | RUNNING    |       |           32 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00164 | PENDING    |       |          512 |     0 | 0.1   |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 145 more trials not shown (16 RUNNING, 128 TERMINATED)


Result for _inner_e8deb_00141:
  auc: 0.7432752311229706
  date: 2021-03-19_14-19-53
  done: true
  experiment_id: 1971084ecc67441a8510583feba8135e
  experiment_tag: 141_batch_size=64,eta=0.0,lr=2,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 23089
  time_since_restore: 1602.9726037979126
  time_this_iter_s: 1602.9726037979126
  time_total_s: 1602.9726037979126
  timestamp: 1616159993
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00141
  
[2m[36m(pid=49344)[0m time to fit was 396.80382108688354
[2m[36m(pid=49344)[0m GPU available: False, used: False
[2m[36m(pid=49344)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49344)[0m 
[2m[36m(pid=49344)[0m   | Name      | Type              | Params
[2m[36m(pid=49344)[0m ------------------------------------------------
[2m[36m(pid=49344)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49344)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49344)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49344)[0m ------------------------------------------------
[2m[36m(pid=49344)[0m 8.7 K     Trainable params
[2m[36m(pid=49344)[0m 0         Non-trainable params
[2m[36m(pid=49344)[0m 8.7 K     Total params
[2m[36m(pid=19179)[0m Starting run with seed 0 - lr 0.1 - sec_lr 5 - bs 512
[2m[36m(pid=19179)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=19179)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=19179)[0m GPU available: False, used: False
[2m[36m(pid=19179)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=19179)[0m 
[2m[36m(pid=19179)[0m   | Name      | Type              | Params
[2m[36m(pid=19179)[0m ------------------------------------------------
[2m[36m(pid=19179)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=19179)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=19179)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=19179)[0m ------------------------------------------------
[2m[36m(pid=19179)[0m 8.7 K     Trainable params
[2m[36m(pid=19179)[0m 0         Non-trainable params
[2m[36m(pid=19179)[0m 8.7 K     Total params
[2m[36m(pid=19179)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=19179)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=19179)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=19179)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=19179)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=19179)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=47886)[0m time to fit was 642.0830750465393
[2m[36m(pid=47886)[0m GPU available: False, used: False
[2m[36m(pid=47886)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=47886)[0m 
[2m[36m(pid=47886)[0m   | Name      | Type              | Params
[2m[36m(pid=47886)[0m ------------------------------------------------
[2m[36m(pid=47886)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=47886)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=47886)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=47886)[0m ------------------------------------------------
[2m[36m(pid=47886)[0m 8.7 K     Trainable params
[2m[36m(pid=47886)[0m 0         Non-trainable params
[2m[36m(pid=47886)[0m 8.7 K     Total params
[2m[36m(pid=564)[0m time to fit was 238.99376010894775
[2m[36m(pid=564)[0m GPU available: False, used: False
[2m[36m(pid=564)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=564)[0m 
[2m[36m(pid=564)[0m   | Name      | Type              | Params
[2m[36m(pid=564)[0m ------------------------------------------------
[2m[36m(pid=564)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=564)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=564)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=564)[0m ------------------------------------------------
[2m[36m(pid=564)[0m 8.7 K     Trainable params
[2m[36m(pid=564)[0m 0         Non-trainable params
[2m[36m(pid=564)[0m 8.7 K     Total params
[2m[36m(pid=11272)[0m time to fit was 394.6752848625183
Result for _inner_e8deb_00085:
  auc: 0.6898356795310974
  date: 2021-03-19_14-20-13
  done: false
  experiment_id: a1282df116e946e2b7fd47b53877c545
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 11272
  time_since_restore: 5434.641186714172
  time_this_iter_s: 5434.641186714172
  time_total_s: 5434.641186714172
  timestamp: 1616160013
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00085
  
[2m[36m(pid=11272)[0m Finished run with seed 0 - lr 5 - sec_lr 0.1 - bs 32 - mean val auc: 0.6898356795310974
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 166/180 (1 PENDING, 26 RUNNING, 139 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00085 | RUNNING    | 145.101.32.82:11272 |           32 |     0 | 5     |    0.1   |      1 |          5434.64 | 0.689836 |
| _inner_e8deb_00090 | RUNNING    |                     |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |                     |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |                     |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |                     |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00130 | RUNNING    |                     |           32 |     0 | 0.1   |    2     |        |                  |          |
| _inner_e8deb_00135 | RUNNING    |                     |           32 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |                     |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00165 | PENDING    |                     |           32 |     0 | 1     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 146 more trials not shown (16 RUNNING, 129 TERMINATED)


Result for _inner_e8deb_00085:
  auc: 0.6898356795310974
  date: 2021-03-19_14-20-13
  done: true
  experiment_id: a1282df116e946e2b7fd47b53877c545
  experiment_tag: 85_batch_size=32,eta=0.0,lr=5,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 11272
  time_since_restore: 5434.641186714172
  time_this_iter_s: 5434.641186714172
  time_total_s: 5434.641186714172
  timestamp: 1616160013
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00085
  
[2m[36m(pid=19841)[0m Starting run with seed 0 - lr 1 - sec_lr 5 - bs 32
[2m[36m(pid=19841)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=19841)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=19841)[0m GPU available: False, used: False
[2m[36m(pid=19841)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=19841)[0m 
[2m[36m(pid=19841)[0m   | Name      | Type              | Params
[2m[36m(pid=19841)[0m ------------------------------------------------
[2m[36m(pid=19841)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=19841)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=19841)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=19841)[0m ------------------------------------------------
[2m[36m(pid=19841)[0m 8.7 K     Trainable params
[2m[36m(pid=19841)[0m 0         Non-trainable params
[2m[36m(pid=19841)[0m 8.7 K     Total params
[2m[36m(pid=19841)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=19841)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=19841)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=19841)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=19841)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=19841)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=2527)[0m time to fit was 216.23756766319275
[2m[36m(pid=8610)[0m time to fit was 125.31200098991394
[2m[36m(pid=2527)[0m GPU available: False, used: False
[2m[36m(pid=2527)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=2527)[0m 
[2m[36m(pid=2527)[0m   | Name      | Type              | Params
[2m[36m(pid=2527)[0m ------------------------------------------------
[2m[36m(pid=2527)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=2527)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=2527)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=2527)[0m ------------------------------------------------
[2m[36m(pid=2527)[0m 8.7 K     Trainable params
[2m[36m(pid=2527)[0m 0         Non-trainable params
[2m[36m(pid=2527)[0m 8.7 K     Total params
[2m[36m(pid=8610)[0m GPU available: False, used: False
[2m[36m(pid=8610)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8610)[0m 
[2m[36m(pid=8610)[0m   | Name      | Type              | Params
[2m[36m(pid=8610)[0m ------------------------------------------------
[2m[36m(pid=8610)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8610)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8610)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8610)[0m ------------------------------------------------
[2m[36m(pid=8610)[0m 8.7 K     Trainable params
[2m[36m(pid=8610)[0m 0         Non-trainable params
[2m[36m(pid=8610)[0m 8.7 K     Total params
[2m[36m(pid=22604)[0m time to fit was 463.83832359313965
[2m[36m(pid=22604)[0m GPU available: False, used: False
[2m[36m(pid=22604)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=22604)[0m 
[2m[36m(pid=22604)[0m   | Name      | Type              | Params
[2m[36m(pid=22604)[0m ------------------------------------------------
[2m[36m(pid=22604)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=22604)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=22604)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=22604)[0m ------------------------------------------------
[2m[36m(pid=22604)[0m 8.7 K     Trainable params
[2m[36m(pid=22604)[0m 0         Non-trainable params
[2m[36m(pid=22604)[0m 8.7 K     Total params
[2m[36m(pid=19179)[0m time to fit was 60.07976984977722
[2m[36m(pid=19179)[0m GPU available: False, used: False
[2m[36m(pid=19179)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=19179)[0m 
[2m[36m(pid=19179)[0m   | Name      | Type              | Params
[2m[36m(pid=19179)[0m ------------------------------------------------
[2m[36m(pid=19179)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=19179)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=19179)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=19179)[0m ------------------------------------------------
[2m[36m(pid=19179)[0m 8.7 K     Trainable params
[2m[36m(pid=19179)[0m 0         Non-trainable params
[2m[36m(pid=19179)[0m 8.7 K     Total params
[2m[36m(pid=10664)[0m time to fit was 98.97341442108154
[2m[36m(pid=10664)[0m GPU available: False, used: False
[2m[36m(pid=10664)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=10664)[0m 
[2m[36m(pid=10664)[0m   | Name      | Type              | Params
[2m[36m(pid=10664)[0m ------------------------------------------------
[2m[36m(pid=10664)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=10664)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=10664)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=10664)[0m ------------------------------------------------
[2m[36m(pid=10664)[0m 8.7 K     Trainable params
[2m[36m(pid=10664)[0m 0         Non-trainable params
[2m[36m(pid=10664)[0m 8.7 K     Total params
[2m[36m(pid=1110)[0m time to fit was 308.1962869167328
[2m[36m(pid=1110)[0m GPU available: False, used: False
[2m[36m(pid=1110)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=1110)[0m 
[2m[36m(pid=1110)[0m   | Name      | Type              | Params
[2m[36m(pid=1110)[0m ------------------------------------------------
[2m[36m(pid=1110)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=1110)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=1110)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=1110)[0m ------------------------------------------------
[2m[36m(pid=1110)[0m 8.7 K     Trainable params
[2m[36m(pid=1110)[0m 0         Non-trainable params
[2m[36m(pid=1110)[0m 8.7 K     Total params
[2m[36m(pid=52747)[0m time to fit was 462.44834876060486
Result for _inner_e8deb_00130:
  auc: 0.909476923942566
  date: 2021-03-19_14-21-57
  done: false
  experiment_id: aba48391879c42af94b14b1a56f03db4
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 52747
  time_since_restore: 2418.568858861923
  time_this_iter_s: 2418.568858861923
  time_total_s: 2418.568858861923
  timestamp: 1616160117
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00130
  
[2m[36m(pid=52747)[0m Finished run with seed 0 - lr 0.1 - sec_lr 2 - bs 32 - mean val auc: 0.909476923942566
[2m[36m(pid=7482)[0m time to fit was 263.11683225631714
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 167/180 (1 PENDING, 26 RUNNING, 140 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |                     |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |                     |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |                     |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |                     |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00130 | RUNNING    | 145.101.32.82:52747 |           32 |     0 | 0.1   |    2     |      1 |          2418.57 | 0.909477 |
| _inner_e8deb_00135 | RUNNING    |                     |           32 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |                     |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                     |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00166 | PENDING    |                     |           64 |     0 | 1     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 147 more trials not shown (16 RUNNING, 130 TERMINATED)


Result for _inner_e8deb_00130:
  auc: 0.909476923942566
  date: 2021-03-19_14-21-57
  done: true
  experiment_id: aba48391879c42af94b14b1a56f03db4
  experiment_tag: 130_batch_size=32,eta=0.0,lr=0.1,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 52747
  time_since_restore: 2418.568858861923
  time_this_iter_s: 2418.568858861923
  time_total_s: 2418.568858861923
  timestamp: 1616160117
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00130
  
[2m[36m(pid=7482)[0m GPU available: False, used: False
[2m[36m(pid=7482)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=7482)[0m 
[2m[36m(pid=7482)[0m   | Name      | Type              | Params
[2m[36m(pid=7482)[0m ------------------------------------------------
[2m[36m(pid=7482)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=7482)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=7482)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=7482)[0m ------------------------------------------------
[2m[36m(pid=7482)[0m 8.7 K     Trainable params
[2m[36m(pid=7482)[0m 0         Non-trainable params
[2m[36m(pid=7482)[0m 8.7 K     Total params
[2m[36m(pid=19179)[0m time to fit was 56.142739057540894
[2m[36m(pid=19179)[0m GPU available: False, used: False
[2m[36m(pid=19179)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=19179)[0m 
[2m[36m(pid=19179)[0m   | Name      | Type              | Params
[2m[36m(pid=19179)[0m ------------------------------------------------
[2m[36m(pid=19179)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=19179)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=19179)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=19179)[0m ------------------------------------------------
[2m[36m(pid=19179)[0m 8.7 K     Trainable params
[2m[36m(pid=19179)[0m 0         Non-trainable params
[2m[36m(pid=19179)[0m 8.7 K     Total params
[2m[36m(pid=23176)[0m Starting run with seed 0 - lr 1 - sec_lr 5 - bs 64
[2m[36m(pid=23176)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=23176)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=23176)[0m GPU available: False, used: False
[2m[36m(pid=23176)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23176)[0m 
[2m[36m(pid=23176)[0m   | Name      | Type              | Params
[2m[36m(pid=23176)[0m ------------------------------------------------
[2m[36m(pid=23176)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23176)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23176)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23176)[0m ------------------------------------------------
[2m[36m(pid=23176)[0m 8.7 K     Trainable params
[2m[36m(pid=23176)[0m 0         Non-trainable params
[2m[36m(pid=23176)[0m 8.7 K     Total params
[2m[36m(pid=23176)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=23176)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23176)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=23176)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=23176)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=23176)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=2527)[0m time to fit was 97.49946594238281
[2m[36m(pid=2527)[0m GPU available: False, used: False
[2m[36m(pid=2527)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=2527)[0m 
[2m[36m(pid=2527)[0m   | Name      | Type              | Params
[2m[36m(pid=2527)[0m ------------------------------------------------
[2m[36m(pid=2527)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=2527)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=2527)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=2527)[0m ------------------------------------------------
[2m[36m(pid=2527)[0m 8.7 K     Trainable params
[2m[36m(pid=2527)[0m 0         Non-trainable params
[2m[36m(pid=2527)[0m 8.7 K     Total params
[2m[36m(pid=10664)[0m time to fit was 85.50818872451782
Result for _inner_e8deb_00163:
  auc: 0.9106068134307861
  date: 2021-03-19_14-22-50
  done: false
  experiment_id: e86836949cd947a68c819b167c387575
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 10664
  time_since_restore: 446.49102306365967
  time_this_iter_s: 446.49102306365967
  time_total_s: 446.49102306365967
  timestamp: 1616160170
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00163
  
[2m[36m(pid=10664)[0m Finished run with seed 0 - lr 0.1 - sec_lr 5 - bs 256 - mean val auc: 0.9106068134307861
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 168/180 (1 PENDING, 26 RUNNING, 141 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00135 | RUNNING    |       |           32 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00167 | PENDING    |       |          128 |     0 | 1     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 148 more trials not shown (16 RUNNING, 131 TERMINATED)


Result for _inner_e8deb_00163:
  auc: 0.9106068134307861
  date: 2021-03-19_14-22-50
  done: true
  experiment_id: e86836949cd947a68c819b167c387575
  experiment_tag: 163_batch_size=256,eta=0.0,lr=0.1,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 10664
  time_since_restore: 446.49102306365967
  time_this_iter_s: 446.49102306365967
  time_total_s: 446.49102306365967
  timestamp: 1616160170
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00163
  
2021-03-19 14:22:51,763	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff89225da301000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=42969)[0m time to fit was 312.8709797859192
Result for _inner_e8deb_00153:
  auc: 0.8530268192291259
  date: 2021-03-19_14-22-57
  done: false
  experiment_id: fa3cb5d83e1e4af98c62f256f662f3e1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 42969
  time_since_restore: 1124.6831307411194
  time_this_iter_s: 1124.6831307411194
  time_total_s: 1124.6831307411194
  timestamp: 1616160177
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00153
  
[2m[36m(pid=42969)[0m Finished run with seed 0 - lr 0.001 - sec_lr 5 - bs 256 - mean val auc: 0.8530268192291259
== Status ==
Memory usage on this node: 9.7/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 169/180 (1 PENDING, 26 RUNNING, 142 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    |       |           32 |     0 | 0.001 |    1     |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00135 | RUNNING    |       |           32 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00168 | PENDING    |       |          256 |     0 | 1     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 149 more trials not shown (16 RUNNING, 132 TERMINATED)


Result for _inner_e8deb_00153:
  auc: 0.8530268192291259
  date: 2021-03-19_14-22-57
  done: true
  experiment_id: fa3cb5d83e1e4af98c62f256f662f3e1
  experiment_tag: 153_batch_size=256,eta=0.0,lr=0.001,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 42969
  time_since_restore: 1124.6831307411194
  time_this_iter_s: 1124.6831307411194
  time_total_s: 1124.6831307411194
  timestamp: 1616160177
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00153
  
[2m[36m(pid=19179)[0m time to fit was 55.271430253982544
[2m[36m(pid=19179)[0m GPU available: False, used: False
[2m[36m(pid=19179)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=19179)[0m 
[2m[36m(pid=19179)[0m   | Name      | Type              | Params
[2m[36m(pid=19179)[0m ------------------------------------------------
[2m[36m(pid=19179)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=19179)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=19179)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=19179)[0m ------------------------------------------------
[2m[36m(pid=19179)[0m 8.7 K     Trainable params
[2m[36m(pid=19179)[0m 0         Non-trainable params
[2m[36m(pid=19179)[0m 8.7 K     Total params
[2m[36m(pid=24926)[0m Starting run with seed 0 - lr 1 - sec_lr 5 - bs 128
[2m[36m(pid=24926)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=24926)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=24926)[0m GPU available: False, used: False
[2m[36m(pid=24926)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=24926)[0m 
[2m[36m(pid=24926)[0m   | Name      | Type              | Params
[2m[36m(pid=24926)[0m ------------------------------------------------
[2m[36m(pid=24926)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=24926)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=24926)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=24926)[0m ------------------------------------------------
[2m[36m(pid=24926)[0m 8.7 K     Trainable params
[2m[36m(pid=24926)[0m 0         Non-trainable params
[2m[36m(pid=24926)[0m 8.7 K     Total params
[2m[36m(pid=24926)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=24926)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=24926)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=24926)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=24926)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=24926)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=25100)[0m Starting run with seed 0 - lr 1 - sec_lr 5 - bs 256
[2m[36m(pid=25100)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=25100)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=25100)[0m GPU available: False, used: False
[2m[36m(pid=25100)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=25100)[0m 
[2m[36m(pid=25100)[0m   | Name      | Type              | Params
[2m[36m(pid=25100)[0m ------------------------------------------------
[2m[36m(pid=25100)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=25100)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=25100)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=25100)[0m ------------------------------------------------
[2m[36m(pid=25100)[0m 8.7 K     Trainable params
[2m[36m(pid=25100)[0m 0         Non-trainable params
[2m[36m(pid=25100)[0m 8.7 K     Total params
[2m[36m(pid=25100)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=25100)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8610)[0m time to fit was 146.75770258903503
[2m[36m(pid=8610)[0m GPU available: False, used: False
[2m[36m(pid=8610)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=8610)[0m 
[2m[36m(pid=8610)[0m   | Name      | Type              | Params
[2m[36m(pid=8610)[0m ------------------------------------------------
[2m[36m(pid=8610)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=8610)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=8610)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=8610)[0m ------------------------------------------------
[2m[36m(pid=8610)[0m 8.7 K     Trainable params
[2m[36m(pid=8610)[0m 0         Non-trainable params
[2m[36m(pid=8610)[0m 8.7 K     Total params
[2m[36m(pid=25100)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=25100)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=25100)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=25100)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=564)[0m time to fit was 180.2475621700287
[2m[36m(pid=564)[0m GPU available: False, used: False
[2m[36m(pid=564)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=564)[0m 
[2m[36m(pid=564)[0m   | Name      | Type              | Params
[2m[36m(pid=564)[0m ------------------------------------------------
[2m[36m(pid=564)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=564)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=564)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=564)[0m ------------------------------------------------
[2m[36m(pid=564)[0m 8.7 K     Trainable params
[2m[36m(pid=564)[0m 0         Non-trainable params
[2m[36m(pid=564)[0m 8.7 K     Total params
[2m[36m(pid=25102)[0m time to fit was 1779.4866950511932
Result for _inner_e8deb_00090:
  auc: 0.8905896067619323
  date: 2021-03-19_14-23-30
  done: false
  experiment_id: ffb52946fcd747928e69f16091ea5305
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 25102
  time_since_restore: 5155.131307125092
  time_this_iter_s: 5155.131307125092
  time_total_s: 5155.131307125092
  timestamp: 1616160210
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00090
  
[2m[36m(pid=25102)[0m Finished run with seed 0 - lr 0.001 - sec_lr 1 - bs 32 - mean val auc: 0.8905896067619323
== Status ==
Memory usage on this node: 9.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 170/180 (1 PENDING, 26 RUNNING, 143 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00090 | RUNNING    | 145.101.32.82:25102 |           32 |     0 | 0.001 |    1     |      1 |          5155.13 | 0.89059  |
| _inner_e8deb_00115 | RUNNING    |                     |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |                     |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |                     |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00135 | RUNNING    |                     |           32 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |                     |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                     |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |                     |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00169 | PENDING    |                     |          512 |     0 | 1     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 150 more trials not shown (16 RUNNING, 133 TERMINATED)


Result for _inner_e8deb_00090:
  auc: 0.8905896067619323
  date: 2021-03-19_14-23-30
  done: true
  experiment_id: ffb52946fcd747928e69f16091ea5305
  experiment_tag: 90_batch_size=32,eta=0.0,lr=0.001,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 25102
  time_since_restore: 5155.131307125092
  time_this_iter_s: 5155.131307125092
  time_total_s: 5155.131307125092
  timestamp: 1616160210
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00090
  
2021-03-19 14:23:31,873	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff2ab1eae001000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=26177)[0m Starting run with seed 0 - lr 1 - sec_lr 5 - bs 512
[2m[36m(pid=26177)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=26177)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=26177)[0m GPU available: False, used: False
[2m[36m(pid=26177)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26177)[0m 
[2m[36m(pid=26177)[0m   | Name      | Type              | Params
[2m[36m(pid=26177)[0m ------------------------------------------------
[2m[36m(pid=26177)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26177)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26177)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26177)[0m ------------------------------------------------
[2m[36m(pid=26177)[0m 8.7 K     Trainable params
[2m[36m(pid=26177)[0m 0         Non-trainable params
[2m[36m(pid=26177)[0m 8.7 K     Total params
[2m[36m(pid=26177)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=26177)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=1110)[0m time to fit was 110.12885785102844
Result for _inner_e8deb_00158:
  auc: 0.9089365124702453
  date: 2021-03-19_14-23-42
  done: false
  experiment_id: c6b369a2b61049fbb381e98f6440cc37
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 1110
  time_since_restore: 816.6174716949463
  time_this_iter_s: 816.6174716949463
  time_total_s: 816.6174716949463
  timestamp: 1616160222
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00158
  
[2m[36m(pid=1110)[0m Finished run with seed 0 - lr 0.01 - sec_lr 5 - bs 256 - mean val auc: 0.9089365124702453
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 171/180 (1 PENDING, 26 RUNNING, 144 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00135 | RUNNING    |       |           32 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |       |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00170 | PENDING    |       |           32 |     0 | 2     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 151 more trials not shown (16 RUNNING, 134 TERMINATED)


Result for _inner_e8deb_00158:
  auc: 0.9089365124702453
  date: 2021-03-19_14-23-42
  done: true
  experiment_id: c6b369a2b61049fbb381e98f6440cc37
  experiment_tag: 158_batch_size=256,eta=0.0,lr=0.01,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 1110
  time_since_restore: 816.6174716949463
  time_this_iter_s: 816.6174716949463
  time_total_s: 816.6174716949463
  timestamp: 1616160222
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00158
  
[2m[36m(pid=26177)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=26177)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=26177)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=26177)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=19179)[0m time to fit was 51.21289014816284
[2m[36m(pid=19179)[0m GPU available: False, used: False
[2m[36m(pid=19179)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=19179)[0m 
[2m[36m(pid=19179)[0m   | Name      | Type              | Params
[2m[36m(pid=19179)[0m ------------------------------------------------
[2m[36m(pid=19179)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=19179)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=19179)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=19179)[0m ------------------------------------------------
[2m[36m(pid=19179)[0m 8.7 K     Trainable params
[2m[36m(pid=19179)[0m 0         Non-trainable params
[2m[36m(pid=19179)[0m 8.7 K     Total params
[2m[36m(pid=26552)[0m Starting run with seed 0 - lr 2 - sec_lr 5 - bs 32
[2m[36m(pid=26552)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=26552)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=26552)[0m GPU available: False, used: False
[2m[36m(pid=26552)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26552)[0m 
[2m[36m(pid=26552)[0m   | Name      | Type              | Params
[2m[36m(pid=26552)[0m ------------------------------------------------
[2m[36m(pid=26552)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26552)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26552)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26552)[0m ------------------------------------------------
[2m[36m(pid=26552)[0m 8.7 K     Trainable params
[2m[36m(pid=26552)[0m 0         Non-trainable params
[2m[36m(pid=26552)[0m 8.7 K     Total params
[2m[36m(pid=26552)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=26552)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=26552)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=26552)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=26552)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=26552)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=2527)[0m time to fit was 103.25176858901978
Result for _inner_e8deb_00159:
  auc: 0.9084614396095276
  date: 2021-03-19_14-24-03
  done: false
  experiment_id: f1ba075713404912b556f748452bfb63
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 2527
  time_since_restore: 792.3751664161682
  time_this_iter_s: 792.3751664161682
  time_total_s: 792.3751664161682
  timestamp: 1616160243
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00159
  
[2m[36m(pid=2527)[0m Finished run with seed 0 - lr 0.01 - sec_lr 5 - bs 512 - mean val auc: 0.9084614396095276
== Status ==
Memory usage on this node: 9.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 172/180 (1 PENDING, 26 RUNNING, 145 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00135 | RUNNING    |       |           32 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |       |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00171 | PENDING    |       |           64 |     0 | 2     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 152 more trials not shown (16 RUNNING, 135 TERMINATED)


Result for _inner_e8deb_00159:
  auc: 0.9084614396095276
  date: 2021-03-19_14-24-03
  done: true
  experiment_id: f1ba075713404912b556f748452bfb63
  experiment_tag: 159_batch_size=512,eta=0.0,lr=0.01,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 2527
  time_since_restore: 792.3751664161682
  time_this_iter_s: 792.3751664161682
  time_total_s: 792.3751664161682
  timestamp: 1616160243
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00159
  
[2m[36m(pid=27334)[0m Starting run with seed 0 - lr 2 - sec_lr 5 - bs 64
[2m[36m(pid=27334)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=27334)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=27334)[0m GPU available: False, used: False
[2m[36m(pid=27334)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27334)[0m 
[2m[36m(pid=27334)[0m   | Name      | Type              | Params
[2m[36m(pid=27334)[0m ------------------------------------------------
[2m[36m(pid=27334)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27334)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27334)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27334)[0m ------------------------------------------------
[2m[36m(pid=27334)[0m 8.7 K     Trainable params
[2m[36m(pid=27334)[0m 0         Non-trainable params
[2m[36m(pid=27334)[0m 8.7 K     Total params
[2m[36m(pid=27334)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=27334)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=27334)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=27334)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=27334)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=27334)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=25100)[0m time to fit was 84.65194606781006
[2m[36m(pid=25100)[0m GPU available: False, used: False
[2m[36m(pid=25100)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=25100)[0m 
[2m[36m(pid=25100)[0m   | Name      | Type              | Params
[2m[36m(pid=25100)[0m ------------------------------------------------
[2m[36m(pid=25100)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=25100)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=25100)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=25100)[0m ------------------------------------------------
[2m[36m(pid=25100)[0m 8.7 K     Trainable params
[2m[36m(pid=25100)[0m 0         Non-trainable params
[2m[36m(pid=25100)[0m 8.7 K     Total params
[2m[36m(pid=19179)[0m time to fit was 55.13846158981323
Result for _inner_e8deb_00164:
  auc: 0.9109229564666748
  date: 2021-03-19_14-24-44
  done: false
  experiment_id: a5123b282eea4b7cabed0727b424b283
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 19179
  time_since_restore: 279.03993940353394
  time_this_iter_s: 279.03993940353394
  time_total_s: 279.03993940353394
  timestamp: 1616160284
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00164
  
[2m[36m(pid=19179)[0m Finished run with seed 0 - lr 0.1 - sec_lr 5 - bs 512 - mean val auc: 0.9109229564666748
== Status ==
Memory usage on this node: 9.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 173/180 (1 PENDING, 26 RUNNING, 146 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00135 | RUNNING    |       |           32 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |       |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00172 | PENDING    |       |          128 |     0 | 2     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 153 more trials not shown (16 RUNNING, 136 TERMINATED)


Result for _inner_e8deb_00164:
  auc: 0.9109229564666748
  date: 2021-03-19_14-24-44
  done: true
  experiment_id: a5123b282eea4b7cabed0727b424b283
  experiment_tag: 164_batch_size=512,eta=0.0,lr=0.1,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 19179
  time_since_restore: 279.03993940353394
  time_this_iter_s: 279.03993940353394
  time_total_s: 279.03993940353394
  timestamp: 1616160284
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00164
  
[2m[36m(pid=26177)[0m time to fit was 68.38001227378845
[2m[36m(pid=26177)[0m GPU available: False, used: False
[2m[36m(pid=26177)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26177)[0m 
[2m[36m(pid=26177)[0m   | Name      | Type              | Params
[2m[36m(pid=26177)[0m ------------------------------------------------
[2m[36m(pid=26177)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26177)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26177)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26177)[0m ------------------------------------------------
[2m[36m(pid=26177)[0m 8.7 K     Trainable params
[2m[36m(pid=26177)[0m 0         Non-trainable params
[2m[36m(pid=26177)[0m 8.7 K     Total params
[2m[36m(pid=28496)[0m Starting run with seed 0 - lr 2 - sec_lr 5 - bs 128
[2m[36m(pid=28496)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=28496)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=28496)[0m GPU available: False, used: False
[2m[36m(pid=28496)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=28496)[0m 
[2m[36m(pid=28496)[0m   | Name      | Type              | Params
[2m[36m(pid=28496)[0m ------------------------------------------------
[2m[36m(pid=28496)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=28496)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=28496)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=28496)[0m ------------------------------------------------
[2m[36m(pid=28496)[0m 8.7 K     Trainable params
[2m[36m(pid=28496)[0m 0         Non-trainable params
[2m[36m(pid=28496)[0m 8.7 K     Total params
[2m[36m(pid=28496)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=28496)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=28496)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=28496)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=28496)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=28496)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8610)[0m time to fit was 115.02904176712036
[2m[36m(pid=8610)[0m Finished run with seed 0 - lr 0.1 - sec_lr 5 - bs 128 - mean val auc: 0.9102550029754639
Result for _inner_e8deb_00162:
  auc: 0.9102550029754639
  date: 2021-03-19_14-25-04
  done: false
  experiment_id: 19fd9aca09e54d50b79978ba0911ea19
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8610
  time_since_restore: 656.5321888923645
  time_this_iter_s: 656.5321888923645
  time_total_s: 656.5321888923645
  timestamp: 1616160304
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00162
  
== Status ==
Memory usage on this node: 9.1/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 174/180 (1 PENDING, 26 RUNNING, 147 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00135 | RUNNING    |       |           32 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |       |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00173 | PENDING    |       |          256 |     0 | 2     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 154 more trials not shown (16 RUNNING, 137 TERMINATED)


Result for _inner_e8deb_00162:
  auc: 0.9102550029754639
  date: 2021-03-19_14-25-04
  done: true
  experiment_id: 19fd9aca09e54d50b79978ba0911ea19
  experiment_tag: 162_batch_size=128,eta=0.0,lr=0.1,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8610
  time_since_restore: 656.5321888923645
  time_this_iter_s: 656.5321888923645
  time_total_s: 656.5321888923645
  timestamp: 1616160304
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00162
  
[2m[36m(pid=31106)[0m time to fit was 726.1750214099884
[2m[36m(pid=31106)[0m GPU available: False, used: False
[2m[36m(pid=31106)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31106)[0m 
[2m[36m(pid=31106)[0m   | Name      | Type              | Params
[2m[36m(pid=31106)[0m ------------------------------------------------
[2m[36m(pid=31106)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31106)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31106)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31106)[0m ------------------------------------------------
[2m[36m(pid=31106)[0m 8.7 K     Trainable params
[2m[36m(pid=31106)[0m 0         Non-trainable params
[2m[36m(pid=31106)[0m 8.7 K     Total params
[2m[36m(pid=24926)[0m time to fit was 124.31598973274231
[2m[36m(pid=24926)[0m GPU available: False, used: False
[2m[36m(pid=24926)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=24926)[0m 
[2m[36m(pid=24926)[0m   | Name      | Type              | Params
[2m[36m(pid=24926)[0m ------------------------------------------------
[2m[36m(pid=24926)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=24926)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=24926)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=24926)[0m ------------------------------------------------
[2m[36m(pid=24926)[0m 8.7 K     Trainable params
[2m[36m(pid=24926)[0m 0         Non-trainable params
[2m[36m(pid=24926)[0m 8.7 K     Total params
[2m[36m(pid=29072)[0m Starting run with seed 0 - lr 2 - sec_lr 5 - bs 256
[2m[36m(pid=29072)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=29072)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=29072)[0m GPU available: False, used: False
[2m[36m(pid=29072)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29072)[0m 
[2m[36m(pid=29072)[0m   | Name      | Type              | Params
[2m[36m(pid=29072)[0m ------------------------------------------------
[2m[36m(pid=29072)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29072)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29072)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29072)[0m ------------------------------------------------
[2m[36m(pid=29072)[0m 8.7 K     Trainable params
[2m[36m(pid=29072)[0m 0         Non-trainable params
[2m[36m(pid=29072)[0m 8.7 K     Total params
[2m[36m(pid=29072)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=29072)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=29072)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=29072)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=29072)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=29072)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=26177)[0m time to fit was 53.2972891330719
[2m[36m(pid=26177)[0m GPU available: False, used: False
[2m[36m(pid=26177)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26177)[0m 
[2m[36m(pid=26177)[0m   | Name      | Type              | Params
[2m[36m(pid=26177)[0m ------------------------------------------------
[2m[36m(pid=26177)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26177)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26177)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26177)[0m ------------------------------------------------
[2m[36m(pid=26177)[0m 8.7 K     Trainable params
[2m[36m(pid=26177)[0m 0         Non-trainable params
[2m[36m(pid=26177)[0m 8.7 K     Total params
[2m[36m(pid=23636)[0m time to fit was 399.3720123767853
[2m[36m(pid=23636)[0m GPU available: False, used: False
[2m[36m(pid=23636)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23636)[0m 
[2m[36m(pid=23636)[0m   | Name      | Type              | Params
[2m[36m(pid=23636)[0m ------------------------------------------------
[2m[36m(pid=23636)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23636)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23636)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23636)[0m ------------------------------------------------
[2m[36m(pid=23636)[0m 8.7 K     Trainable params
[2m[36m(pid=23636)[0m 0         Non-trainable params
[2m[36m(pid=23636)[0m 8.7 K     Total params
[2m[36m(pid=23176)[0m time to fit was 224.84031748771667
[2m[36m(pid=23176)[0m GPU available: False, used: False
[2m[36m(pid=23176)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23176)[0m 
[2m[36m(pid=23176)[0m   | Name      | Type              | Params
[2m[36m(pid=23176)[0m ------------------------------------------------
[2m[36m(pid=23176)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23176)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23176)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23176)[0m ------------------------------------------------
[2m[36m(pid=23176)[0m 8.7 K     Trainable params
[2m[36m(pid=23176)[0m 0         Non-trainable params
[2m[36m(pid=23176)[0m 8.7 K     Total params
[2m[36m(pid=7482)[0m time to fit was 242.56573963165283
[2m[36m(pid=7482)[0m GPU available: False, used: False
[2m[36m(pid=7482)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=7482)[0m 
[2m[36m(pid=7482)[0m   | Name      | Type              | Params
[2m[36m(pid=7482)[0m ------------------------------------------------
[2m[36m(pid=7482)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=7482)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=7482)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=7482)[0m ------------------------------------------------
[2m[36m(pid=7482)[0m 8.7 K     Trainable params
[2m[36m(pid=7482)[0m 0         Non-trainable params
[2m[36m(pid=7482)[0m 8.7 K     Total params
[2m[36m(pid=29455)[0m time to fit was 1755.6728825569153
[2m[36m(pid=29455)[0m GPU available: False, used: False
[2m[36m(pid=29455)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29455)[0m 
[2m[36m(pid=29455)[0m   | Name      | Type              | Params
[2m[36m(pid=29455)[0m ------------------------------------------------
[2m[36m(pid=29455)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29455)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29455)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29455)[0m ------------------------------------------------
[2m[36m(pid=29455)[0m 8.7 K     Trainable params
[2m[36m(pid=29455)[0m 0         Non-trainable params
[2m[36m(pid=29455)[0m 8.7 K     Total params
[2m[36m(pid=3499)[0m time to fit was 461.3805146217346
[2m[36m(pid=3499)[0m GPU available: False, used: False
[2m[36m(pid=3499)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=3499)[0m 
[2m[36m(pid=3499)[0m   | Name      | Type              | Params
[2m[36m(pid=3499)[0m ------------------------------------------------
[2m[36m(pid=3499)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=3499)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=3499)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=3499)[0m ------------------------------------------------
[2m[36m(pid=3499)[0m 8.7 K     Trainable params
[2m[36m(pid=3499)[0m 0         Non-trainable params
[2m[36m(pid=3499)[0m 8.7 K     Total params
[2m[36m(pid=564)[0m time to fit was 177.86537837982178
Result for _inner_e8deb_00157:
  auc: 0.9091006875038147
  date: 2021-03-19_14-26-10
  done: false
  experiment_id: 92727482367f42f7a810b65028b05831
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 564
  time_since_restore: 982.4035029411316
  time_this_iter_s: 982.4035029411316
  time_total_s: 982.4035029411316
  timestamp: 1616160370
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00157
  
[2m[36m(pid=564)[0m Finished run with seed 0 - lr 0.01 - sec_lr 5 - bs 128 - mean val auc: 0.9091006875038147
== Status ==
Memory usage on this node: 9.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 175/180 (1 PENDING, 26 RUNNING, 148 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00135 | RUNNING    |       |           32 |     0 | 1     |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |       |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00174 | PENDING    |       |          512 |     0 | 2     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 155 more trials not shown (16 RUNNING, 138 TERMINATED)


Result for _inner_e8deb_00157:
  auc: 0.9091006875038147
  date: 2021-03-19_14-26-10
  done: true
  experiment_id: 92727482367f42f7a810b65028b05831
  experiment_tag: 157_batch_size=128,eta=0.0,lr=0.01,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 564
  time_since_restore: 982.4035029411316
  time_this_iter_s: 982.4035029411316
  time_total_s: 982.4035029411316
  timestamp: 1616160370
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00157
  
2021-03-19 14:26:12,303	WARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff5d4202bb01000000 cannot be scheduled right now. It requires {CPU: 2.000000} for placement, but this node only has remaining {CPU: 2.000000}, {memory: 252.929688 GiB}, {node:145.101.32.82: 1.000000}, {object_store_memory: 77.539062 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
[2m[36m(pid=25100)[0m time to fit was 101.75916290283203
[2m[36m(pid=25100)[0m GPU available: False, used: False
[2m[36m(pid=25100)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=25100)[0m 
[2m[36m(pid=25100)[0m   | Name      | Type              | Params
[2m[36m(pid=25100)[0m ------------------------------------------------
[2m[36m(pid=25100)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=25100)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=25100)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=25100)[0m ------------------------------------------------
[2m[36m(pid=25100)[0m 8.7 K     Trainable params
[2m[36m(pid=25100)[0m 0         Non-trainable params
[2m[36m(pid=25100)[0m 8.7 K     Total params
[2m[36m(pid=29072)[0m time to fit was 64.79138469696045
[2m[36m(pid=29072)[0m GPU available: False, used: False
[2m[36m(pid=29072)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29072)[0m 
[2m[36m(pid=29072)[0m   | Name      | Type              | Params
[2m[36m(pid=29072)[0m ------------------------------------------------
[2m[36m(pid=29072)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29072)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29072)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29072)[0m ------------------------------------------------
[2m[36m(pid=29072)[0m 8.7 K     Trainable params
[2m[36m(pid=29072)[0m 0         Non-trainable params
[2m[36m(pid=29072)[0m 8.7 K     Total params
[2m[36m(pid=31065)[0m Starting run with seed 0 - lr 2 - sec_lr 5 - bs 512
[2m[36m(pid=31065)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=31065)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=31065)[0m GPU available: False, used: False
[2m[36m(pid=31065)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31065)[0m 
[2m[36m(pid=31065)[0m   | Name      | Type              | Params
[2m[36m(pid=31065)[0m ------------------------------------------------
[2m[36m(pid=31065)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31065)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31065)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31065)[0m ------------------------------------------------
[2m[36m(pid=31065)[0m 8.7 K     Trainable params
[2m[36m(pid=31065)[0m 0         Non-trainable params
[2m[36m(pid=31065)[0m 8.7 K     Total params
[2m[36m(pid=31065)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=31065)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31065)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=31065)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31065)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=31065)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=8639)[0m time to fit was 465.21734833717346
Result for _inner_e8deb_00135:
  auc: 0.9061580300331116
  date: 2021-03-19_14-26-27
  done: false
  experiment_id: 5de603348a5345868e8f38a9005240c0
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8639
  time_since_restore: 2421.967658996582
  time_this_iter_s: 2421.967658996582
  time_total_s: 2421.967658996582
  timestamp: 1616160387
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00135
  
[2m[36m(pid=8639)[0m Finished run with seed 0 - lr 1 - sec_lr 2 - bs 32 - mean val auc: 0.9061580300331116
== Status ==
Memory usage on this node: 9.6/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 176/180 (1 PENDING, 26 RUNNING, 149 TERMINATED)
+--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |                    |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |                    |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |                    |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |                    |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |                    |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00135 | RUNNING    | 145.101.32.82:8639 |           32 |     0 | 1     |    2     |      1 |          2421.97 | 0.906158 |
| _inner_e8deb_00140 | RUNNING    |                    |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                    |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |                    |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |                    |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00175 | PENDING    |                    |           32 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                    |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                    |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                    |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                    |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                    |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                    |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                    |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                    |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                    |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                    |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 156 more trials not shown (16 RUNNING, 139 TERMINATED)


Result for _inner_e8deb_00135:
  auc: 0.9061580300331116
  date: 2021-03-19_14-26-27
  done: true
  experiment_id: 5de603348a5345868e8f38a9005240c0
  experiment_tag: 135_batch_size=32,eta=0.0,lr=1,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 8639
  time_since_restore: 2421.967658996582
  time_this_iter_s: 2421.967658996582
  time_total_s: 2421.967658996582
  timestamp: 1616160387
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00135
  
[2m[36m(pid=26177)[0m time to fit was 53.01940441131592
[2m[36m(pid=26177)[0m GPU available: False, used: False
[2m[36m(pid=26177)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26177)[0m 
[2m[36m(pid=26177)[0m   | Name      | Type              | Params
[2m[36m(pid=26177)[0m ------------------------------------------------
[2m[36m(pid=26177)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26177)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26177)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26177)[0m ------------------------------------------------
[2m[36m(pid=26177)[0m 8.7 K     Trainable params
[2m[36m(pid=26177)[0m 0         Non-trainable params
[2m[36m(pid=26177)[0m 8.7 K     Total params
[2m[36m(pid=31641)[0m Starting run with seed 0 - lr 5 - sec_lr 5 - bs 32
[2m[36m(pid=31641)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=31641)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=31641)[0m GPU available: False, used: False
[2m[36m(pid=31641)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31641)[0m 
[2m[36m(pid=31641)[0m   | Name      | Type              | Params
[2m[36m(pid=31641)[0m ------------------------------------------------
[2m[36m(pid=31641)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31641)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31641)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31641)[0m ------------------------------------------------
[2m[36m(pid=31641)[0m 8.7 K     Trainable params
[2m[36m(pid=31641)[0m 0         Non-trainable params
[2m[36m(pid=31641)[0m 8.7 K     Total params
[2m[36m(pid=31641)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=31641)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31641)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=31641)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31641)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=31641)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=28496)[0m time to fit was 110.93051958084106
[2m[36m(pid=28496)[0m GPU available: False, used: False
[2m[36m(pid=28496)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=28496)[0m 
[2m[36m(pid=28496)[0m   | Name      | Type              | Params
[2m[36m(pid=28496)[0m ------------------------------------------------
[2m[36m(pid=28496)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=28496)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=28496)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=28496)[0m ------------------------------------------------
[2m[36m(pid=28496)[0m 8.7 K     Trainable params
[2m[36m(pid=28496)[0m 0         Non-trainable params
[2m[36m(pid=28496)[0m 8.7 K     Total params
[2m[36m(pid=19841)[0m time to fit was 394.70390939712524
[2m[36m(pid=19841)[0m GPU available: False, used: False
[2m[36m(pid=19841)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=19841)[0m 
[2m[36m(pid=19841)[0m   | Name      | Type              | Params
[2m[36m(pid=19841)[0m ------------------------------------------------
[2m[36m(pid=19841)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=19841)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=19841)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=19841)[0m ------------------------------------------------
[2m[36m(pid=19841)[0m 8.7 K     Trainable params
[2m[36m(pid=19841)[0m 0         Non-trainable params
[2m[36m(pid=19841)[0m 8.7 K     Total params
[2m[36m(pid=31065)[0m time to fit was 55.51717019081116
[2m[36m(pid=31065)[0m GPU available: False, used: False
[2m[36m(pid=31065)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31065)[0m 
[2m[36m(pid=31065)[0m   | Name      | Type              | Params
[2m[36m(pid=31065)[0m ------------------------------------------------
[2m[36m(pid=31065)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31065)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31065)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31065)[0m ------------------------------------------------
[2m[36m(pid=31065)[0m 8.7 K     Trainable params
[2m[36m(pid=31065)[0m 0         Non-trainable params
[2m[36m(pid=31065)[0m 8.7 K     Total params
[2m[36m(pid=26177)[0m time to fit was 59.29170870780945
[2m[36m(pid=26177)[0m GPU available: False, used: False
[2m[36m(pid=26177)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26177)[0m 
[2m[36m(pid=26177)[0m   | Name      | Type              | Params
[2m[36m(pid=26177)[0m ------------------------------------------------
[2m[36m(pid=26177)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26177)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26177)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26177)[0m ------------------------------------------------
[2m[36m(pid=26177)[0m 8.7 K     Trainable params
[2m[36m(pid=26177)[0m 0         Non-trainable params
[2m[36m(pid=26177)[0m 8.7 K     Total params
[2m[36m(pid=24926)[0m time to fit was 159.49943828582764
[2m[36m(pid=25100)[0m time to fit was 90.23907470703125
[2m[36m(pid=24926)[0m GPU available: False, used: False
[2m[36m(pid=24926)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=24926)[0m 
[2m[36m(pid=24926)[0m   | Name      | Type              | Params
[2m[36m(pid=24926)[0m ------------------------------------------------
[2m[36m(pid=24926)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=24926)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=24926)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=24926)[0m ------------------------------------------------
[2m[36m(pid=24926)[0m 8.7 K     Trainable params
[2m[36m(pid=24926)[0m 0         Non-trainable params
[2m[36m(pid=24926)[0m 8.7 K     Total params
[2m[36m(pid=25100)[0m GPU available: False, used: False
[2m[36m(pid=25100)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=25100)[0m 
[2m[36m(pid=25100)[0m   | Name      | Type              | Params
[2m[36m(pid=25100)[0m ------------------------------------------------
[2m[36m(pid=25100)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=25100)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=25100)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=25100)[0m ------------------------------------------------
[2m[36m(pid=25100)[0m 8.7 K     Trainable params
[2m[36m(pid=25100)[0m 0         Non-trainable params
[2m[36m(pid=25100)[0m 8.7 K     Total params
[2m[36m(pid=31065)[0m time to fit was 42.58404994010925
[2m[36m(pid=31065)[0m GPU available: False, used: False
[2m[36m(pid=31065)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31065)[0m 
[2m[36m(pid=31065)[0m   | Name      | Type              | Params
[2m[36m(pid=31065)[0m ------------------------------------------------
[2m[36m(pid=31065)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31065)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31065)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31065)[0m ------------------------------------------------
[2m[36m(pid=31065)[0m 8.7 K     Trainable params
[2m[36m(pid=31065)[0m 0         Non-trainable params
[2m[36m(pid=31065)[0m 8.7 K     Total params
[2m[36m(pid=29072)[0m time to fit was 122.31643414497375
[2m[36m(pid=29072)[0m GPU available: False, used: False
[2m[36m(pid=29072)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29072)[0m 
[2m[36m(pid=29072)[0m   | Name      | Type              | Params
[2m[36m(pid=29072)[0m ------------------------------------------------
[2m[36m(pid=29072)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29072)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29072)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29072)[0m ------------------------------------------------
[2m[36m(pid=29072)[0m 8.7 K     Trainable params
[2m[36m(pid=29072)[0m 0         Non-trainable params
[2m[36m(pid=29072)[0m 8.7 K     Total params
[2m[36m(pid=31016)[0m time to fit was 538.8026216030121
[2m[36m(pid=31016)[0m GPU available: False, used: False
[2m[36m(pid=31016)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31016)[0m 
[2m[36m(pid=31016)[0m   | Name      | Type              | Params
[2m[36m(pid=31016)[0m ------------------------------------------------
[2m[36m(pid=31016)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31016)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31016)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31016)[0m ------------------------------------------------
[2m[36m(pid=31016)[0m 8.7 K     Trainable params
[2m[36m(pid=31016)[0m 0         Non-trainable params
[2m[36m(pid=31016)[0m 8.7 K     Total params
[2m[36m(pid=26177)[0m time to fit was 59.31511640548706
Result for _inner_e8deb_00169:
  auc: 0.9119979023933411
  date: 2021-03-19_14-28-36
  done: false
  experiment_id: a3c4de84a8044d4db226eb998e66fc86
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 26177
  time_since_restore: 294.54349398612976
  time_this_iter_s: 294.54349398612976
  time_total_s: 294.54349398612976
  timestamp: 1616160516
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00169
  
[2m[36m(pid=26177)[0m Finished run with seed 0 - lr 1 - sec_lr 5 - bs 512 - mean val auc: 0.9119979023933411
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 177/180 (1 PENDING, 26 RUNNING, 150 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |       |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |       |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00176 | PENDING    |       |           64 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 157 more trials not shown (16 RUNNING, 140 TERMINATED)


Result for _inner_e8deb_00169:
  auc: 0.9119979023933411
  date: 2021-03-19_14-28-36
  done: true
  experiment_id: a3c4de84a8044d4db226eb998e66fc86
  experiment_tag: 169_batch_size=512,eta=0.0,lr=1,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 26177
  time_since_restore: 294.54349398612976
  time_this_iter_s: 294.54349398612976
  time_total_s: 294.54349398612976
  timestamp: 1616160516
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00169
  
[2m[36m(pid=45882)[0m time to fit was 821.874020576477
[2m[36m(pid=28496)[0m time to fit was 111.95447373390198
[2m[36m(pid=45882)[0m GPU available: False, used: False
[2m[36m(pid=45882)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=45882)[0m 
[2m[36m(pid=45882)[0m   | Name      | Type              | Params
[2m[36m(pid=45882)[0m ------------------------------------------------
[2m[36m(pid=45882)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=45882)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=45882)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=45882)[0m ------------------------------------------------
[2m[36m(pid=45882)[0m 8.7 K     Trainable params
[2m[36m(pid=45882)[0m 0         Non-trainable params
[2m[36m(pid=45882)[0m 8.7 K     Total params
[2m[36m(pid=28496)[0m GPU available: False, used: False
[2m[36m(pid=28496)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=28496)[0m 
[2m[36m(pid=28496)[0m   | Name      | Type              | Params
[2m[36m(pid=28496)[0m ------------------------------------------------
[2m[36m(pid=28496)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=28496)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=28496)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=28496)[0m ------------------------------------------------
[2m[36m(pid=28496)[0m 8.7 K     Trainable params
[2m[36m(pid=28496)[0m 0         Non-trainable params
[2m[36m(pid=28496)[0m 8.7 K     Total params
[2m[36m(pid=35872)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=35872)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=35872)[0m Starting run with seed 0 - lr 5 - sec_lr 5 - bs 64
[2m[36m(pid=35872)[0m GPU available: False, used: False
[2m[36m(pid=35872)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35872)[0m 
[2m[36m(pid=35872)[0m   | Name      | Type              | Params
[2m[36m(pid=35872)[0m ------------------------------------------------
[2m[36m(pid=35872)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35872)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35872)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35872)[0m ------------------------------------------------
[2m[36m(pid=35872)[0m 8.7 K     Trainable params
[2m[36m(pid=35872)[0m 0         Non-trainable params
[2m[36m(pid=35872)[0m 8.7 K     Total params
[2m[36m(pid=35872)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35872)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35872)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=35872)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=35872)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=35872)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=25100)[0m time to fit was 71.14408302307129
[2m[36m(pid=25100)[0m GPU available: False, used: False
[2m[36m(pid=25100)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=25100)[0m 
[2m[36m(pid=25100)[0m   | Name      | Type              | Params
[2m[36m(pid=25100)[0m ------------------------------------------------
[2m[36m(pid=25100)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=25100)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=25100)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=25100)[0m ------------------------------------------------
[2m[36m(pid=25100)[0m 8.7 K     Trainable params
[2m[36m(pid=25100)[0m 0         Non-trainable params
[2m[36m(pid=25100)[0m 8.7 K     Total params
[2m[36m(pid=22604)[0m time to fit was 505.34398126602173
[2m[36m(pid=22604)[0m GPU available: False, used: False
[2m[36m(pid=22604)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=22604)[0m 
[2m[36m(pid=22604)[0m   | Name      | Type              | Params
[2m[36m(pid=22604)[0m ------------------------------------------------
[2m[36m(pid=22604)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=22604)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=22604)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=22604)[0m ------------------------------------------------
[2m[36m(pid=22604)[0m 8.7 K     Trainable params
[2m[36m(pid=22604)[0m 0         Non-trainable params
[2m[36m(pid=22604)[0m 8.7 K     Total params
[2m[36m(pid=31065)[0m time to fit was 83.3896951675415
[2m[36m(pid=31065)[0m GPU available: False, used: False
[2m[36m(pid=31065)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31065)[0m 
[2m[36m(pid=31065)[0m   | Name      | Type              | Params
[2m[36m(pid=31065)[0m ------------------------------------------------
[2m[36m(pid=31065)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31065)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31065)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31065)[0m ------------------------------------------------
[2m[36m(pid=31065)[0m 8.7 K     Trainable params
[2m[36m(pid=31065)[0m 0         Non-trainable params
[2m[36m(pid=31065)[0m 8.7 K     Total params
[2m[36m(pid=7482)[0m time to fit was 206.94404768943787
[2m[36m(pid=7482)[0m GPU available: False, used: False
[2m[36m(pid=7482)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=7482)[0m 
[2m[36m(pid=7482)[0m   | Name      | Type              | Params
[2m[36m(pid=7482)[0m ------------------------------------------------
[2m[36m(pid=7482)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=7482)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=7482)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=7482)[0m ------------------------------------------------
[2m[36m(pid=7482)[0m 8.7 K     Trainable params
[2m[36m(pid=7482)[0m 0         Non-trainable params
[2m[36m(pid=7482)[0m 8.7 K     Total params
[2m[36m(pid=29072)[0m time to fit was 71.39319801330566
[2m[36m(pid=29072)[0m GPU available: False, used: False
[2m[36m(pid=29072)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29072)[0m 
[2m[36m(pid=29072)[0m   | Name      | Type              | Params
[2m[36m(pid=29072)[0m ------------------------------------------------
[2m[36m(pid=29072)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29072)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29072)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29072)[0m ------------------------------------------------
[2m[36m(pid=29072)[0m 8.7 K     Trainable params
[2m[36m(pid=29072)[0m 0         Non-trainable params
[2m[36m(pid=29072)[0m 8.7 K     Total params
[2m[36m(pid=29801)[0m time to fit was 1772.0999641418457
[2m[36m(pid=29801)[0m GPU available: False, used: False
[2m[36m(pid=29801)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29801)[0m 
[2m[36m(pid=29801)[0m   | Name      | Type              | Params
[2m[36m(pid=29801)[0m ------------------------------------------------
[2m[36m(pid=29801)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29801)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29801)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29801)[0m ------------------------------------------------
[2m[36m(pid=29801)[0m 8.7 K     Trainable params
[2m[36m(pid=29801)[0m 0         Non-trainable params
[2m[36m(pid=29801)[0m 8.7 K     Total params
[2m[36m(pid=25100)[0m time to fit was 72.5724036693573
Result for _inner_e8deb_00168:
  auc: 0.910231602191925
  date: 2021-03-19_14-30-09
  done: false
  experiment_id: 9ce0e849bca44dc28fb47a846a2303e1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 25100
  time_since_restore: 421.63740587234497
  time_this_iter_s: 421.63740587234497
  time_total_s: 421.63740587234497
  timestamp: 1616160609
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00168
  
[2m[36m(pid=25100)[0m Finished run with seed 0 - lr 1 - sec_lr 5 - bs 256 - mean val auc: 0.910231602191925
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 178/180 (1 PENDING, 26 RUNNING, 151 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    |       |           32 |     0 | 0.01  |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |       |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |       |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00177 | PENDING    |       |          128 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 158 more trials not shown (16 RUNNING, 141 TERMINATED)


Result for _inner_e8deb_00168:
  auc: 0.910231602191925
  date: 2021-03-19_14-30-09
  done: true
  experiment_id: 9ce0e849bca44dc28fb47a846a2303e1
  experiment_tag: 168_batch_size=256,eta=0.0,lr=1,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 25100
  time_since_restore: 421.63740587234497
  time_this_iter_s: 421.63740587234497
  time_total_s: 421.63740587234497
  timestamp: 1616160609
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00168
  
[2m[36m(pid=27334)[0m time to fit was 356.07083892822266
[2m[36m(pid=27334)[0m GPU available: False, used: False
[2m[36m(pid=27334)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27334)[0m 
[2m[36m(pid=27334)[0m   | Name      | Type              | Params
[2m[36m(pid=27334)[0m ------------------------------------------------
[2m[36m(pid=27334)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27334)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27334)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27334)[0m ------------------------------------------------
[2m[36m(pid=27334)[0m 8.7 K     Trainable params
[2m[36m(pid=27334)[0m 0         Non-trainable params
[2m[36m(pid=27334)[0m 8.7 K     Total params
[2m[36m(pid=24926)[0m time to fit was 150.22573685646057
[2m[36m(pid=24926)[0m GPU available: False, used: False
[2m[36m(pid=24926)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=24926)[0m 
[2m[36m(pid=24926)[0m   | Name      | Type              | Params
[2m[36m(pid=24926)[0m ------------------------------------------------
[2m[36m(pid=24926)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=24926)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=24926)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=24926)[0m ------------------------------------------------
[2m[36m(pid=24926)[0m 8.7 K     Trainable params
[2m[36m(pid=24926)[0m 0         Non-trainable params
[2m[36m(pid=24926)[0m 8.7 K     Total params
[2m[36m(pid=38477)[0m Starting run with seed 0 - lr 5 - sec_lr 5 - bs 128
[2m[36m(pid=38477)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38477)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=38477)[0m GPU available: False, used: False
[2m[36m(pid=38477)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=38477)[0m 
[2m[36m(pid=38477)[0m   | Name      | Type              | Params
[2m[36m(pid=38477)[0m ------------------------------------------------
[2m[36m(pid=38477)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=38477)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=38477)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=38477)[0m ------------------------------------------------
[2m[36m(pid=38477)[0m 8.7 K     Trainable params
[2m[36m(pid=38477)[0m 0         Non-trainable params
[2m[36m(pid=38477)[0m 8.7 K     Total params
[2m[36m(pid=38477)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=38477)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=38477)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=38477)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=38477)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=38477)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=47886)[0m time to fit was 641.3062946796417
Result for _inner_e8deb_00125:
  auc: 0.9094825983047485
  date: 2021-03-19_14-30-49
  done: false
  experiment_id: 1c78aa09c6d64a27a75e12e17b320d53
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 47886
  time_since_restore: 3106.1266901493073
  time_this_iter_s: 3106.1266901493073
  time_total_s: 3106.1266901493073
  timestamp: 1616160649
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00125
  
[2m[36m(pid=47886)[0m Finished run with seed 0 - lr 0.01 - sec_lr 2 - bs 32 - mean val auc: 0.9094825983047485
== Status ==
Memory usage on this node: 9.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 179/180 (1 PENDING, 26 RUNNING, 152 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |                     |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |                     |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00125 | RUNNING    | 145.101.32.82:47886 |           32 |     0 | 0.01  |    2     |      1 |          3106.13 | 0.909483 |
| _inner_e8deb_00140 | RUNNING    |                     |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                     |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |                     |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |                     |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |                     |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00178 | PENDING    |                     |          256 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 159 more trials not shown (16 RUNNING, 142 TERMINATED)


Result for _inner_e8deb_00125:
  auc: 0.9094825983047485
  date: 2021-03-19_14-30-49
  done: true
  experiment_id: 1c78aa09c6d64a27a75e12e17b320d53
  experiment_tag: 125_batch_size=32,eta=0.0,lr=0.01,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 47886
  time_since_restore: 3106.1266901493073
  time_this_iter_s: 3106.1266901493073
  time_total_s: 3106.1266901493073
  timestamp: 1616160649
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00125
  
[2m[36m(pid=49344)[0m time to fit was 652.8527643680573
[2m[36m(pid=49344)[0m GPU available: False, used: False
[2m[36m(pid=49344)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49344)[0m 
[2m[36m(pid=49344)[0m   | Name      | Type              | Params
[2m[36m(pid=49344)[0m ------------------------------------------------
[2m[36m(pid=49344)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49344)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49344)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49344)[0m ------------------------------------------------
[2m[36m(pid=49344)[0m 8.7 K     Trainable params
[2m[36m(pid=49344)[0m 0         Non-trainable params
[2m[36m(pid=49344)[0m 8.7 K     Total params
[2m[36m(pid=39533)[0m Starting run with seed 0 - lr 5 - sec_lr 5 - bs 256
[2m[36m(pid=39533)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=39533)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=39533)[0m GPU available: False, used: False
[2m[36m(pid=39533)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=39533)[0m 
[2m[36m(pid=39533)[0m   | Name      | Type              | Params
[2m[36m(pid=39533)[0m ------------------------------------------------
[2m[36m(pid=39533)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=39533)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=39533)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=39533)[0m ------------------------------------------------
[2m[36m(pid=39533)[0m 8.7 K     Trainable params
[2m[36m(pid=39533)[0m 0         Non-trainable params
[2m[36m(pid=39533)[0m 8.7 K     Total params
[2m[36m(pid=39533)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=39533)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=39533)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=39533)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=39533)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=39533)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31019)[0m time to fit was 1759.7848126888275
[2m[36m(pid=31019)[0m GPU available: False, used: False
[2m[36m(pid=31019)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31019)[0m 
[2m[36m(pid=31019)[0m   | Name      | Type              | Params
[2m[36m(pid=31019)[0m ------------------------------------------------
[2m[36m(pid=31019)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31019)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31019)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31019)[0m ------------------------------------------------
[2m[36m(pid=31019)[0m 8.7 K     Trainable params
[2m[36m(pid=31019)[0m 0         Non-trainable params
[2m[36m(pid=31019)[0m 8.7 K     Total params
[2m[36m(pid=23176)[0m time to fit was 335.7707169055939
[2m[36m(pid=23176)[0m GPU available: False, used: False
[2m[36m(pid=23176)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23176)[0m 
[2m[36m(pid=23176)[0m   | Name      | Type              | Params
[2m[36m(pid=23176)[0m ------------------------------------------------
[2m[36m(pid=23176)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23176)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23176)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23176)[0m ------------------------------------------------
[2m[36m(pid=23176)[0m 8.7 K     Trainable params
[2m[36m(pid=23176)[0m 0         Non-trainable params
[2m[36m(pid=23176)[0m 8.7 K     Total params
[2m[36m(pid=28496)[0m time to fit was 174.96101546287537
[2m[36m(pid=28496)[0m GPU available: False, used: False
[2m[36m(pid=28496)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=28496)[0m 
[2m[36m(pid=28496)[0m   | Name      | Type              | Params
[2m[36m(pid=28496)[0m ------------------------------------------------
[2m[36m(pid=28496)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=28496)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=28496)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=28496)[0m ------------------------------------------------
[2m[36m(pid=28496)[0m 8.7 K     Trainable params
[2m[36m(pid=28496)[0m 0         Non-trainable params
[2m[36m(pid=28496)[0m 8.7 K     Total params
[2m[36m(pid=29072)[0m time to fit was 121.11387205123901
[2m[36m(pid=29072)[0m GPU available: False, used: False
[2m[36m(pid=29072)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29072)[0m 
[2m[36m(pid=29072)[0m   | Name      | Type              | Params
[2m[36m(pid=29072)[0m ------------------------------------------------
[2m[36m(pid=29072)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29072)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29072)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29072)[0m ------------------------------------------------
[2m[36m(pid=29072)[0m 8.7 K     Trainable params
[2m[36m(pid=29072)[0m 0         Non-trainable params
[2m[36m(pid=29072)[0m 8.7 K     Total params
[2m[36m(pid=31065)[0m time to fit was 148.21287202835083
[2m[36m(pid=31065)[0m GPU available: False, used: False
[2m[36m(pid=31065)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31065)[0m 
[2m[36m(pid=31065)[0m   | Name      | Type              | Params
[2m[36m(pid=31065)[0m ------------------------------------------------
[2m[36m(pid=31065)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31065)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31065)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31065)[0m ------------------------------------------------
[2m[36m(pid=31065)[0m 8.7 K     Trainable params
[2m[36m(pid=31065)[0m 0         Non-trainable params
[2m[36m(pid=31065)[0m 8.7 K     Total params
[2m[36m(pid=29932)[0m time to fit was 952.1932942867279
[2m[36m(pid=29932)[0m GPU available: False, used: False
[2m[36m(pid=29932)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29932)[0m 
[2m[36m(pid=29932)[0m   | Name      | Type              | Params
[2m[36m(pid=29932)[0m ------------------------------------------------
[2m[36m(pid=29932)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29932)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29932)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29932)[0m ------------------------------------------------
[2m[36m(pid=29932)[0m 8.7 K     Trainable params
[2m[36m(pid=29932)[0m 0         Non-trainable params
[2m[36m(pid=29932)[0m 8.7 K     Total params
[2m[36m(pid=39533)[0m time to fit was 71.03841209411621
[2m[36m(pid=39533)[0m GPU available: False, used: False
[2m[36m(pid=39533)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=39533)[0m 
[2m[36m(pid=39533)[0m   | Name      | Type              | Params
[2m[36m(pid=39533)[0m ------------------------------------------------
[2m[36m(pid=39533)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=39533)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=39533)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=39533)[0m ------------------------------------------------
[2m[36m(pid=39533)[0m 8.7 K     Trainable params
[2m[36m(pid=39533)[0m 0         Non-trainable params
[2m[36m(pid=39533)[0m 8.7 K     Total params
[2m[36m(pid=38477)[0m time to fit was 112.63543772697449
[2m[36m(pid=38477)[0m GPU available: False, used: False
[2m[36m(pid=38477)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=38477)[0m 
[2m[36m(pid=38477)[0m   | Name      | Type              | Params
[2m[36m(pid=38477)[0m ------------------------------------------------
[2m[36m(pid=38477)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=38477)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=38477)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=38477)[0m ------------------------------------------------
[2m[36m(pid=38477)[0m 8.7 K     Trainable params
[2m[36m(pid=38477)[0m 0         Non-trainable params
[2m[36m(pid=38477)[0m 8.7 K     Total params
[2m[36m(pid=24926)[0m time to fit was 120.4128303527832
[2m[36m(pid=24926)[0m GPU available: False, used: False
[2m[36m(pid=24926)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=24926)[0m 
[2m[36m(pid=24926)[0m   | Name      | Type              | Params
[2m[36m(pid=24926)[0m ------------------------------------------------
[2m[36m(pid=24926)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=24926)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=24926)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=24926)[0m ------------------------------------------------
[2m[36m(pid=24926)[0m 8.7 K     Trainable params
[2m[36m(pid=24926)[0m 0         Non-trainable params
[2m[36m(pid=24926)[0m 8.7 K     Total params
[2m[36m(pid=31065)[0m time to fit was 43.680129528045654
Result for _inner_e8deb_00174:
  auc: 0.7806110739707947
  date: 2021-03-19_14-32-36
  done: false
  experiment_id: de399585166d4755b833b2aaed73a1a7
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 31065
  time_since_restore: 374.5842213630676
  time_this_iter_s: 374.5842213630676
  time_total_s: 374.5842213630676
  timestamp: 1616160756
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00174
  
[2m[36m(pid=31065)[0m Finished run with seed 0 - lr 2 - sec_lr 5 - bs 512 - mean val auc: 0.7806110739707947
== Status ==
Memory usage on this node: 9.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (1 PENDING, 26 RUNNING, 153 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |       |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |       |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00152 | RUNNING    |       |          128 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00179 | PENDING    |       |          512 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (16 RUNNING, 143 TERMINATED)


Result for _inner_e8deb_00174:
  auc: 0.7806110739707947
  date: 2021-03-19_14-32-36
  done: true
  experiment_id: de399585166d4755b833b2aaed73a1a7
  experiment_tag: 174_batch_size=512,eta=0.0,lr=2,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 31065
  time_since_restore: 374.5842213630676
  time_this_iter_s: 374.5842213630676
  time_total_s: 374.5842213630676
  timestamp: 1616160756
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00174
  
[2m[36m(pid=42722)[0m Starting run with seed 0 - lr 5 - sec_lr 5 - bs 512
[2m[36m(pid=42722)[0m /home/nolte/fact-ai/datasets.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=42722)[0m   return self.dataset.memberships[self.indices]
[2m[36m(pid=42722)[0m GPU available: False, used: False
[2m[36m(pid=42722)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=42722)[0m 
[2m[36m(pid=42722)[0m   | Name      | Type              | Params
[2m[36m(pid=42722)[0m ------------------------------------------------
[2m[36m(pid=42722)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=42722)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=42722)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=42722)[0m ------------------------------------------------
[2m[36m(pid=42722)[0m 8.7 K     Trainable params
[2m[36m(pid=42722)[0m 0         Non-trainable params
[2m[36m(pid=42722)[0m 8.7 K     Total params
[2m[36m(pid=42722)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=42722)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=42722)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
[2m[36m(pid=42722)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=42722)[0m /home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
[2m[36m(pid=42722)[0m   warnings.warn(*args, **kwargs)
[2m[36m(pid=31006)[0m time to fit was 933.0410203933716
[2m[36m(pid=31006)[0m GPU available: False, used: False
[2m[36m(pid=31006)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31006)[0m 
[2m[36m(pid=31006)[0m   | Name      | Type              | Params
[2m[36m(pid=31006)[0m ------------------------------------------------
[2m[36m(pid=31006)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31006)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31006)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31006)[0m ------------------------------------------------
[2m[36m(pid=31006)[0m 8.7 K     Trainable params
[2m[36m(pid=31006)[0m 0         Non-trainable params
[2m[36m(pid=31006)[0m 8.7 K     Total params
[2m[36m(pid=29072)[0m time to fit was 91.03140902519226
[2m[36m(pid=29072)[0m Finished run with seed 0 - lr 2 - sec_lr 5 - bs 256 - mean val auc: 0.8258102297782898
Result for _inner_e8deb_00173:
  auc: 0.8258102297782898
  date: 2021-03-19_14-33-06
  done: false
  experiment_id: 2285da2587894361badc991c25d86a28
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 29072
  time_since_restore: 472.0156214237213
  time_this_iter_s: 472.0156214237213
  time_total_s: 472.0156214237213
  timestamp: 1616160786
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00173
  
== Status ==
Memory usage on this node: 9.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 52/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (26 RUNNING, 154 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |       |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |       |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00152 | RUNNING    |       |          128 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (16 RUNNING, 144 TERMINATED)


Result for _inner_e8deb_00173:
  auc: 0.8258102297782898
  date: 2021-03-19_14-33-06
  done: true
  experiment_id: 2285da2587894361badc991c25d86a28
  experiment_tag: 173_batch_size=256,eta=0.0,lr=2,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 29072
  time_since_restore: 472.0156214237213
  time_this_iter_s: 472.0156214237213
  time_total_s: 472.0156214237213
  timestamp: 1616160786
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00173
  
[2m[36m(pid=35872)[0m time to fit was 259.10798811912537
[2m[36m(pid=35872)[0m GPU available: False, used: False
[2m[36m(pid=35872)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35872)[0m 
[2m[36m(pid=35872)[0m   | Name      | Type              | Params
[2m[36m(pid=35872)[0m ------------------------------------------------
[2m[36m(pid=35872)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35872)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35872)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35872)[0m ------------------------------------------------
[2m[36m(pid=35872)[0m 8.7 K     Trainable params
[2m[36m(pid=35872)[0m 0         Non-trainable params
[2m[36m(pid=35872)[0m 8.7 K     Total params
[2m[36m(pid=39533)[0m time to fit was 71.0219144821167
[2m[36m(pid=39533)[0m GPU available: False, used: False
[2m[36m(pid=39533)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=39533)[0m 
[2m[36m(pid=39533)[0m   | Name      | Type              | Params
[2m[36m(pid=39533)[0m ------------------------------------------------
[2m[36m(pid=39533)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=39533)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=39533)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=39533)[0m ------------------------------------------------
[2m[36m(pid=39533)[0m 8.7 K     Trainable params
[2m[36m(pid=39533)[0m 0         Non-trainable params
[2m[36m(pid=39533)[0m 8.7 K     Total params
[2m[36m(pid=42722)[0m time to fit was 56.89613318443298
[2m[36m(pid=42722)[0m GPU available: False, used: False
[2m[36m(pid=42722)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=42722)[0m 
[2m[36m(pid=42722)[0m   | Name      | Type              | Params
[2m[36m(pid=42722)[0m ------------------------------------------------
[2m[36m(pid=42722)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=42722)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=42722)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=42722)[0m ------------------------------------------------
[2m[36m(pid=42722)[0m 8.7 K     Trainable params
[2m[36m(pid=42722)[0m 0         Non-trainable params
[2m[36m(pid=42722)[0m 8.7 K     Total params
[2m[36m(pid=28496)[0m time to fit was 134.19037914276123
[2m[36m(pid=28496)[0m GPU available: False, used: False
[2m[36m(pid=28496)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=28496)[0m 
[2m[36m(pid=28496)[0m   | Name      | Type              | Params
[2m[36m(pid=28496)[0m ------------------------------------------------
[2m[36m(pid=28496)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=28496)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=28496)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=28496)[0m ------------------------------------------------
[2m[36m(pid=28496)[0m 8.7 K     Trainable params
[2m[36m(pid=28496)[0m 0         Non-trainable params
[2m[36m(pid=28496)[0m 8.7 K     Total params
[2m[36m(pid=7482)[0m time to fit was 261.3371114730835
Result for _inner_e8deb_00161:
  auc: 0.9091181993484497
  date: 2021-03-19_14-33-49
  done: false
  experiment_id: 28e908965edf4dc2bc2c302a26b03067
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 7482
  time_since_restore: 1218.356733083725
  time_this_iter_s: 1218.356733083725
  time_total_s: 1218.356733083725
  timestamp: 1616160829
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00161
  
[2m[36m(pid=7482)[0m Finished run with seed 0 - lr 0.1 - sec_lr 5 - bs 64 - mean val auc: 0.9091181993484497
== Status ==
Memory usage on this node: 9.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 50/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (25 RUNNING, 155 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |       |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |       |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00152 | RUNNING    |       |          128 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (15 RUNNING, 145 TERMINATED)


Result for _inner_e8deb_00161:
  auc: 0.9091181993484497
  date: 2021-03-19_14-33-49
  done: true
  experiment_id: 28e908965edf4dc2bc2c302a26b03067
  experiment_tag: 161_batch_size=64,eta=0.0,lr=0.1,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 7482
  time_since_restore: 1218.356733083725
  time_this_iter_s: 1218.356733083725
  time_total_s: 1218.356733083725
  timestamp: 1616160829
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00161
  
[2m[36m(pid=3499)[0m time to fit was 494.28209805488586
[2m[36m(pid=3499)[0m GPU available: False, used: False
[2m[36m(pid=3499)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=3499)[0m 
[2m[36m(pid=3499)[0m   | Name      | Type              | Params
[2m[36m(pid=3499)[0m ------------------------------------------------
[2m[36m(pid=3499)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=3499)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=3499)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=3499)[0m ------------------------------------------------
[2m[36m(pid=3499)[0m 8.7 K     Trainable params
[2m[36m(pid=3499)[0m 0         Non-trainable params
[2m[36m(pid=3499)[0m 8.7 K     Total params
[2m[36m(pid=24926)[0m time to fit was 137.73910999298096
Result for _inner_e8deb_00167:
  auc: 0.9093719244003295
  date: 2021-03-19_14-34-34
  done: false
  experiment_id: 19c0a7d1100a4d13be237a20f96e14c9
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 24926
  time_since_restore: 693.4653704166412
  time_this_iter_s: 693.4653704166412
  time_total_s: 693.4653704166412
  timestamp: 1616160874
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00167
  
[2m[36m(pid=24926)[0m Finished run with seed 0 - lr 1 - sec_lr 5 - bs 128 - mean val auc: 0.9093719244003295
== Status ==
Memory usage on this node: 9.0/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 48/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (24 RUNNING, 156 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |       |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |       |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00152 | RUNNING    |       |          128 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (14 RUNNING, 146 TERMINATED)


Result for _inner_e8deb_00167:
  auc: 0.9093719244003295
  date: 2021-03-19_14-34-34
  done: true
  experiment_id: 19c0a7d1100a4d13be237a20f96e14c9
  experiment_tag: 167_batch_size=128,eta=0.0,lr=1,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 24926
  time_since_restore: 693.4653704166412
  time_this_iter_s: 693.4653704166412
  time_total_s: 693.4653704166412
  timestamp: 1616160874
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00167
  
[2m[36m(pid=39533)[0m time to fit was 86.66598081588745
[2m[36m(pid=39533)[0m GPU available: False, used: False
[2m[36m(pid=39533)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=39533)[0m 
[2m[36m(pid=39533)[0m   | Name      | Type              | Params
[2m[36m(pid=39533)[0m ------------------------------------------------
[2m[36m(pid=39533)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=39533)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=39533)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=39533)[0m ------------------------------------------------
[2m[36m(pid=39533)[0m 8.7 K     Trainable params
[2m[36m(pid=39533)[0m 0         Non-trainable params
[2m[36m(pid=39533)[0m 8.7 K     Total params
[2m[36m(pid=42722)[0m time to fit was 84.98461294174194
[2m[36m(pid=42722)[0m GPU available: False, used: False
[2m[36m(pid=42722)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=42722)[0m 
[2m[36m(pid=42722)[0m   | Name      | Type              | Params
[2m[36m(pid=42722)[0m ------------------------------------------------
[2m[36m(pid=42722)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=42722)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=42722)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=42722)[0m ------------------------------------------------
[2m[36m(pid=42722)[0m 8.7 K     Trainable params
[2m[36m(pid=42722)[0m 0         Non-trainable params
[2m[36m(pid=42722)[0m 8.7 K     Total params
[2m[36m(pid=29932)[0m time to fit was 199.72822213172913
[2m[36m(pid=29932)[0m GPU available: False, used: False
[2m[36m(pid=29932)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29932)[0m 
[2m[36m(pid=29932)[0m   | Name      | Type              | Params
[2m[36m(pid=29932)[0m ------------------------------------------------
[2m[36m(pid=29932)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29932)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29932)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29932)[0m ------------------------------------------------
[2m[36m(pid=29932)[0m 8.7 K     Trainable params
[2m[36m(pid=29932)[0m 0         Non-trainable params
[2m[36m(pid=29932)[0m 8.7 K     Total params
[2m[36m(pid=28496)[0m time to fit was 116.63463091850281
[2m[36m(pid=28496)[0m Finished run with seed 0 - lr 2 - sec_lr 5 - bs 128 - mean val auc: 0.8274362802505493
Result for _inner_e8deb_00172:
  auc: 0.8274362802505493
  date: 2021-03-19_14-35-45
  done: false
  experiment_id: 83e47c7d3ce8456285d2b65ef44dd4b2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 28496
  time_since_restore: 649.8514671325684
  time_this_iter_s: 649.8514671325684
  time_total_s: 649.8514671325684
  timestamp: 1616160945
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00172
  
== Status ==
Memory usage on this node: 8.8/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 46/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (23 RUNNING, 157 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |       |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |       |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00152 | RUNNING    |       |          128 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (13 RUNNING, 147 TERMINATED)


Result for _inner_e8deb_00172:
  auc: 0.8274362802505493
  date: 2021-03-19_14-35-45
  done: true
  experiment_id: 83e47c7d3ce8456285d2b65ef44dd4b2
  experiment_tag: 172_batch_size=128,eta=0.0,lr=2,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 28496
  time_since_restore: 649.8514671325684
  time_this_iter_s: 649.8514671325684
  time_total_s: 649.8514671325684
  timestamp: 1616160945
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00172
  
[2m[36m(pid=49344)[0m time to fit was 294.2040934562683
[2m[36m(pid=49344)[0m GPU available: False, used: False
[2m[36m(pid=49344)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=49344)[0m 
[2m[36m(pid=49344)[0m   | Name      | Type              | Params
[2m[36m(pid=49344)[0m ------------------------------------------------
[2m[36m(pid=49344)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=49344)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=49344)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=49344)[0m ------------------------------------------------
[2m[36m(pid=49344)[0m 8.7 K     Trainable params
[2m[36m(pid=49344)[0m 0         Non-trainable params
[2m[36m(pid=49344)[0m 8.7 K     Total params
[2m[36m(pid=42722)[0m time to fit was 38.691426038742065
[2m[36m(pid=42722)[0m GPU available: False, used: False
[2m[36m(pid=42722)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=42722)[0m 
[2m[36m(pid=42722)[0m   | Name      | Type              | Params
[2m[36m(pid=42722)[0m ------------------------------------------------
[2m[36m(pid=42722)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=42722)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=42722)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=42722)[0m ------------------------------------------------
[2m[36m(pid=42722)[0m 8.7 K     Trainable params
[2m[36m(pid=42722)[0m 0         Non-trainable params
[2m[36m(pid=42722)[0m 8.7 K     Total params
[2m[36m(pid=39533)[0m time to fit was 61.637848138809204
[2m[36m(pid=39533)[0m GPU available: False, used: False
[2m[36m(pid=39533)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=39533)[0m 
[2m[36m(pid=39533)[0m   | Name      | Type              | Params
[2m[36m(pid=39533)[0m ------------------------------------------------
[2m[36m(pid=39533)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=39533)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=39533)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=39533)[0m ------------------------------------------------
[2m[36m(pid=39533)[0m 8.7 K     Trainable params
[2m[36m(pid=39533)[0m 0         Non-trainable params
[2m[36m(pid=39533)[0m 8.7 K     Total params
[2m[36m(pid=26552)[0m time to fit was 729.3047108650208
[2m[36m(pid=26552)[0m GPU available: False, used: False
[2m[36m(pid=26552)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26552)[0m 
[2m[36m(pid=26552)[0m   | Name      | Type              | Params
[2m[36m(pid=26552)[0m ------------------------------------------------
[2m[36m(pid=26552)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26552)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26552)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26552)[0m ------------------------------------------------
[2m[36m(pid=26552)[0m 8.7 K     Trainable params
[2m[36m(pid=26552)[0m 0         Non-trainable params
[2m[36m(pid=26552)[0m 8.7 K     Total params
[2m[36m(pid=35872)[0m time to fit was 191.52691340446472
[2m[36m(pid=35872)[0m GPU available: False, used: False
[2m[36m(pid=35872)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35872)[0m 
[2m[36m(pid=35872)[0m   | Name      | Type              | Params
[2m[36m(pid=35872)[0m ------------------------------------------------
[2m[36m(pid=35872)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35872)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35872)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35872)[0m ------------------------------------------------
[2m[36m(pid=35872)[0m 8.7 K     Trainable params
[2m[36m(pid=35872)[0m 0         Non-trainable params
[2m[36m(pid=35872)[0m 8.7 K     Total params
[2m[36m(pid=23176)[0m time to fit was 305.22866582870483
[2m[36m(pid=23176)[0m GPU available: False, used: False
[2m[36m(pid=23176)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23176)[0m 
[2m[36m(pid=23176)[0m   | Name      | Type              | Params
[2m[36m(pid=23176)[0m ------------------------------------------------
[2m[36m(pid=23176)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23176)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23176)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23176)[0m ------------------------------------------------
[2m[36m(pid=23176)[0m 8.7 K     Trainable params
[2m[36m(pid=23176)[0m 0         Non-trainable params
[2m[36m(pid=23176)[0m 8.7 K     Total params
[2m[36m(pid=42722)[0m time to fit was 50.570247411727905
[2m[36m(pid=42722)[0m GPU available: False, used: False
[2m[36m(pid=42722)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=42722)[0m 
[2m[36m(pid=42722)[0m   | Name      | Type              | Params
[2m[36m(pid=42722)[0m ------------------------------------------------
[2m[36m(pid=42722)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=42722)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=42722)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=42722)[0m ------------------------------------------------
[2m[36m(pid=42722)[0m 8.7 K     Trainable params
[2m[36m(pid=42722)[0m 0         Non-trainable params
[2m[36m(pid=42722)[0m 8.7 K     Total params
[2m[36m(pid=19841)[0m time to fit was 586.1790118217468
[2m[36m(pid=19841)[0m GPU available: False, used: False
[2m[36m(pid=19841)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=19841)[0m 
[2m[36m(pid=19841)[0m   | Name      | Type              | Params
[2m[36m(pid=19841)[0m ------------------------------------------------
[2m[36m(pid=19841)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=19841)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=19841)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=19841)[0m ------------------------------------------------
[2m[36m(pid=19841)[0m 8.7 K     Trainable params
[2m[36m(pid=19841)[0m 0         Non-trainable params
[2m[36m(pid=19841)[0m 8.7 K     Total params
[2m[36m(pid=39533)[0m time to fit was 57.8091983795166
Result for _inner_e8deb_00178:
  auc: 0.5855594038963318
  date: 2021-03-19_14-36-49
  done: false
  experiment_id: f530c8a2a4f8417081963d99f092703a
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 39533
  time_since_restore: 349.3828122615814
  time_this_iter_s: 349.3828122615814
  time_total_s: 349.3828122615814
  timestamp: 1616161009
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00178
  
[2m[36m(pid=39533)[0m Finished run with seed 0 - lr 5 - sec_lr 5 - bs 256 - mean val auc: 0.5855594038963318
== Status ==
Memory usage on this node: 8.8/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 44/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (22 RUNNING, 158 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |       |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |       |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00152 | RUNNING    |       |          128 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (12 RUNNING, 148 TERMINATED)


Result for _inner_e8deb_00178:
  auc: 0.5855594038963318
  date: 2021-03-19_14-36-49
  done: true
  experiment_id: f530c8a2a4f8417081963d99f092703a
  experiment_tag: 178_batch_size=256,eta=0.0,lr=5,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 39533
  time_since_restore: 349.3828122615814
  time_this_iter_s: 349.3828122615814
  time_total_s: 349.3828122615814
  timestamp: 1616161009
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00178
  
[2m[36m(pid=27334)[0m time to fit was 410.26399874687195
[2m[36m(pid=27334)[0m GPU available: False, used: False
[2m[36m(pid=27334)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27334)[0m 
[2m[36m(pid=27334)[0m   | Name      | Type              | Params
[2m[36m(pid=27334)[0m ------------------------------------------------
[2m[36m(pid=27334)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27334)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27334)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27334)[0m ------------------------------------------------
[2m[36m(pid=27334)[0m 8.7 K     Trainable params
[2m[36m(pid=27334)[0m 0         Non-trainable params
[2m[36m(pid=27334)[0m 8.7 K     Total params
[2m[36m(pid=31016)[0m time to fit was 509.5984511375427
[2m[36m(pid=31016)[0m GPU available: False, used: False
[2m[36m(pid=31016)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31016)[0m 
[2m[36m(pid=31016)[0m   | Name      | Type              | Params
[2m[36m(pid=31016)[0m ------------------------------------------------
[2m[36m(pid=31016)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31016)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31016)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31016)[0m ------------------------------------------------
[2m[36m(pid=31016)[0m 8.7 K     Trainable params
[2m[36m(pid=31016)[0m 0         Non-trainable params
[2m[36m(pid=31016)[0m 8.7 K     Total params
[2m[36m(pid=38477)[0m time to fit was 291.5977580547333
[2m[36m(pid=38477)[0m GPU available: False, used: False
[2m[36m(pid=38477)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=38477)[0m 
[2m[36m(pid=38477)[0m   | Name      | Type              | Params
[2m[36m(pid=38477)[0m ------------------------------------------------
[2m[36m(pid=38477)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=38477)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=38477)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=38477)[0m ------------------------------------------------
[2m[36m(pid=38477)[0m 8.7 K     Trainable params
[2m[36m(pid=38477)[0m 0         Non-trainable params
[2m[36m(pid=38477)[0m 8.7 K     Total params
[2m[36m(pid=31019)[0m time to fit was 398.9725821018219
[2m[36m(pid=31019)[0m GPU available: False, used: False
[2m[36m(pid=31019)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31019)[0m 
[2m[36m(pid=31019)[0m   | Name      | Type              | Params
[2m[36m(pid=31019)[0m ------------------------------------------------
[2m[36m(pid=31019)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31019)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31019)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31019)[0m ------------------------------------------------
[2m[36m(pid=31019)[0m 8.7 K     Trainable params
[2m[36m(pid=31019)[0m 0         Non-trainable params
[2m[36m(pid=31019)[0m 8.7 K     Total params
[2m[36m(pid=42722)[0m time to fit was 107.68462109565735
Result for _inner_e8deb_00179:
  auc: 0.6210705697536468
  date: 2021-03-19_14-38-27
  done: false
  experiment_id: 398aed5b9941496d89ead8ac56164ea7
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 42722
  time_since_restore: 339.80863881111145
  time_this_iter_s: 339.80863881111145
  time_total_s: 339.80863881111145
  timestamp: 1616161107
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00179
  
== Status ==
Memory usage on this node: 8.6/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 42/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (21 RUNNING, 159 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    |       |           64 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |       |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |       |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |       |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |       |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00152 | RUNNING    |       |          128 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (11 RUNNING, 149 TERMINATED)


Result for _inner_e8deb_00179:[2m[36m(pid=42722)[0m Finished run with seed 0 - lr 5 - sec_lr 5 - bs 512 - mean val auc: 0.6210705697536468

  auc: 0.6210705697536468
  date: 2021-03-19_14-38-27
  done: true
  experiment_id: 398aed5b9941496d89ead8ac56164ea7
  experiment_tag: 179_batch_size=512,eta=0.0,lr=5,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 42722
  time_since_restore: 339.80863881111145
  time_this_iter_s: 339.80863881111145
  time_total_s: 339.80863881111145
  timestamp: 1616161107
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00179
  
[2m[36m(pid=31106)[0m time to fit was 814.7029633522034
Result for _inner_e8deb_00121:
  auc: 0.8970103025436401
  date: 2021-03-19_14-38-40
  done: false
  experiment_id: 9c15ad225eab465b9931cba33665da1f
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 31106
  time_since_restore: 4159.395418643951
  time_this_iter_s: 4159.395418643951
  time_total_s: 4159.395418643951
  timestamp: 1616161120
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00121
  
[2m[36m(pid=31106)[0m Finished run with seed 0 - lr 0.001 - sec_lr 2 - bs 64 - mean val auc: 0.8970103025436401
== Status ==
Memory usage on this node: 8.5/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 40/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (20 RUNNING, 160 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |                     |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00121 | RUNNING    | 145.101.32.82:31106 |           64 |     0 | 0.001 |    2     |      1 |          4159.4  | 0.89701  |
| _inner_e8deb_00140 | RUNNING    |                     |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                     |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    |                     |           64 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |                     |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |                     |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00152 | RUNNING    |                     |          128 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (10 RUNNING, 150 TERMINATED)


Result for _inner_e8deb_00121:
  auc: 0.8970103025436401
  date: 2021-03-19_14-38-40
  done: true
  experiment_id: 9c15ad225eab465b9931cba33665da1f
  experiment_tag: 121_batch_size=64,eta=0.0,lr=0.001,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 31106
  time_since_restore: 4159.395418643951
  time_this_iter_s: 4159.395418643951
  time_total_s: 4159.395418643951
  timestamp: 1616161120
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00121
  
[2m[36m(pid=38477)[0m time to fit was 96.20234251022339
[2m[36m(pid=38477)[0m GPU available: False, used: False
[2m[36m(pid=38477)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=38477)[0m 
[2m[36m(pid=38477)[0m   | Name      | Type              | Params
[2m[36m(pid=38477)[0m ------------------------------------------------
[2m[36m(pid=38477)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=38477)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=38477)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=38477)[0m ------------------------------------------------
[2m[36m(pid=38477)[0m 8.7 K     Trainable params
[2m[36m(pid=38477)[0m 0         Non-trainable params
[2m[36m(pid=38477)[0m 8.7 K     Total params
[2m[36m(pid=29932)[0m time to fit was 224.67631196975708
Result for _inner_e8deb_00146:
  auc: 0.7278820395469665
  date: 2021-03-19_14-39-12
  done: false
  experiment_id: 51d4c3ff646342cab05bbf080c371ddb
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 29932
  time_since_restore: 2532.245393514633
  time_this_iter_s: 2532.245393514633
  time_total_s: 2532.245393514633
  timestamp: 1616161152
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00146
  
[2m[36m(pid=29932)[0m Finished run with seed 0 - lr 5 - sec_lr 2 - bs 64 - mean val auc: 0.7278820395469665
== Status ==
Memory usage on this node: 8.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 38/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (19 RUNNING, 161 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |                     |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00140 | RUNNING    |                     |           32 |     0 | 2     |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                     |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00146 | RUNNING    | 145.101.32.82:29932 |           64 |     0 | 5     |    2     |      1 |          2532.25 | 0.727882 |
| _inner_e8deb_00150 | RUNNING    |                     |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |                     |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00152 | RUNNING    |                     |          128 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00155 | RUNNING    |                     |           32 |     0 | 0.01  |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (9 RUNNING, 151 TERMINATED)


Result for _inner_e8deb_00146:
  auc: 0.7278820395469665
  date: 2021-03-19_14-39-12
  done: true
  experiment_id: 51d4c3ff646342cab05bbf080c371ddb
  experiment_tag: 146_batch_size=64,eta=0.0,lr=5,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 29932
  time_since_restore: 2532.245393514633
  time_this_iter_s: 2532.245393514633
  time_total_s: 2532.245393514633
  timestamp: 1616161152
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00146
  
[2m[36m(pid=22604)[0m time to fit was 591.0252072811127
Result for _inner_e8deb_00140:
  auc: 0.9005393862724305
  date: 2021-03-19_14-39-13
  done: false
  experiment_id: fe6fd341127b4e92bc62f01809362f48
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 22604
  time_since_restore: 2777.9713835716248
  time_this_iter_s: 2777.9713835716248
  time_total_s: 2777.9713835716248
  timestamp: 1616161153
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00140
  
Result for _inner_e8deb_00140:
  auc: 0.9005393862724305
  date: 2021-03-19_14-39-13
  done: true
  experiment_id: fe6fd341127b4e92bc62f01809362f48
  experiment_tag: 140_batch_size=32,eta=0.0,lr=2,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 22604
  time_since_restore: 2777.9713835716248
  time_this_iter_s: 2777.9713835716248
  time_total_s: 2777.9713835716248
  timestamp: 1616161153
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00140
  
[2m[36m(pid=22604)[0m Finished run with seed 0 - lr 2 - sec_lr 2 - bs 32 - mean val auc: 0.9005393862724305
[2m[36m(pid=45882)[0m time to fit was 653.0401089191437
[2m[36m(pid=45882)[0m GPU available: False, used: False
[2m[36m(pid=45882)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=45882)[0m 
[2m[36m(pid=45882)[0m   | Name      | Type              | Params
[2m[36m(pid=45882)[0m ------------------------------------------------
[2m[36m(pid=45882)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=45882)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=45882)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=45882)[0m ------------------------------------------------
[2m[36m(pid=45882)[0m 8.7 K     Trainable params
[2m[36m(pid=45882)[0m 0         Non-trainable params
[2m[36m(pid=45882)[0m 8.7 K     Total params
[2m[36m(pid=3499)[0m time to fit was 320.3346607685089
[2m[36m(pid=3499)[0m GPU available: False, used: False
[2m[36m(pid=3499)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=3499)[0m 
[2m[36m(pid=3499)[0m   | Name      | Type              | Params
[2m[36m(pid=3499)[0m ------------------------------------------------
[2m[36m(pid=3499)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=3499)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=3499)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=3499)[0m ------------------------------------------------
[2m[36m(pid=3499)[0m 8.7 K     Trainable params
[2m[36m(pid=3499)[0m 0         Non-trainable params
[2m[36m(pid=3499)[0m 8.7 K     Total params
[2m[36m(pid=49344)[0m time to fit was 244.71808624267578
Result for _inner_e8deb_00156:
  auc: 0.9104632139205933
  date: 2021-03-19_14-39-50
  done: false
  experiment_id: c112bdb0297f4beea66354404a14b01d
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 49344
  time_since_restore: 1944.3648464679718
  time_this_iter_s: 1944.3648464679718
  time_total_s: 1944.3648464679718
  timestamp: 1616161190
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00156
  
[2m[36m(pid=49344)[0m Finished run with seed 0 - lr 0.01 - sec_lr 5 - bs 64 - mean val auc: 0.9104632139205933
== Status ==
Memory usage on this node: 7.9/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 34/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (17 RUNNING, 163 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |                     |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                     |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |                     |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |                     |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00152 | RUNNING    |                     |          128 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00155 | RUNNING    |                     |           32 |     0 | 0.01  |    5     |        |                  |          |
| _inner_e8deb_00156 | RUNNING    | 145.101.32.82:49344 |           64 |     0 | 0.01  |    5     |      1 |          1944.36 | 0.910463 |
| _inner_e8deb_00160 | RUNNING    |                     |           32 |     0 | 0.1   |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (7 RUNNING, 153 TERMINATED)


Result for _inner_e8deb_00156:
  auc: 0.9104632139205933
  date: 2021-03-19_14-39-50
  done: true
  experiment_id: c112bdb0297f4beea66354404a14b01d
  experiment_tag: 156_batch_size=64,eta=0.0,lr=0.01,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 49344
  time_since_restore: 1944.3648464679718
  time_this_iter_s: 1944.3648464679718
  time_total_s: 1944.3648464679718
  timestamp: 1616161190
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00156
  
[2m[36m(pid=35872)[0m time to fit was 227.73066234588623
[2m[36m(pid=35872)[0m GPU available: False, used: False
[2m[36m(pid=35872)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35872)[0m 
[2m[36m(pid=35872)[0m   | Name      | Type              | Params
[2m[36m(pid=35872)[0m ------------------------------------------------
[2m[36m(pid=35872)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35872)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35872)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35872)[0m ------------------------------------------------
[2m[36m(pid=35872)[0m 8.7 K     Trainable params
[2m[36m(pid=35872)[0m 0         Non-trainable params
[2m[36m(pid=35872)[0m 8.7 K     Total params
[2m[36m(pid=23176)[0m time to fit was 210.99729561805725
[2m[36m(pid=23176)[0m GPU available: False, used: False
[2m[36m(pid=23176)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=23176)[0m 
[2m[36m(pid=23176)[0m   | Name      | Type              | Params
[2m[36m(pid=23176)[0m ------------------------------------------------
[2m[36m(pid=23176)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=23176)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=23176)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=23176)[0m ------------------------------------------------
[2m[36m(pid=23176)[0m 8.7 K     Trainable params
[2m[36m(pid=23176)[0m 0         Non-trainable params
[2m[36m(pid=23176)[0m 8.7 K     Total params
[2m[36m(pid=31006)[0m time to fit was 425.4043393135071
[2m[36m(pid=31006)[0m GPU available: False, used: False
[2m[36m(pid=31006)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31006)[0m 
[2m[36m(pid=31006)[0m   | Name      | Type              | Params
[2m[36m(pid=31006)[0m ------------------------------------------------
[2m[36m(pid=31006)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31006)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31006)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31006)[0m ------------------------------------------------
[2m[36m(pid=31006)[0m 8.7 K     Trainable params
[2m[36m(pid=31006)[0m 0         Non-trainable params
[2m[36m(pid=31006)[0m 8.7 K     Total params
[2m[36m(pid=38477)[0m time to fit was 94.12400841712952
[2m[36m(pid=38477)[0m GPU available: False, used: False
[2m[36m(pid=38477)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=38477)[0m 
[2m[36m(pid=38477)[0m   | Name      | Type              | Params
[2m[36m(pid=38477)[0m ------------------------------------------------
[2m[36m(pid=38477)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=38477)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=38477)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=38477)[0m ------------------------------------------------
[2m[36m(pid=38477)[0m 8.7 K     Trainable params
[2m[36m(pid=38477)[0m 0         Non-trainable params
[2m[36m(pid=38477)[0m 8.7 K     Total params
[2m[36m(pid=31016)[0m time to fit was 220.32554411888123
Result for _inner_e8deb_00152:
  auc: 0.8833660364151001
  date: 2021-03-19_14-40-44
  done: false
  experiment_id: 74a178cf0faf4360a790c195cac776e1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 31016
  time_since_restore: 2261.9884564876556
  time_this_iter_s: 2261.9884564876556
  time_total_s: 2261.9884564876556
  timestamp: 1616161244
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00152
  
[2m[36m(pid=31016)[0m Finished run with seed 0 - lr 0.001 - sec_lr 5 - bs 128 - mean val auc: 0.8833660364151001
== Status ==
Memory usage on this node: 7.6/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 32/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (16 RUNNING, 164 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |                     |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |                     |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                     |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |                     |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |                     |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00152 | RUNNING    | 145.101.32.82:31016 |          128 |     0 | 0.001 |    5     |      1 |          2261.99 | 0.883366 |
| _inner_e8deb_00155 | RUNNING    |                     |           32 |     0 | 0.01  |    5     |        |                  |          |
| _inner_e8deb_00160 | RUNNING    |                     |           32 |     0 | 0.1   |    5     |        |                  |          |
| _inner_e8deb_00165 | RUNNING    |                     |           32 |     0 | 1     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (6 RUNNING, 154 TERMINATED)


Result for _inner_e8deb_00152:
  auc: 0.8833660364151001
  date: 2021-03-19_14-40-44
  done: true
  experiment_id: 74a178cf0faf4360a790c195cac776e1
  experiment_tag: 152_batch_size=128,eta=0.0,lr=0.001,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 31016
  time_since_restore: 2261.9884564876556
  time_this_iter_s: 2261.9884564876556
  time_total_s: 2261.9884564876556
  timestamp: 1616161244
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00152
  
[2m[36m(pid=26552)[0m time to fit was 287.68739581108093
[2m[36m(pid=26552)[0m GPU available: False, used: False
[2m[36m(pid=26552)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26552)[0m 
[2m[36m(pid=26552)[0m   | Name      | Type              | Params
[2m[36m(pid=26552)[0m ------------------------------------------------
[2m[36m(pid=26552)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26552)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26552)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26552)[0m ------------------------------------------------
[2m[36m(pid=26552)[0m 8.7 K     Trainable params
[2m[36m(pid=26552)[0m 0         Non-trainable params
[2m[36m(pid=26552)[0m 8.7 K     Total params
[2m[36m(pid=27334)[0m time to fit was 292.7566249370575
[2m[36m(pid=27334)[0m GPU available: False, used: False
[2m[36m(pid=27334)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27334)[0m 
[2m[36m(pid=27334)[0m   | Name      | Type              | Params
[2m[36m(pid=27334)[0m ------------------------------------------------
[2m[36m(pid=27334)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27334)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27334)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27334)[0m ------------------------------------------------
[2m[36m(pid=27334)[0m 8.7 K     Trainable params
[2m[36m(pid=27334)[0m 0         Non-trainable params
[2m[36m(pid=27334)[0m 8.7 K     Total params
[2m[36m(pid=38477)[0m time to fit was 104.56752872467041
Result for _inner_e8deb_00177:
  auc: 0.6782505631446838
  date: 2021-03-19_14-42-00
  done: false
  experiment_id: 433db45fdb234666acbf97fe9ef3aabc
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 38477
  time_since_restore: 700.1314523220062
  time_this_iter_s: 700.1314523220062
  time_total_s: 700.1314523220062
  timestamp: 1616161320
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00177
  
[2m[36m(pid=38477)[0m Finished run with seed 0 - lr 5 - sec_lr 5 - bs 128 - mean val auc: 0.6782505631446838
== Status ==
Memory usage on this node: 7.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 30/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (15 RUNNING, 165 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    |       |           32 |     0 | 0.001 |    0.1   |        |                  |          |
| _inner_e8deb_00115 | RUNNING    |       |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |       |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |       |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |       |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |       |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00155 | RUNNING    |       |           32 |     0 | 0.01  |    5     |        |                  |          |
| _inner_e8deb_00160 | RUNNING    |       |           32 |     0 | 0.1   |    5     |        |                  |          |
| _inner_e8deb_00165 | RUNNING    |       |           32 |     0 | 1     |    5     |        |                  |          |
| _inner_e8deb_00166 | RUNNING    |       |           64 |     0 | 1     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (5 RUNNING, 155 TERMINATED)


Result for _inner_e8deb_00177:
  auc: 0.6782505631446838
  date: 2021-03-19_14-42-00
  done: true
  experiment_id: 433db45fdb234666acbf97fe9ef3aabc
  experiment_tag: 177_batch_size=128,eta=0.0,lr=5,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 38477
  time_since_restore: 700.1314523220062
  time_this_iter_s: 700.1314523220062
  time_total_s: 700.1314523220062
  timestamp: 1616161320
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00177
  
[2m[36m(pid=23176)[0m time to fit was 117.66691875457764
Result for _inner_e8deb_00166:
  auc: 0.9090475678443909
  date: 2021-03-19_14-42-05
  done: false
  experiment_id: 67cd900ff6ab4de19a1c2917a3359183
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 23176
  time_since_restore: 1195.4974575042725
  time_this_iter_s: 1195.4974575042725
  time_total_s: 1195.4974575042725
  timestamp: 1616161325
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00166
  
Result for _inner_e8deb_00166:
  auc: 0.9090475678443909
  date: 2021-03-19_14-42-05
  done: true
  experiment_id: 67cd900ff6ab4de19a1c2917a3359183
  experiment_tag: 166_batch_size=64,eta=0.0,lr=1,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 23176
  time_since_restore: 1195.4974575042725
  time_this_iter_s: 1195.4974575042725
  time_total_s: 1195.4974575042725
  timestamp: 1616161325
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00166
  
[2m[36m(pid=23176)[0m Finished run with seed 0 - lr 1 - sec_lr 5 - bs 64 - mean val auc: 0.9090475678443909
[2m[36m(pid=19841)[0m time to fit was 345.01445603370667
[2m[36m(pid=19841)[0m GPU available: False, used: False
[2m[36m(pid=19841)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=19841)[0m 
[2m[36m(pid=19841)[0m   | Name      | Type              | Params
[2m[36m(pid=19841)[0m ------------------------------------------------
[2m[36m(pid=19841)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=19841)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=19841)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=19841)[0m ------------------------------------------------
[2m[36m(pid=19841)[0m 8.7 K     Trainable params
[2m[36m(pid=19841)[0m 0         Non-trainable params
[2m[36m(pid=19841)[0m 8.7 K     Total params
[2m[36m(pid=31019)[0m time to fit was 282.0276679992676
[2m[36m(pid=31019)[0m GPU available: False, used: False
[2m[36m(pid=31019)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31019)[0m 
[2m[36m(pid=31019)[0m   | Name      | Type              | Params
[2m[36m(pid=31019)[0m ------------------------------------------------
[2m[36m(pid=31019)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31019)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31019)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31019)[0m ------------------------------------------------
[2m[36m(pid=31019)[0m 8.7 K     Trainable params
[2m[36m(pid=31019)[0m 0         Non-trainable params
[2m[36m(pid=31019)[0m 8.7 K     Total params
[2m[36m(pid=29455)[0m time to fit was 1001.9900872707367
[2m[36m(pid=29455)[0m GPU available: False, used: False
[2m[36m(pid=29455)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29455)[0m 
[2m[36m(pid=29455)[0m   | Name      | Type              | Params
[2m[36m(pid=29455)[0m ------------------------------------------------
[2m[36m(pid=29455)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29455)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29455)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29455)[0m ------------------------------------------------
[2m[36m(pid=29455)[0m 8.7 K     Trainable params
[2m[36m(pid=29455)[0m 0         Non-trainable params
[2m[36m(pid=29455)[0m 8.7 K     Total params
[2m[36m(pid=45882)[0m time to fit was 206.96257162094116
[2m[36m(pid=45882)[0m GPU available: False, used: False
[2m[36m(pid=45882)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=45882)[0m 
[2m[36m(pid=45882)[0m   | Name      | Type              | Params
[2m[36m(pid=45882)[0m ------------------------------------------------
[2m[36m(pid=45882)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=45882)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=45882)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=45882)[0m ------------------------------------------------
[2m[36m(pid=45882)[0m 8.7 K     Trainable params
[2m[36m(pid=45882)[0m 0         Non-trainable params
[2m[36m(pid=45882)[0m 8.7 K     Total params
[2m[36m(pid=6688)[0m time to fit was 1516.7570960521698
Result for _inner_e8deb_00060:
  auc: 0.9082527041435242
  date: 2021-03-19_14-43-24
  done: false
  experiment_id: aa9f51712ee74b8687329b97b68a350b
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 6688
  time_since_restore: 8602.696579933167
  time_this_iter_s: 8602.696579933167
  time_total_s: 8602.696579933167
  timestamp: 1616161404
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00060
  
[2m[36m(pid=6688)[0m Finished run with seed 0 - lr 0.001 - sec_lr 0.1 - bs 32 - mean val auc: 0.9082527041435242
== Status ==
Memory usage on this node: 7.1/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 26/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (13 RUNNING, 167 TERMINATED)
+--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00060 | RUNNING    | 145.101.32.82:6688 |           32 |     0 | 0.001 |    0.1   |      1 |          8602.7  | 0.908253 |
| _inner_e8deb_00115 | RUNNING    |                    |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |                    |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                    |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |                    |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |                    |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00155 | RUNNING    |                    |           32 |     0 | 0.01  |    5     |        |                  |          |
| _inner_e8deb_00160 | RUNNING    |                    |           32 |     0 | 0.1   |    5     |        |                  |          |
| _inner_e8deb_00165 | RUNNING    |                    |           32 |     0 | 1     |    5     |        |                  |          |
| _inner_e8deb_00170 | RUNNING    |                    |           32 |     0 | 2     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                    |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                    |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                    |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                    |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                    |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                    |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                    |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                    |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                    |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                    |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (3 RUNNING, 157 TERMINATED)


Result for _inner_e8deb_00060:
  auc: 0.9082527041435242
  date: 2021-03-19_14-43-24
  done: true
  experiment_id: aa9f51712ee74b8687329b97b68a350b
  experiment_tag: 60_batch_size=32,eta=0.0,lr=0.001,sec_lr=0.1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 6688
  time_since_restore: 8602.696579933167
  time_this_iter_s: 8602.696579933167
  time_total_s: 8602.696579933167
  timestamp: 1616161404
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00060
  
[2m[36m(pid=3499)[0m time to fit was 246.54982018470764
Result for _inner_e8deb_00160:
  auc: 0.9085903525352478
  date: 2021-03-19_14-43-48
  done: false
  experiment_id: 0cd65ed8b17446948ef65d873719a5d1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 3499
  time_since_restore: 1947.1772344112396
  time_this_iter_s: 1947.1772344112396
  time_total_s: 1947.1772344112396
  timestamp: 1616161428
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00160
  
== Status ==
Memory usage on this node: 6.8/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 24/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (12 RUNNING, 168 TERMINATED)
+--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00115 | RUNNING    |                    |           32 |     0 | 5     |    1     |        |                  |          |
| _inner_e8deb_00120 | RUNNING    |                    |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                    |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |                    |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |                    |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00155 | RUNNING    |                    |           32 |     0 | 0.01  |    5     |        |                  |          |
| _inner_e8deb_00160 | RUNNING    | 145.101.32.82:3499 |           32 |     0 | 0.1   |    5     |      1 |          1947.18 | 0.90859  |
| _inner_e8deb_00165 | RUNNING    |                    |           32 |     0 | 1     |    5     |        |                  |          |
| _inner_e8deb_00170 | RUNNING    |                    |           32 |     0 | 2     |    5     |        |                  |          |
| _inner_e8deb_00171 | RUNNING    |                    |           64 |     0 | 2     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                    |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                    |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                    |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                    |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                    |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                    |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                    |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                    |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                    |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                    |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+--------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (2 RUNNING, 158 TERMINATED)


Result for _inner_e8deb_00160:
  auc: 0.9085903525352478
  date: 2021-03-19_14-43-48
  done: true
  experiment_id: 0cd65ed8b17446948ef65d873719a5d1
  experiment_tag: 160_batch_size=32,eta=0.0,lr=0.1,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 3499
  time_since_restore: 1947.1772344112396
  time_this_iter_s: 1947.1772344112396
  time_total_s: 1947.1772344112396
  timestamp: 1616161428
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00160
  [2m[36m(pid=3499)[0m Finished run with seed 0 - lr 0.1 - sec_lr 5 - bs 32 - mean val auc: 0.9085903525352478

[2m[36m(pid=31641)[0m time to fit was 1043.7405099868774
[2m[36m(pid=31641)[0m GPU available: False, used: False
[2m[36m(pid=31641)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31641)[0m 
[2m[36m(pid=31641)[0m   | Name      | Type              | Params
[2m[36m(pid=31641)[0m ------------------------------------------------
[2m[36m(pid=31641)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31641)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31641)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31641)[0m ------------------------------------------------
[2m[36m(pid=31641)[0m 8.7 K     Trainable params
[2m[36m(pid=31641)[0m 0         Non-trainable params
[2m[36m(pid=31641)[0m 8.7 K     Total params
[2m[36m(pid=31006)[0m time to fit was 251.17796921730042
[2m[36m(pid=31006)[0m GPU available: False, used: False
[2m[36m(pid=31006)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31006)[0m 
[2m[36m(pid=31006)[0m   | Name      | Type              | Params
[2m[36m(pid=31006)[0m ------------------------------------------------
[2m[36m(pid=31006)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31006)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31006)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31006)[0m ------------------------------------------------
[2m[36m(pid=31006)[0m 8.7 K     Trainable params
[2m[36m(pid=31006)[0m 0         Non-trainable params
[2m[36m(pid=31006)[0m 8.7 K     Total params
[2m[36m(pid=26552)[0m time to fit was 217.8461833000183
[2m[36m(pid=26552)[0m GPU available: False, used: False
[2m[36m(pid=26552)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26552)[0m 
[2m[36m(pid=26552)[0m   | Name      | Type              | Params
[2m[36m(pid=26552)[0m ------------------------------------------------
[2m[36m(pid=26552)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26552)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26552)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26552)[0m ------------------------------------------------
[2m[36m(pid=26552)[0m 8.7 K     Trainable params
[2m[36m(pid=26552)[0m 0         Non-trainable params
[2m[36m(pid=26552)[0m 8.7 K     Total params
[2m[36m(pid=23636)[0m time to fit was 1171.0804769992828
Result for _inner_e8deb_00115:
  auc: 0.6997745633125305
  date: 2021-03-19_14-45-18
  done: false
  experiment_id: 58a8191c9a6e4326995d3ff8197db075
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 23636
  time_since_restore: 4814.869384288788
  time_this_iter_s: 4814.869384288788
  time_total_s: 4814.869384288788
  timestamp: 1616161518
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00115
  
== Status ==
Memory usage on this node: 6.7/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 22/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (11 RUNNING, 169 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00115 | RUNNING    | 145.101.32.82:23636 |           32 |     0 | 5     |    1     |      1 |          4814.87 | 0.699775 |
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                     |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |                     |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |                     |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00155 | RUNNING    |                     |           32 |     0 | 0.01  |    5     |        |                  |          |
| _inner_e8deb_00165 | RUNNING    |                     |           32 |     0 | 1     |    5     |        |                  |          |
| _inner_e8deb_00170 | RUNNING    |                     |           32 |     0 | 2     |    5     |        |                  |          |
| _inner_e8deb_00171 | RUNNING    |                     |           64 |     0 | 2     |    5     |        |                  |          |
| _inner_e8deb_00175 | RUNNING    |                     |           32 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (1 RUNNING, 159 TERMINATED)


Result for _inner_e8deb_00115:
  auc: 0.6997745633125305
  date: 2021-03-19_14-45-18
  done: true
  experiment_id: 58a8191c9a6e4326995d3ff8197db075
  experiment_tag: 115_batch_size=32,eta=0.0,lr=5,sec_lr=1
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 23636
  time_since_restore: 4814.869384288788
  time_this_iter_s: 4814.869384288788
  time_total_s: 4814.869384288788
  timestamp: 1616161518
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00115
  [2m[36m(pid=23636)[0m Finished run with seed 0 - lr 5 - sec_lr 1 - bs 32 - mean val auc: 0.6997745633125305

[2m[36m(pid=29455)[0m time to fit was 156.10968613624573
[2m[36m(pid=29455)[0m GPU available: False, used: False
[2m[36m(pid=29455)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29455)[0m 
[2m[36m(pid=29455)[0m   | Name      | Type              | Params
[2m[36m(pid=29455)[0m ------------------------------------------------
[2m[36m(pid=29455)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29455)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29455)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29455)[0m ------------------------------------------------
[2m[36m(pid=29455)[0m 8.7 K     Trainable params
[2m[36m(pid=29455)[0m 0         Non-trainable params
[2m[36m(pid=29455)[0m 8.7 K     Total params
[2m[36m(pid=27334)[0m time to fit was 218.95483422279358
[2m[36m(pid=27334)[0m GPU available: False, used: False
[2m[36m(pid=27334)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=27334)[0m 
[2m[36m(pid=27334)[0m   | Name      | Type              | Params
[2m[36m(pid=27334)[0m ------------------------------------------------
[2m[36m(pid=27334)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=27334)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=27334)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=27334)[0m ------------------------------------------------
[2m[36m(pid=27334)[0m 8.7 K     Trainable params
[2m[36m(pid=27334)[0m 0         Non-trainable params
[2m[36m(pid=27334)[0m 8.7 K     Total params
[2m[36m(pid=19841)[0m time to fit was 219.92679405212402
[2m[36m(pid=19841)[0m GPU available: False, used: False
[2m[36m(pid=19841)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=19841)[0m 
[2m[36m(pid=19841)[0m   | Name      | Type              | Params
[2m[36m(pid=19841)[0m ------------------------------------------------
[2m[36m(pid=19841)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=19841)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=19841)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=19841)[0m ------------------------------------------------
[2m[36m(pid=19841)[0m 8.7 K     Trainable params
[2m[36m(pid=19841)[0m 0         Non-trainable params
[2m[36m(pid=19841)[0m 8.7 K     Total params
[2m[36m(pid=31019)[0m time to fit was 225.2275743484497
[2m[36m(pid=31019)[0m GPU available: False, used: False
[2m[36m(pid=31019)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31019)[0m 
[2m[36m(pid=31019)[0m   | Name      | Type              | Params
[2m[36m(pid=31019)[0m ------------------------------------------------
[2m[36m(pid=31019)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31019)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31019)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31019)[0m ------------------------------------------------
[2m[36m(pid=31019)[0m 8.7 K     Trainable params
[2m[36m(pid=31019)[0m 0         Non-trainable params
[2m[36m(pid=31019)[0m 8.7 K     Total params
[2m[36m(pid=31641)[0m time to fit was 145.39307475090027
[2m[36m(pid=31641)[0m GPU available: False, used: False
[2m[36m(pid=31641)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31641)[0m 
[2m[36m(pid=31641)[0m   | Name      | Type              | Params
[2m[36m(pid=31641)[0m ------------------------------------------------
[2m[36m(pid=31641)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31641)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31641)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31641)[0m ------------------------------------------------
[2m[36m(pid=31641)[0m 8.7 K     Trainable params
[2m[36m(pid=31641)[0m 0         Non-trainable params
[2m[36m(pid=31641)[0m 8.7 K     Total params
[2m[36m(pid=35872)[0m time to fit was 392.4281756877899
[2m[36m(pid=35872)[0m GPU available: False, used: False
[2m[36m(pid=35872)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=35872)[0m 
[2m[36m(pid=35872)[0m   | Name      | Type              | Params
[2m[36m(pid=35872)[0m ------------------------------------------------
[2m[36m(pid=35872)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=35872)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=35872)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=35872)[0m ------------------------------------------------
[2m[36m(pid=35872)[0m 8.7 K     Trainable params
[2m[36m(pid=35872)[0m 0         Non-trainable params
[2m[36m(pid=35872)[0m 8.7 K     Total params
[2m[36m(pid=27334)[0m time to fit was 70.76994705200195
Result for _inner_e8deb_00171:
  auc: 0.9028917908668518
  date: 2021-03-19_14-46-43
  done: false
  experiment_id: 7d61ee1b95234d7c9e0bbc7e25785363
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 27334
  time_since_restore: 1349.693559885025
  time_this_iter_s: 1349.693559885025
  time_total_s: 1349.693559885025
  timestamp: 1616161603
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00171
  
== Status ==
Memory usage on this node: 6.6/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 20/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (10 RUNNING, 170 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                     |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |                     |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    |                     |           64 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00155 | RUNNING    |                     |           32 |     0 | 0.01  |    5     |        |                  |          |
| _inner_e8deb_00165 | RUNNING    |                     |           32 |     0 | 1     |    5     |        |                  |          |
| _inner_e8deb_00170 | RUNNING    |                     |           32 |     0 | 2     |    5     |        |                  |          |
| _inner_e8deb_00171 | RUNNING    | 145.101.32.82:27334 |           64 |     0 | 2     |    5     |      1 |          1349.69 | 0.902892 |
| _inner_e8deb_00175 | RUNNING    |                     |           32 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00176 | RUNNING    |                     |           64 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (160 TERMINATED)


Result for _inner_e8deb_00171:
  auc: 0.9028917908668518
  date: 2021-03-19_14-46-43
  done: true
  experiment_id: 7d61ee1b95234d7c9e0bbc7e25785363
  experiment_tag: 171_batch_size=64,eta=0.0,lr=2,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 27334
  time_since_restore: 1349.693559885025
  time_this_iter_s: 1349.693559885025
  time_total_s: 1349.693559885025
  timestamp: 1616161603
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00171
  
[2m[36m(pid=27334)[0m Finished run with seed 0 - lr 2 - sec_lr 5 - bs 64 - mean val auc: 0.9028917908668518
[2m[36m(pid=31006)[0m time to fit was 148.39931631088257
Result for _inner_e8deb_00151:
  auc: 0.8884048819541931
  date: 2021-03-19_14-46-51
  done: false
  experiment_id: 8e234663c76842cb906902d49d6072a0
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 31006
  time_since_restore: 2683.1234543323517
  time_this_iter_s: 2683.1234543323517
  time_total_s: 2683.1234543323517
  timestamp: 1616161611
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00151
  
== Status ==
Memory usage on this node: 6.3/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 18/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (9 RUNNING, 171 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                     |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |                     |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00151 | RUNNING    | 145.101.32.82:31006 |           64 |     0 | 0.001 |    5     |      1 |          2683.12 | 0.888405 |
| _inner_e8deb_00155 | RUNNING    |                     |           32 |     0 | 0.01  |    5     |        |                  |          |
| _inner_e8deb_00165 | RUNNING    |                     |           32 |     0 | 1     |    5     |        |                  |          |
| _inner_e8deb_00170 | RUNNING    |                     |           32 |     0 | 2     |    5     |        |                  |          |
| _inner_e8deb_00175 | RUNNING    |                     |           32 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00176 | RUNNING    |                     |           64 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (160 TERMINATED)


[2m[36m(pid=31006)[0m Finished run with seed 0 - lr 0.001 - sec_lr 5 - bs 64 - mean val auc: 0.8884048819541931
Result for _inner_e8deb_00151:
  auc: 0.8884048819541931
  date: 2021-03-19_14-46-51
  done: true
  experiment_id: 8e234663c76842cb906902d49d6072a0
  experiment_tag: 151_batch_size=64,eta=0.0,lr=0.001,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 31006
  time_since_restore: 2683.1234543323517
  time_this_iter_s: 2683.1234543323517
  time_total_s: 2683.1234543323517
  timestamp: 1616161611
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00151
  
[2m[36m(pid=45882)[0m time to fit was 254.3221480846405
Result for _inner_e8deb_00155:
  auc: 0.9080713987350464
  date: 2021-03-19_14-47-13
  done: false
  experiment_id: 43a1b78fcfef428fbda0b2ce713d6d9a
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 45882
  time_since_restore: 2501.2188065052032
  time_this_iter_s: 2501.2188065052032
  time_total_s: 2501.2188065052032
  timestamp: 1616161633
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00155
  
== Status ==
Memory usage on this node: 6.0/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 16/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (8 RUNNING, 172 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                     |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |                     |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00155 | RUNNING    | 145.101.32.82:45882 |           32 |     0 | 0.01  |    5     |      1 |          2501.22 | 0.908071 |
| _inner_e8deb_00165 | RUNNING    |                     |           32 |     0 | 1     |    5     |        |                  |          |
| _inner_e8deb_00170 | RUNNING    |                     |           32 |     0 | 2     |    5     |        |                  |          |
| _inner_e8deb_00175 | RUNNING    |                     |           32 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00176 | RUNNING    |                     |           64 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (160 TERMINATED)


Result for _inner_e8deb_00155:
  auc: 0.9080713987350464
  date: 2021-03-19_14-47-13
  done: true
  experiment_id: 43a1b78fcfef428fbda0b2ce713d6d9a
  experiment_tag: 155_batch_size=32,eta=0.0,lr=0.01,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 45882
  time_since_restore: 2501.2188065052032
  time_this_iter_s: 2501.2188065052032
  time_total_s: 2501.2188065052032
  timestamp: 1616161633
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00155
  
[2m[36m(pid=45882)[0m Finished run with seed 0 - lr 0.01 - sec_lr 5 - bs 32 - mean val auc: 0.9080713987350464
[2m[36m(pid=29801)[0m time to fit was 1046.6332428455353
[2m[36m(pid=29801)[0m GPU available: False, used: False
[2m[36m(pid=29801)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29801)[0m 
[2m[36m(pid=29801)[0m   | Name      | Type              | Params
[2m[36m(pid=29801)[0m ------------------------------------------------
[2m[36m(pid=29801)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29801)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29801)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29801)[0m ------------------------------------------------
[2m[36m(pid=29801)[0m 8.7 K     Trainable params
[2m[36m(pid=29801)[0m 0         Non-trainable params
[2m[36m(pid=29801)[0m 8.7 K     Total params
[2m[36m(pid=35872)[0m time to fit was 56.380881786346436
Result for _inner_e8deb_00176:
  auc: 0.5919669151306153
  date: 2021-03-19_14-47-35
  done: false
  experiment_id: 2033372739f24e6c9408c70f47535f89
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35872
  time_since_restore: 1128.006113767624
  time_this_iter_s: 1128.006113767624
  time_total_s: 1128.006113767624
  timestamp: 1616161655
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00176
  
[2m[36m(pid=35872)[0m Finished run with seed 0 - lr 5 - sec_lr 5 - bs 64 - mean val auc: 0.5919669151306153
== Status ==
Memory usage on this node: 5.9/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 14/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (7 RUNNING, 173 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                     |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |                     |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00165 | RUNNING    |                     |           32 |     0 | 1     |    5     |        |                  |          |
| _inner_e8deb_00170 | RUNNING    |                     |           32 |     0 | 2     |    5     |        |                  |          |
| _inner_e8deb_00175 | RUNNING    |                     |           32 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00176 | RUNNING    | 145.101.32.82:35872 |           64 |     0 | 5     |    5     |      1 |          1128.01 | 0.591967 |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |          9052.73 | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |          4880.39 | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |          2820.88 | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |          1730.81 | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |          1215.21 | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |          7539.4  | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |          3864.76 | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |          2667.71 | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |          1738.32 | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |          1207.23 | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |          3300.48 | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |          1861.56 | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |          1197.15 | 0.9138   |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (160 TERMINATED)


Result for _inner_e8deb_00176:
  auc: 0.5919669151306153
  date: 2021-03-19_14-47-35
  done: true
  experiment_id: 2033372739f24e6c9408c70f47535f89
  experiment_tag: 176_batch_size=64,eta=0.0,lr=5,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 35872
  time_since_restore: 1128.006113767624
  time_this_iter_s: 1128.006113767624
  time_total_s: 1128.006113767624
  timestamp: 1616161655
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00176
  
[2m[36m(pid=26552)[0m time to fit was 197.69838309288025
[2m[36m(pid=26552)[0m GPU available: False, used: False
[2m[36m(pid=26552)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=26552)[0m 
[2m[36m(pid=26552)[0m   | Name      | Type              | Params
[2m[36m(pid=26552)[0m ------------------------------------------------
[2m[36m(pid=26552)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=26552)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=26552)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=26552)[0m ------------------------------------------------
[2m[36m(pid=26552)[0m 8.7 K     Trainable params
[2m[36m(pid=26552)[0m 0         Non-trainable params
[2m[36m(pid=26552)[0m 8.7 K     Total params
Result for _inner_e8deb_00165:
  auc: 0.9055731534957886
  date: 2021-03-19_14-48-26
  done: false
  experiment_id: 9ba2f5b91896463cb21fce773184930d
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 19841
  time_since_restore: 1681.870525598526
  time_this_iter_s: 1681.870525598526
  time_total_s: 1681.870525598526
  timestamp: 1616161706
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00165
  
== Status ==
Memory usage on this node: 5.7/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 12/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (6 RUNNING, 174 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                     |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    |                     |           32 |     0 | 0.001 |    5     |        |                  |          |
| _inner_e8deb_00165 | RUNNING    | 145.101.32.82:19841 |           32 |     0 | 1     |    5     |      1 |         1681.87  | 0.905573 |
| _inner_e8deb_00170 | RUNNING    |                     |           32 |     0 | 2     |    5     |        |                  |          |
| _inner_e8deb_00175 | RUNNING    |                     |           32 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |         9052.73  | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |         4880.39  | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |         7539.4   | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |         3864.76  | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (160 TERMINATED)


[2m[36m(pid=19841)[0m time to fit was 135.26278376579285
[2m[36m(pid=19841)[0m Finished run with seed 0 - lr 1 - sec_lr 5 - bs 32 - mean val auc: 0.9055731534957886
Result for _inner_e8deb_00165:
  auc: 0.9055731534957886
  date: 2021-03-19_14-48-26
  done: true
  experiment_id: 9ba2f5b91896463cb21fce773184930d
  experiment_tag: 165_batch_size=32,eta=0.0,lr=1,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 19841
  time_since_restore: 1681.870525598526
  time_this_iter_s: 1681.870525598526
  time_total_s: 1681.870525598526
  timestamp: 1616161706
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00165
  
[2m[36m(pid=31019)[0m time to fit was 139.45259809494019
Result for _inner_e8deb_00150:
  auc: 0.8889848709106445
  date: 2021-03-19_14-48-39
  done: false
  experiment_id: a635b160d5a3417cbc9f505542351bcd
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 31019
  time_since_restore: 2806.299375772476
  time_this_iter_s: 2806.299375772476
  time_total_s: 2806.299375772476
  timestamp: 1616161719
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00150
  
[2m[36m(pid=31019)[0m Finished run with seed 0 - lr 0.001 - sec_lr 5 - bs 32 - mean val auc: 0.8889848709106445
== Status ==
Memory usage on this node: 5.4/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 10/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (5 RUNNING, 175 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                     |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00150 | RUNNING    | 145.101.32.82:31019 |           32 |     0 | 0.001 |    5     |      1 |         2806.3   | 0.888985 |
| _inner_e8deb_00170 | RUNNING    |                     |           32 |     0 | 2     |    5     |        |                  |          |
| _inner_e8deb_00175 | RUNNING    |                     |           32 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |         9052.73  | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |         4880.39  | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |         7539.4   | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |         3864.76  | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |                     |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (160 TERMINATED)


Result for _inner_e8deb_00150:
  auc: 0.8889848709106445
  date: 2021-03-19_14-48-39
  done: true
  experiment_id: a635b160d5a3417cbc9f505542351bcd
  experiment_tag: 150_batch_size=32,eta=0.0,lr=0.001,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 31019
  time_since_restore: 2806.299375772476
  time_this_iter_s: 2806.299375772476
  time_total_s: 2806.299375772476
  timestamp: 1616161719
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00150
  
Result for _inner_e8deb_00170:
  auc: 0.8517366766929626
  date: 2021-03-19_14-48-57
  done: false
  experiment_id: e8fb01712f9d47e1b7e111505fa1709a
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 26552
  time_since_restore: 1504.1863403320312
  time_this_iter_s: 1504.1863403320312
  time_total_s: 1504.1863403320312
  timestamp: 1616161737
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00170
  
== Status ==
Memory usage on this node: 5.2/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 8/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (4 RUNNING, 176 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00120 | RUNNING    |                     |           32 |     0 | 0.001 |    2     |        |                  |          |
| _inner_e8deb_00145 | RUNNING    |                     |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00170 | RUNNING    | 145.101.32.82:26552 |           32 |     0 | 2     |    5     |      1 |         1504.19  | 0.851737 |
| _inner_e8deb_00175 | RUNNING    |                     |           32 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |         9052.73  | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |         4880.39  | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |         7539.4   | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |         3864.76  | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |                     |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00015 | TERMINATED |                     |           32 |     0 | 1     |    0.001 |      1 |         3119.15  | 0.912845 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (160 TERMINATED)


Result for _inner_e8deb_00170:
  auc: 0.8517366766929626
  date: 2021-03-19_14-48-57
  done: true
  experiment_id: e8fb01712f9d47e1b7e111505fa1709a
  experiment_tag: 170_batch_size=32,eta=0.0,lr=2,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 26552
  time_since_restore: 1504.1863403320312
  time_this_iter_s: 1504.1863403320312
  time_total_s: 1504.1863403320312
  timestamp: 1616161737
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00170
  
[2m[36m(pid=26552)[0m time to fit was 70.92941546440125
[2m[36m(pid=26552)[0m Finished run with seed 0 - lr 2 - sec_lr 5 - bs 32 - mean val auc: 0.8517366766929626
Result for _inner_e8deb_00120:
  auc: 0.8978522777557373
  date: 2021-03-19_14-49-25
  done: false
  experiment_id: af9c4613018b4331b47cbd2f7588c9b6
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 29801
  time_since_restore: 4846.070712804794
  time_this_iter_s: 4846.070712804794
  time_total_s: 4846.070712804794
  timestamp: 1616161765
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00120
  
== Status ==
Memory usage on this node: 5.0/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (3 RUNNING, 177 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00120 | RUNNING    | 145.101.32.82:29801 |           32 |     0 | 0.001 |    2     |      1 |         4846.07  | 0.897852 |
| _inner_e8deb_00145 | RUNNING    |                     |           32 |     0 | 5     |    2     |        |                  |          |
| _inner_e8deb_00175 | RUNNING    |                     |           32 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |         9052.73  | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |         4880.39  | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |         7539.4   | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |         3864.76  | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |                     |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00015 | TERMINATED |                     |           32 |     0 | 1     |    0.001 |      1 |         3119.15  | 0.912845 |
| _inner_e8deb_00016 | TERMINATED |                     |           64 |     0 | 1     |    0.001 |      1 |         1864.57  | 0.913413 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (160 TERMINATED)


Result for _inner_e8deb_00120:
  auc: 0.8978522777557373
  date: 2021-03-19_14-49-25
  done: true
  experiment_id: af9c4613018b4331b47cbd2f7588c9b6
  experiment_tag: 120_batch_size=32,eta=0.0,lr=0.001,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 29801
  time_since_restore: 4846.070712804794
  time_this_iter_s: 4846.070712804794
  time_total_s: 4846.070712804794
  timestamp: 1616161765
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00120
  
[2m[36m(pid=29801)[0m time to fit was 126.59687376022339
[2m[36m(pid=29801)[0m Finished run with seed 0 - lr 0.001 - sec_lr 2 - bs 32 - mean val auc: 0.8978522777557373
[2m[36m(pid=29455)[0m time to fit was 259.55550956726074
[2m[36m(pid=29455)[0m GPU available: False, used: False
[2m[36m(pid=29455)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=29455)[0m 
[2m[36m(pid=29455)[0m   | Name      | Type              | Params
[2m[36m(pid=29455)[0m ------------------------------------------------
[2m[36m(pid=29455)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=29455)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=29455)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=29455)[0m ------------------------------------------------
[2m[36m(pid=29455)[0m 8.7 K     Trainable params
[2m[36m(pid=29455)[0m 0         Non-trainable params
[2m[36m(pid=29455)[0m 8.7 K     Total params
[2m[36m(pid=29455)[0m time to fit was 37.187211751937866
Result for _inner_e8deb_00145:
  auc: 0.6548390984535217
  date: 2021-03-19_14-50-17
  done: false
  experiment_id: 2b6dc34d22454fddb01f82362321ed03
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 29455
  time_since_restore: 3211.2054748535156
  time_this_iter_s: 3211.2054748535156
  time_total_s: 3211.2054748535156
  timestamp: 1616161817
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00145
  
== Status ==
Memory usage on this node: 4.8/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 4/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (2 RUNNING, 178 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00145 | RUNNING    | 145.101.32.82:29455 |           32 |     0 | 5     |    2     |      1 |         3211.21  | 0.654839 |
| _inner_e8deb_00175 | RUNNING    |                     |           32 |     0 | 5     |    5     |        |                  |          |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |         9052.73  | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |         4880.39  | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |         7539.4   | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |         3864.76  | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |                     |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00015 | TERMINATED |                     |           32 |     0 | 1     |    0.001 |      1 |         3119.15  | 0.912845 |
| _inner_e8deb_00016 | TERMINATED |                     |           64 |     0 | 1     |    0.001 |      1 |         1864.57  | 0.913413 |
| _inner_e8deb_00017 | TERMINATED |                     |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (160 TERMINATED)


Result for _inner_e8deb_00145:
  auc: 0.6548390984535217
  date: 2021-03-19_14-50-17
  done: true
  experiment_id: 2b6dc34d22454fddb01f82362321ed03
  experiment_tag: 145_batch_size=32,eta=0.0,lr=5,sec_lr=2
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 29455
  time_since_restore: 3211.2054748535156
  time_this_iter_s: 3211.2054748535156
  time_total_s: 3211.2054748535156
  timestamp: 1616161817
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00145
  
[2m[36m(pid=29455)[0m Finished run with seed 0 - lr 5 - sec_lr 2 - bs 32 - mean val auc: 0.6548390984535217
[2m[36m(pid=31641)[0m time to fit was 237.56686925888062
[2m[36m(pid=31641)[0m GPU available: False, used: False
[2m[36m(pid=31641)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31641)[0m 
[2m[36m(pid=31641)[0m   | Name      | Type              | Params
[2m[36m(pid=31641)[0m ------------------------------------------------
[2m[36m(pid=31641)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31641)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31641)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31641)[0m ------------------------------------------------
[2m[36m(pid=31641)[0m 8.7 K     Trainable params
[2m[36m(pid=31641)[0m 0         Non-trainable params
[2m[36m(pid=31641)[0m 8.7 K     Total params
[2m[36m(pid=31641)[0m time to fit was 37.25678753852844
[2m[36m(pid=31641)[0m GPU available: False, used: False
[2m[36m(pid=31641)[0m TPU available: None, using: 0 TPU cores
[2m[36m(pid=31641)[0m 
[2m[36m(pid=31641)[0m   | Name      | Type              | Params
[2m[36m(pid=31641)[0m ------------------------------------------------
[2m[36m(pid=31641)[0m 0 | learner   | Learner           | 8.6 K 
[2m[36m(pid=31641)[0m 1 | adversary | Adversary         | 103   
[2m[36m(pid=31641)[0m 2 | loss_fct  | BCEWithLogitsLoss | 0     
[2m[36m(pid=31641)[0m ------------------------------------------------
[2m[36m(pid=31641)[0m 8.7 K     Trainable params
[2m[36m(pid=31641)[0m 0         Non-trainable params
[2m[36m(pid=31641)[0m 8.7 K     Total params
Result for _inner_e8deb_00175:
  auc: 0.6320257663726807
  date: 2021-03-19_14-51-38
  done: false
  experiment_id: 6065935c47c54c1b873f1f7d07ac86e5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 31641
  time_since_restore: 1501.481066942215
  time_this_iter_s: 1501.481066942215
  time_total_s: 1501.481066942215
  timestamp: 1616161898
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00175
  
[2m[36m(pid=31641)[0m time to fit was 37.130287647247314
[2m[36m(pid=31641)[0m Finished run with seed 0 - lr 5 - sec_lr 5 - bs 32 - mean val auc: 0.6320257663726807
== Status ==
Memory usage on this node: 4.6/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 2/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (1 RUNNING, 179 TERMINATED)
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc                 |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00175 | RUNNING    | 145.101.32.82:31641 |           32 |     0 | 5     |    5     |      1 |         1501.48  | 0.632026 |
| _inner_e8deb_00000 | TERMINATED |                     |           32 |     0 | 0.001 |    0.001 |      1 |         9052.73  | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |                     |           64 |     0 | 0.001 |    0.001 |      1 |         4880.39  | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |                     |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |                     |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |                     |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |                     |           32 |     0 | 0.01  |    0.001 |      1 |         7539.4   | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |                     |           64 |     0 | 0.01  |    0.001 |      1 |         3864.76  | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |                     |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |                     |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |                     |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |                     |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |                     |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |                     |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |                     |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |                     |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00015 | TERMINATED |                     |           32 |     0 | 1     |    0.001 |      1 |         3119.15  | 0.912845 |
| _inner_e8deb_00016 | TERMINATED |                     |           64 |     0 | 1     |    0.001 |      1 |         1864.57  | 0.913413 |
| _inner_e8deb_00017 | TERMINATED |                     |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
| _inner_e8deb_00018 | TERMINATED |                     |          256 |     0 | 1     |    0.001 |      1 |          725.295 | 0.913732 |
+--------------------+------------+---------------------+--------------+-------+-------+----------+--------+------------------+----------+
... 160 more trials not shown (160 TERMINATED)


Result for _inner_e8deb_00175:
  auc: 0.6320257663726807
  date: 2021-03-19_14-51-38
  done: true
  experiment_id: 6065935c47c54c1b873f1f7d07ac86e5
  experiment_tag: 175_batch_size=32,eta=0.0,lr=5,sec_lr=5
  hostname: r37n1.lisa.surfsara.nl
  iterations_since_restore: 1
  node_ip: 145.101.32.82
  pid: 31641
  time_since_restore: 1501.481066942215
  time_this_iter_s: 1501.481066942215
  time_total_s: 1501.481066942215
  timestamp: 1616161898
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: e8deb_00175
  
== Status ==
Memory usage on this node: 4.6/376.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/52 CPUs, 0/0 GPUs, 0.0/252.93 GiB heap, 0.0/77.54 GiB objects
Current best trial: e8deb_00019 with auc=0.9138412833213806 and parameters={'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
Result logdir: /home/nolte/fact-ai/grid_search/ARL_Adult_version_1616149298
Number of trials: 180/180 (180 TERMINATED)
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+
| Trial name         | status     | loc   |   batch_size |   eta |    lr |   sec_lr |   iter |   total time (s) |      auc |
|--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------|
| _inner_e8deb_00000 | TERMINATED |       |           32 |     0 | 0.001 |    0.001 |      1 |         9052.73  | 0.908784 |
| _inner_e8deb_00001 | TERMINATED |       |           64 |     0 | 0.001 |    0.001 |      1 |         4880.39  | 0.906858 |
| _inner_e8deb_00002 | TERMINATED |       |          128 |     0 | 0.001 |    0.001 |      1 |         2820.88  | 0.903411 |
| _inner_e8deb_00003 | TERMINATED |       |          256 |     0 | 0.001 |    0.001 |      1 |         1730.81  | 0.897361 |
| _inner_e8deb_00004 | TERMINATED |       |          512 |     0 | 0.001 |    0.001 |      1 |         1215.21  | 0.890098 |
| _inner_e8deb_00005 | TERMINATED |       |           32 |     0 | 0.01  |    0.001 |      1 |         7539.4   | 0.913266 |
| _inner_e8deb_00006 | TERMINATED |       |           64 |     0 | 0.01  |    0.001 |      1 |         3864.76  | 0.912662 |
| _inner_e8deb_00007 | TERMINATED |       |          128 |     0 | 0.01  |    0.001 |      1 |         2667.71  | 0.912655 |
| _inner_e8deb_00008 | TERMINATED |       |          256 |     0 | 0.01  |    0.001 |      1 |         1738.32  | 0.911813 |
| _inner_e8deb_00009 | TERMINATED |       |          512 |     0 | 0.01  |    0.001 |      1 |         1207.23  | 0.912101 |
| _inner_e8deb_00010 | TERMINATED |       |           32 |     0 | 0.1   |    0.001 |      1 |         3300.48  | 0.913491 |
| _inner_e8deb_00011 | TERMINATED |       |           64 |     0 | 0.1   |    0.001 |      1 |         1861.56  | 0.913766 |
| _inner_e8deb_00012 | TERMINATED |       |          128 |     0 | 0.1   |    0.001 |      1 |         1197.15  | 0.9138   |
| _inner_e8deb_00013 | TERMINATED |       |          256 |     0 | 0.1   |    0.001 |      1 |          736.564 | 0.912859 |
| _inner_e8deb_00014 | TERMINATED |       |          512 |     0 | 0.1   |    0.001 |      1 |          668.141 | 0.912879 |
| _inner_e8deb_00015 | TERMINATED |       |           32 |     0 | 1     |    0.001 |      1 |         3119.15  | 0.912845 |
| _inner_e8deb_00016 | TERMINATED |       |           64 |     0 | 1     |    0.001 |      1 |         1864.57  | 0.913413 |
| _inner_e8deb_00017 | TERMINATED |       |          128 |     0 | 1     |    0.001 |      1 |         1131.2   | 0.913408 |
| _inner_e8deb_00018 | TERMINATED |       |          256 |     0 | 1     |    0.001 |      1 |          725.295 | 0.913732 |
| _inner_e8deb_00019 | TERMINATED |       |          512 |     0 | 1     |    0.001 |      1 |          495.228 | 0.913841 |
| _inner_e8deb_00020 | TERMINATED |       |           32 |     0 | 2     |    0.001 |      1 |         3520.68  | 0.826314 |
| _inner_e8deb_00021 | TERMINATED |       |           64 |     0 | 2     |    0.001 |      1 |         2063.59  | 0.829034 |
| _inner_e8deb_00022 | TERMINATED |       |          128 |     0 | 2     |    0.001 |      1 |         1423.35  | 0.830453 |
| _inner_e8deb_00023 | TERMINATED |       |          256 |     0 | 2     |    0.001 |      1 |          975.463 | 0.911412 |
| _inner_e8deb_00024 | TERMINATED |       |          512 |     0 | 2     |    0.001 |      1 |          791.187 | 0.91044  |
| _inner_e8deb_00025 | TERMINATED |       |           32 |     0 | 5     |    0.001 |      1 |         3421.4   | 0.617594 |
| _inner_e8deb_00026 | TERMINATED |       |           64 |     0 | 5     |    0.001 |      1 |         1976.91  | 0.642937 |
| _inner_e8deb_00027 | TERMINATED |       |          128 |     0 | 5     |    0.001 |      1 |          622.547 | 0.51805  |
| _inner_e8deb_00028 | TERMINATED |       |          256 |     0 | 5     |    0.001 |      1 |         1109.74  | 0.712078 |
| _inner_e8deb_00029 | TERMINATED |       |          512 |     0 | 5     |    0.001 |      1 |          638.894 | 0.714204 |
| _inner_e8deb_00030 | TERMINATED |       |           32 |     0 | 0.001 |    0.01  |      1 |         8927.74  | 0.908488 |
| _inner_e8deb_00031 | TERMINATED |       |           64 |     0 | 0.001 |    0.01  |      1 |         4843.98  | 0.907496 |
| _inner_e8deb_00032 | TERMINATED |       |          128 |     0 | 0.001 |    0.01  |      1 |         2716.52  | 0.904642 |
| _inner_e8deb_00033 | TERMINATED |       |          256 |     0 | 0.001 |    0.01  |      1 |         1655.06  | 0.900067 |
| _inner_e8deb_00034 | TERMINATED |       |          512 |     0 | 0.001 |    0.01  |      1 |         1136.19  | 0.890132 |
| _inner_e8deb_00035 | TERMINATED |       |           32 |     0 | 0.01  |    0.01  |      1 |         4552.96  | 0.912166 |
| _inner_e8deb_00036 | TERMINATED |       |           64 |     0 | 0.01  |    0.01  |      1 |         3160.36  | 0.912142 |
| _inner_e8deb_00037 | TERMINATED |       |          128 |     0 | 0.01  |    0.01  |      1 |         2308.4   | 0.912375 |
| _inner_e8deb_00038 | TERMINATED |       |          256 |     0 | 0.01  |    0.01  |      1 |         1633.93  | 0.91213  |
| _inner_e8deb_00039 | TERMINATED |       |          512 |     0 | 0.01  |    0.01  |      1 |         1129.06  | 0.912093 |
| _inner_e8deb_00040 | TERMINATED |       |           32 |     0 | 0.1   |    0.01  |      1 |         2732.2   | 0.913102 |
| _inner_e8deb_00041 | TERMINATED |       |           64 |     0 | 0.1   |    0.01  |      1 |         1767.27  | 0.913312 |
| _inner_e8deb_00042 | TERMINATED |       |          128 |     0 | 0.1   |    0.01  |      1 |         1030.7   | 0.913419 |
| _inner_e8deb_00043 | TERMINATED |       |          256 |     0 | 0.1   |    0.01  |      1 |          655.387 | 0.912964 |
| _inner_e8deb_00044 | TERMINATED |       |          512 |     0 | 0.1   |    0.01  |      1 |          558.893 | 0.913296 |
| _inner_e8deb_00045 | TERMINATED |       |           32 |     0 | 1     |    0.01  |      1 |         3483.47  | 0.912064 |
| _inner_e8deb_00046 | TERMINATED |       |           64 |     0 | 1     |    0.01  |      1 |         1805.8   | 0.913238 |
| _inner_e8deb_00047 | TERMINATED |       |          128 |     0 | 1     |    0.01  |      1 |          941.706 | 0.913636 |
| _inner_e8deb_00048 | TERMINATED |       |          256 |     0 | 1     |    0.01  |      1 |          614.281 | 0.913559 |
| _inner_e8deb_00049 | TERMINATED |       |          512 |     0 | 1     |    0.01  |      1 |          480.393 | 0.913598 |
| _inner_e8deb_00050 | TERMINATED |       |           32 |     0 | 2     |    0.01  |      1 |         3060.31  | 0.810148 |
| _inner_e8deb_00051 | TERMINATED |       |           64 |     0 | 2     |    0.01  |      1 |         1824.06  | 0.909453 |
| _inner_e8deb_00052 | TERMINATED |       |          128 |     0 | 2     |    0.01  |      1 |         1140.73  | 0.828893 |
| _inner_e8deb_00053 | TERMINATED |       |          256 |     0 | 2     |    0.01  |      1 |          906.383 | 0.911645 |
| _inner_e8deb_00054 | TERMINATED |       |          512 |     0 | 2     |    0.01  |      1 |          504.259 | 0.83734  |
| _inner_e8deb_00055 | TERMINATED |       |           32 |     0 | 5     |    0.01  |      1 |         3843.8   | 0.658706 |
| _inner_e8deb_00056 | TERMINATED |       |           64 |     0 | 5     |    0.01  |      1 |         1889.14  | 0.71577  |
| _inner_e8deb_00057 | TERMINATED |       |          128 |     0 | 5     |    0.01  |      1 |         1522.07  | 0.809504 |
| _inner_e8deb_00058 | TERMINATED |       |          256 |     0 | 5     |    0.01  |      1 |          643.47  | 0.696669 |
| _inner_e8deb_00059 | TERMINATED |       |          512 |     0 | 5     |    0.01  |      1 |          257.812 | 0.604499 |
| _inner_e8deb_00060 | TERMINATED |       |           32 |     0 | 0.001 |    0.1   |      1 |         8602.7   | 0.908253 |
| _inner_e8deb_00061 | TERMINATED |       |           64 |     0 | 0.001 |    0.1   |      1 |         4718.32  | 0.907301 |
| _inner_e8deb_00062 | TERMINATED |       |          128 |     0 | 0.001 |    0.1   |      1 |         2655.62  | 0.907075 |
| _inner_e8deb_00063 | TERMINATED |       |          256 |     0 | 0.001 |    0.1   |      1 |         1598.44  | 0.903579 |
| _inner_e8deb_00064 | TERMINATED |       |          512 |     0 | 0.001 |    0.1   |      1 |         1110.54  | 0.895311 |
| _inner_e8deb_00065 | TERMINATED |       |           32 |     0 | 0.01  |    0.1   |      1 |         3298.1   | 0.91011  |
| _inner_e8deb_00066 | TERMINATED |       |           64 |     0 | 0.01  |    0.1   |      1 |         2249.82  | 0.91099  |
| _inner_e8deb_00067 | TERMINATED |       |          128 |     0 | 0.01  |    0.1   |      1 |         1348.59  | 0.910552 |
| _inner_e8deb_00068 | TERMINATED |       |          256 |     0 | 0.01  |    0.1   |      1 |         1226.61  | 0.910381 |
| _inner_e8deb_00069 | TERMINATED |       |          512 |     0 | 0.01  |    0.1   |      1 |          864.423 | 0.910376 |
| _inner_e8deb_00070 | TERMINATED |       |           32 |     0 | 0.1   |    0.1   |      1 |         2492.01  | 0.911624 |
| _inner_e8deb_00071 | TERMINATED |       |           64 |     0 | 0.1   |    0.1   |      1 |         1420.84  | 0.911905 |
| _inner_e8deb_00072 | TERMINATED |       |          128 |     0 | 0.1   |    0.1   |      1 |          772.512 | 0.912112 |
| _inner_e8deb_00073 | TERMINATED |       |          256 |     0 | 0.1   |    0.1   |      1 |          504.507 | 0.911945 |
| _inner_e8deb_00074 | TERMINATED |       |          512 |     0 | 0.1   |    0.1   |      1 |          449.327 | 0.912178 |
| _inner_e8deb_00075 | TERMINATED |       |           32 |     0 | 1     |    0.1   |      1 |         2687.21  | 0.911529 |
| _inner_e8deb_00076 | TERMINATED |       |           64 |     0 | 1     |    0.1   |      1 |         1495.08  | 0.911594 |
| _inner_e8deb_00077 | TERMINATED |       |          128 |     0 | 1     |    0.1   |      1 |          781.789 | 0.912189 |
| _inner_e8deb_00078 | TERMINATED |       |          256 |     0 | 1     |    0.1   |      1 |          528.924 | 0.91273  |
| _inner_e8deb_00079 | TERMINATED |       |          512 |     0 | 1     |    0.1   |      1 |          410.418 | 0.913681 |
| _inner_e8deb_00080 | TERMINATED |       |           32 |     0 | 2     |    0.1   |      1 |         3150.53  | 0.905126 |
| _inner_e8deb_00081 | TERMINATED |       |           64 |     0 | 2     |    0.1   |      1 |         1611.4   | 0.909251 |
| _inner_e8deb_00082 | TERMINATED |       |          128 |     0 | 2     |    0.1   |      1 |         1066.88  | 0.910469 |
| _inner_e8deb_00083 | TERMINATED |       |          256 |     0 | 2     |    0.1   |      1 |          657.088 | 0.746702 |
| _inner_e8deb_00084 | TERMINATED |       |          512 |     0 | 2     |    0.1   |      1 |          403.065 | 0.745827 |
| _inner_e8deb_00085 | TERMINATED |       |           32 |     0 | 5     |    0.1   |      1 |         5434.64  | 0.689836 |
| _inner_e8deb_00086 | TERMINATED |       |           64 |     0 | 5     |    0.1   |      1 |         2946.81  | 0.789742 |
| _inner_e8deb_00087 | TERMINATED |       |          128 |     0 | 5     |    0.1   |      1 |          615.984 | 0.580355 |
| _inner_e8deb_00088 | TERMINATED |       |          256 |     0 | 5     |    0.1   |      1 |          761.325 | 0.654076 |
| _inner_e8deb_00089 | TERMINATED |       |          512 |     0 | 5     |    0.1   |      1 |          468.873 | 0.671461 |
| _inner_e8deb_00090 | TERMINATED |       |           32 |     0 | 0.001 |    1     |      1 |         5155.13  | 0.89059  |
| _inner_e8deb_00091 | TERMINATED |       |           64 |     0 | 0.001 |    1     |      1 |         3545.39  | 0.891636 |
| _inner_e8deb_00092 | TERMINATED |       |          128 |     0 | 0.001 |    1     |      1 |         2571     | 0.88483  |
| _inner_e8deb_00093 | TERMINATED |       |          256 |     0 | 0.001 |    1     |      1 |         1012     | 0.876409 |
| _inner_e8deb_00094 | TERMINATED |       |          512 |     0 | 0.001 |    1     |      1 |          652.241 | 0.85472  |
| _inner_e8deb_00095 | TERMINATED |       |           32 |     0 | 0.01  |    1     |      1 |         2729     | 0.910232 |
| _inner_e8deb_00096 | TERMINATED |       |           64 |     0 | 0.01  |    1     |      1 |         2045.8   | 0.910083 |
| _inner_e8deb_00097 | TERMINATED |       |          128 |     0 | 0.01  |    1     |      1 |         1291.74  | 0.909335 |
| _inner_e8deb_00098 | TERMINATED |       |          256 |     0 | 0.01  |    1     |      1 |          849.505 | 0.909311 |
| _inner_e8deb_00099 | TERMINATED |       |          512 |     0 | 0.01  |    1     |      1 |          815.042 | 0.909275 |
| _inner_e8deb_00100 | TERMINATED |       |           32 |     0 | 0.1   |    1     |      1 |         2380.59  | 0.908908 |
| _inner_e8deb_00101 | TERMINATED |       |           64 |     0 | 0.1   |    1     |      1 |         1215.37  | 0.909695 |
| _inner_e8deb_00102 | TERMINATED |       |          128 |     0 | 0.1   |    1     |      1 |          702.689 | 0.910307 |
| _inner_e8deb_00103 | TERMINATED |       |          256 |     0 | 0.1   |    1     |      1 |          462.436 | 0.911406 |
| _inner_e8deb_00104 | TERMINATED |       |          512 |     0 | 0.1   |    1     |      1 |          307.792 | 0.911722 |
| _inner_e8deb_00105 | TERMINATED |       |           32 |     0 | 1     |    1     |      1 |         2599.69  | 0.908048 |
| _inner_e8deb_00106 | TERMINATED |       |           64 |     0 | 1     |    1     |      1 |         1378.24  | 0.907998 |
| _inner_e8deb_00107 | TERMINATED |       |          128 |     0 | 1     |    1     |      1 |          717.266 | 0.910492 |
| _inner_e8deb_00108 | TERMINATED |       |          256 |     0 | 1     |    1     |      1 |          465.798 | 0.911192 |
| _inner_e8deb_00109 | TERMINATED |       |          512 |     0 | 1     |    1     |      1 |          321.001 | 0.912229 |
| _inner_e8deb_00110 | TERMINATED |       |           32 |     0 | 2     |    1     |      1 |         3152.06  | 0.820811 |
| _inner_e8deb_00111 | TERMINATED |       |           64 |     0 | 2     |    1     |      1 |         1444.37  | 0.843129 |
| _inner_e8deb_00112 | TERMINATED |       |          128 |     0 | 2     |    1     |      1 |          885.026 | 0.906981 |
| _inner_e8deb_00113 | TERMINATED |       |          256 |     0 | 2     |    1     |      1 |          640.1   | 0.907821 |
| _inner_e8deb_00114 | TERMINATED |       |          512 |     0 | 2     |    1     |      1 |          313.827 | 0.829016 |
| _inner_e8deb_00115 | TERMINATED |       |           32 |     0 | 5     |    1     |      1 |         4814.87  | 0.699775 |
| _inner_e8deb_00116 | TERMINATED |       |           64 |     0 | 5     |    1     |      1 |         1764.87  | 0.608674 |
| _inner_e8deb_00117 | TERMINATED |       |          128 |     0 | 5     |    1     |      1 |         1007.78  | 0.634179 |
| _inner_e8deb_00118 | TERMINATED |       |          256 |     0 | 5     |    1     |      1 |          713.601 | 0.611075 |
| _inner_e8deb_00119 | TERMINATED |       |          512 |     0 | 5     |    1     |      1 |          690.151 | 0.631394 |
| _inner_e8deb_00120 | TERMINATED |       |           32 |     0 | 0.001 |    2     |      1 |         4846.07  | 0.897852 |
| _inner_e8deb_00121 | TERMINATED |       |           64 |     0 | 0.001 |    2     |      1 |         4159.4   | 0.89701  |
| _inner_e8deb_00122 | TERMINATED |       |          128 |     0 | 0.001 |    2     |      1 |         2628.17  | 0.888114 |
| _inner_e8deb_00123 | TERMINATED |       |          256 |     0 | 0.001 |    2     |      1 |          866.047 | 0.856351 |
| _inner_e8deb_00124 | TERMINATED |       |          512 |     0 | 0.001 |    2     |      1 |          714.598 | 0.856877 |
| _inner_e8deb_00125 | TERMINATED |       |           32 |     0 | 0.01  |    2     |      1 |         3106.13  | 0.909483 |
| _inner_e8deb_00126 | TERMINATED |       |           64 |     0 | 0.01  |    2     |      1 |         1824.8   | 0.910658 |
| _inner_e8deb_00127 | TERMINATED |       |          128 |     0 | 0.01  |    2     |      1 |         1035.69  | 0.909467 |
| _inner_e8deb_00128 | TERMINATED |       |          256 |     0 | 0.01  |    2     |      1 |          800.11  | 0.90898  |
| _inner_e8deb_00129 | TERMINATED |       |          512 |     0 | 0.01  |    2     |      1 |          799.476 | 0.90906  |
| _inner_e8deb_00130 | TERMINATED |       |           32 |     0 | 0.1   |    2     |      1 |         2418.57  | 0.909477 |
| _inner_e8deb_00131 | TERMINATED |       |           64 |     0 | 0.1   |    2     |      1 |         1237.66  | 0.909493 |
| _inner_e8deb_00132 | TERMINATED |       |          128 |     0 | 0.1   |    2     |      1 |          695.889 | 0.910174 |
| _inner_e8deb_00133 | TERMINATED |       |          256 |     0 | 0.1   |    2     |      1 |          418.06  | 0.911084 |
| _inner_e8deb_00134 | TERMINATED |       |          512 |     0 | 0.1   |    2     |      1 |          322.223 | 0.911983 |
| _inner_e8deb_00135 | TERMINATED |       |           32 |     0 | 1     |    2     |      1 |         2421.97  | 0.906158 |
| _inner_e8deb_00136 | TERMINATED |       |           64 |     0 | 1     |    2     |      1 |         1285.13  | 0.908327 |
| _inner_e8deb_00137 | TERMINATED |       |          128 |     0 | 1     |    2     |      1 |          764.184 | 0.91053  |
| _inner_e8deb_00138 | TERMINATED |       |          256 |     0 | 1     |    2     |      1 |          484.028 | 0.910816 |
| _inner_e8deb_00139 | TERMINATED |       |          512 |     0 | 1     |    2     |      1 |          308.48  | 0.911454 |
| _inner_e8deb_00140 | TERMINATED |       |           32 |     0 | 2     |    2     |      1 |         2777.97  | 0.900539 |
| _inner_e8deb_00141 | TERMINATED |       |           64 |     0 | 2     |    2     |      1 |         1602.97  | 0.743275 |
| _inner_e8deb_00142 | TERMINATED |       |          128 |     0 | 2     |    2     |      1 |          726.883 | 0.745685 |
| _inner_e8deb_00143 | TERMINATED |       |          256 |     0 | 2     |    2     |      1 |          577.176 | 0.885178 |
| _inner_e8deb_00144 | TERMINATED |       |          512 |     0 | 2     |    2     |      1 |          356.292 | 0.826668 |
| _inner_e8deb_00145 | TERMINATED |       |           32 |     0 | 5     |    2     |      1 |         3211.21  | 0.654839 |
| _inner_e8deb_00146 | TERMINATED |       |           64 |     0 | 5     |    2     |      1 |         2532.25  | 0.727882 |
| _inner_e8deb_00147 | TERMINATED |       |          128 |     0 | 5     |    2     |      1 |          982.444 | 0.570081 |
| _inner_e8deb_00148 | TERMINATED |       |          256 |     0 | 5     |    2     |      1 |          577.154 | 0.557867 |
| _inner_e8deb_00149 | TERMINATED |       |          512 |     0 | 5     |    2     |      1 |          625.252 | 0.736412 |
| _inner_e8deb_00150 | TERMINATED |       |           32 |     0 | 0.001 |    5     |      1 |         2806.3   | 0.888985 |
| _inner_e8deb_00151 | TERMINATED |       |           64 |     0 | 0.001 |    5     |      1 |         2683.12  | 0.888405 |
| _inner_e8deb_00152 | TERMINATED |       |          128 |     0 | 0.001 |    5     |      1 |         2261.99  | 0.883366 |
| _inner_e8deb_00153 | TERMINATED |       |          256 |     0 | 0.001 |    5     |      1 |         1124.68  | 0.853027 |
| _inner_e8deb_00154 | TERMINATED |       |          512 |     0 | 0.001 |    5     |      1 |          515.169 | 0.839074 |
| _inner_e8deb_00155 | TERMINATED |       |           32 |     0 | 0.01  |    5     |      1 |         2501.22  | 0.908071 |
| _inner_e8deb_00156 | TERMINATED |       |           64 |     0 | 0.01  |    5     |      1 |         1944.36  | 0.910463 |
| _inner_e8deb_00157 | TERMINATED |       |          128 |     0 | 0.01  |    5     |      1 |          982.404 | 0.909101 |
| _inner_e8deb_00158 | TERMINATED |       |          256 |     0 | 0.01  |    5     |      1 |          816.617 | 0.908937 |
| _inner_e8deb_00159 | TERMINATED |       |          512 |     0 | 0.01  |    5     |      1 |          792.375 | 0.908461 |
| _inner_e8deb_00160 | TERMINATED |       |           32 |     0 | 0.1   |    5     |      1 |         1947.18  | 0.90859  |
| _inner_e8deb_00161 | TERMINATED |       |           64 |     0 | 0.1   |    5     |      1 |         1218.36  | 0.909118 |
| _inner_e8deb_00162 | TERMINATED |       |          128 |     0 | 0.1   |    5     |      1 |          656.532 | 0.910255 |
| _inner_e8deb_00163 | TERMINATED |       |          256 |     0 | 0.1   |    5     |      1 |          446.491 | 0.910607 |
| _inner_e8deb_00164 | TERMINATED |       |          512 |     0 | 0.1   |    5     |      1 |          279.04  | 0.910923 |
| _inner_e8deb_00165 | TERMINATED |       |           32 |     0 | 1     |    5     |      1 |         1681.87  | 0.905573 |
| _inner_e8deb_00166 | TERMINATED |       |           64 |     0 | 1     |    5     |      1 |         1195.5   | 0.909048 |
| _inner_e8deb_00167 | TERMINATED |       |          128 |     0 | 1     |    5     |      1 |          693.465 | 0.909372 |
| _inner_e8deb_00168 | TERMINATED |       |          256 |     0 | 1     |    5     |      1 |          421.637 | 0.910232 |
| _inner_e8deb_00169 | TERMINATED |       |          512 |     0 | 1     |    5     |      1 |          294.543 | 0.911998 |
| _inner_e8deb_00170 | TERMINATED |       |           32 |     0 | 2     |    5     |      1 |         1504.19  | 0.851737 |
| _inner_e8deb_00171 | TERMINATED |       |           64 |     0 | 2     |    5     |      1 |         1349.69  | 0.902892 |
| _inner_e8deb_00172 | TERMINATED |       |          128 |     0 | 2     |    5     |      1 |          649.851 | 0.827436 |
| _inner_e8deb_00173 | TERMINATED |       |          256 |     0 | 2     |    5     |      1 |          472.016 | 0.82581  |
| _inner_e8deb_00174 | TERMINATED |       |          512 |     0 | 2     |    5     |      1 |          374.584 | 0.780611 |
| _inner_e8deb_00175 | TERMINATED |       |           32 |     0 | 5     |    5     |      1 |         1501.48  | 0.632026 |
| _inner_e8deb_00176 | TERMINATED |       |           64 |     0 | 5     |    5     |      1 |         1128.01  | 0.591967 |
| _inner_e8deb_00177 | TERMINATED |       |          128 |     0 | 5     |    5     |      1 |          700.131 | 0.678251 |
| _inner_e8deb_00178 | TERMINATED |       |          256 |     0 | 5     |    5     |      1 |          349.383 | 0.585559 |
| _inner_e8deb_00179 | TERMINATED |       |          512 |     0 | 5     |    5     |      1 |          339.809 | 0.621071 |
+--------------------+------------+-------+--------------+-------+-------+----------+--------+------------------+----------+


2021-03-19 14:51:39,003	INFO tune.py:448 -- Total run time: 12596.87 seconds (12592.10 seconds for the tuning loop).
Best hyperparameters found were:  {'lr': 1, 'sec_lr': 0.001, 'batch_size': 512, 'eta': 0.0}
GPU available: False, used: False
TPU available: None, using: 0 TPU cores

  | Name      | Type              | Params
------------------------------------------------
0 | learner   | Learner           | 8.6 K 
1 | adversary | Adversary         | 103   
2 | loss_fct  | BCEWithLogitsLoss | 0     
------------------------------------------------
8.7 K     Trainable params
0         Non-trainable params
8.7 K     Total params
/home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
/home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 52 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
/home/nolte/.conda/envs/fact-ai-lisa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...
  warnings.warn(*args, **kwargs)
time to fit was 22.92620301246643
Results = {'min_auc': 0.8862430453300476, 'macro_avg_auc': 0.9237168580293655, 'micro_avg_auc': 0.911546528339386, 'minority_auc': 0.9536534547805786, 'accuracy': 0.8554757237434387}
