{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility study on: *Fairness without Demographics through Adversarially Reweighted Learning*\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TomFrederik/fact-ai/blob/main/results.ipynb)\n",
    "\n",
    "This notebook can reproduce all the figures and tables from our report. You can run it from top to bottom without interventions or you can use it more interactively and tweak things along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Setup\n",
    "We first need to import some modules and set up a reasonable default config. You can skip this section, all settings can be adjusted later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available? False\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "import itertools\n",
    "import json\n",
    "import statistics\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from main import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    model='ARL',\n",
    "    prim_hidden=[64, 32],\n",
    "    adv_hidden=[],\n",
    "    eta=0.5,\n",
    "    k=2.0,\n",
    "    pretrained=False,\n",
    "    adv_input=['X', 'Y'],\n",
    "    batch_size=256,\n",
    "    train_steps=5000,\n",
    "    prim_lr=0.1,\n",
    "    seed=0,\n",
    "    seed_run=False,\n",
    "    seed_run_version=0,\n",
    "    pretrain_steps=250,\n",
    "    opt='Adagrad',\n",
    "    log_dir='training_logs',\n",
    "    # progress bar won't work in notebook\n",
    "    p_bar=False,\n",
    "    num_folds=5,\n",
    "    grid_search=False,\n",
    "    eval_batch_size=512,\n",
    "    tf_mode=True,\n",
    "    dataset='Adult',\n",
    "    num_workers=0,\n",
    "    disable_warnings=True,\n",
    "    sensitive_label=False,\n",
    "    num_cpus=1,\n",
    "    num_gpus=0.25\n",
    ")\n",
    "args.dataset_type = 'image' if args.dataset in ['FairFace', 'FairFace_reduced', 'colorMNIST'] else 'tabular'\n",
    "args.working_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = ['baseline', 'DRO', 'ARL', 'IPW']\n",
    "all_datasets = ['Adult', 'LSAC', 'COMPAS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Single training run\n",
    "We begin by training end evaluating a single model to show how our interface works in a simple setting. You can skip this section if you want to immediately reproduce our results.\n",
    "\n",
    "You can change hyperparameters by adjusting attributes of the `args` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.batch_size =\n",
    "\n",
    "# learning rate:\n",
    "# args.prim_lr =\n",
    "\n",
    "# Uncomment to use PyTorch rather than TF defaults for initialization and optimizer\n",
    "# args.tf_mode = False\n",
    "\n",
    "# List of the hidden dimensions for the learner and adversary\n",
    "# args.prim_hidden = []\n",
    "# args.adv_hidden = []\n",
    "\n",
    "# Input that the adversary has access to. X are features, Y labels and S protected group memberships\n",
    "# args.adv_input = ['X', 'Y', 'S']\n",
    "\n",
    "# maximum number of training steps\n",
    "# args.train_steps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and model are also chosen using the `args` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choices are 'ARL', 'baseline', 'DRO', 'IPW' and 'baseline_cnn'\n",
    "args.model = 'ARL'\n",
    "\n",
    "# choices are 'Adult', 'LSAC', 'COMPAS', 'FairFace', 'FairFace_reduced', 'colorMNIST'\n",
    "args.dataset = 'Adult'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can probably leave the remaining settings on their default values, but the documentation of the `main.py` script contains a complete list of attributes that can be set via the `args` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available? False\n",
      "usage: main.py [-h] --model {baseline,ARL,DRO,IPW,baseline_cnn}\n",
      "               [--prim_hidden [PRIM_HIDDEN [PRIM_HIDDEN ...]]]\n",
      "               [--adv_hidden [ADV_HIDDEN [ADV_HIDDEN ...]]] [--eta ETA]\n",
      "               [--k K] [--pretrained] [--adv_input ADV_INPUT [ADV_INPUT ...]]\n",
      "               [--batch_size BATCH_SIZE] [--train_steps TRAIN_STEPS]\n",
      "               [--prim_lr PRIM_LR] [--seed SEED] [--seed_run]\n",
      "               [--seed_run_version SEED_RUN_VERSION]\n",
      "               [--pretrain_steps PRETRAIN_STEPS] [--opt {Adagrad,Adam}]\n",
      "               [--log_dir LOG_DIR] [--p_bar] [--num_folds NUM_FOLDS]\n",
      "               [--no_grid_search] [--eval_batch_size EVAL_BATCH_SIZE]\n",
      "               [--tf_mode] --dataset\n",
      "               {Adult,LSAC,COMPAS,FairFace,FairFace_reduced,colorMNIST}\n",
      "               [--num_workers NUM_WORKERS] [--disable_warnings]\n",
      "               [--sensitive_label] [--num_cpus NUM_CPUS] [--num_gpus NUM_GPUS]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model {baseline,ARL,DRO,IPW,baseline_cnn}\n",
      "  --prim_hidden [PRIM_HIDDEN [PRIM_HIDDEN ...]]\n",
      "                        Number of hidden units in primary network\n",
      "  --adv_hidden [ADV_HIDDEN [ADV_HIDDEN ...]]\n",
      "                        Number of hidden units in adversarial network\n",
      "  --eta ETA             Threshold for single losses that contribute to\n",
      "                        learning objective\n",
      "  --k K                 Exponent to upweight high losses\n",
      "  --pretrained          Whether to load a pretrained dataset from torchvision\n",
      "                        where applicable\n",
      "  --adv_input ADV_INPUT [ADV_INPUT ...]\n",
      "                        Inputs to use for the adversary. Any combination of X\n",
      "                        (features), Y (labels) and S (protected group\n",
      "                        memberships)\n",
      "  --batch_size BATCH_SIZE\n",
      "  --train_steps TRAIN_STEPS\n",
      "  --prim_lr PRIM_LR     Learning rate for primary network\n",
      "  --seed SEED\n",
      "  --seed_run            Whether this is part of a run with multiple seeds\n",
      "  --seed_run_version SEED_RUN_VERSION\n",
      "                        Version of the run with multiple seeds\n",
      "  --pretrain_steps PRETRAIN_STEPS\n",
      "  --opt {Adagrad,Adam}  Name of optimizer\n",
      "  --log_dir LOG_DIR\n",
      "  --p_bar               Whether to use progressbar\n",
      "  --num_folds NUM_FOLDS\n",
      "                        Number of crossvalidation folds\n",
      "  --no_grid_search      Don't optimize batch size and lr via gridsearch\n",
      "  --eval_batch_size EVAL_BATCH_SIZE\n",
      "                        Batch size for evaluation. No effect on training or\n",
      "                        results, set as large as memory allows to maximize\n",
      "                        performance\n",
      "  --tf_mode             Use tensorflow rather than PyTorch defaults where\n",
      "                        possible. Only supports AdaGrad optimizer.\n",
      "  --dataset {Adult,LSAC,COMPAS,FairFace,FairFace_reduced,colorMNIST}\n",
      "  --num_workers NUM_WORKERS\n",
      "                        Number of workers that are used in dataloader\n",
      "  --disable_warnings    Whether to disable warnings about mean and std in the\n",
      "                        dataset\n",
      "  --sensitive_label     If True, target label will be included in list of\n",
      "                        sensitive columns; used for IPW(S+Y)\n",
      "  --num_cpus NUM_CPUS   Number of CPUs used for each trial\n",
      "  --num_gpus NUM_GPUS   Number of GPUs used for each trial\n"
     ]
    }
   ],
   "source": [
    "!python main.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `main` function does the following:\n",
    "1. Load the dataset\n",
    "2. Randomly split off 10% of the training set as a validation set\n",
    "3. Train the model, using the validation set for early stopping\n",
    "4. Load the best model (according to validation performance)\n",
    "5. Evaluate the performance of that model on the test set\n",
    "\n",
    "It prints out the final test performance and also saves it to disk, as well as checkpoints and tensorboard logs. Running it for a single model and dataset should take a few minutes at most.\n",
    "\n",
    "It can also perform a grid search to find the optimal learning rate and batch size, but we won't use that until later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "/home/erik/.miniconda3/envs/fact-ai/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: you defined a validation_step but have no val_dataloader. Skipping validation loop\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | learner   | Learner           | 8.6 K \n",
      "1 | adversary | Adversary         | 103   \n",
      "2 | loss_fct  | BCEWithLogitsLoss | 0     \n",
      "------------------------------------------------\n",
      "8.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.7 K     Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dir ./training_logs/ARL_Adult_0.1_256_5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3733cea645b4f18bb72e75243cf230d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/.miniconda3/envs/fact-ai/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d4552f2c9f41bf89302f947006263f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/.miniconda3/envs/fact-ai/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: training_step returned None if it was on purpose, ignore this warning...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to fit was 75.60707354545593\n",
      "Results = {'min_auc': 0.8681613206863403, 'macro_avg_auc': 0.9054139405488968, 'micro_avg_auc': 0.8955512046813965, 'minority_auc': 0.9349005222320557, 'accuracy': 0.8387691378593445}\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# Runs for all models and datasets\n",
    "To recreate the tables from our report, we need to train every model on each of the datasets. We also need to do several runs with different seeds for each combination. This may take a bit longer (on the order of 1h), so you can also skip directly to the next section and create the tables and figures based on our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading the optimal hyperparameters that we determined using grid search. If you want to do your own grid search, see the corresponding section below first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('optimal_hparams.json') as f:\n",
    "    optimal_hparams = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter settings are simply a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Adult\": {\n",
      "        \"baseline\": {\n",
      "            \"batch_size\": 512,\n",
      "            \"prim_lr\": 2\n",
      "        },\n",
      "        \"DRO\": {\n",
      "            \"batch_size\": 128,\n",
      "            \"prim_lr\": 1,\n",
      "            \"eta\": 0.5\n",
      "        },\n",
      "        \"ARL\": {\n",
      "            \"batch_size\": 128,\n",
      "            \"prim_lr\": 0.1\n",
      "        }\n",
      "    },\n",
      "    \"LSAC\": {\n",
      "        \"baseline\": {\n",
      "            \"batch_size\": 64,\n",
      "            \"prim_lr\": 0.1\n",
      "        },\n",
      "        \"DRO\": {\n",
      "            \"batch_size\": 128,\n",
      "            \"prim_lr\": 1,\n",
      "            \"eta\": 0.6\n",
      "        },\n",
      "        \"ARL\": {\n",
      "            \"batch_size\": 64,\n",
      "            \"prim_lr\": 0.1\n",
      "        }\n",
      "    },\n",
      "    \"COMPAS\": {\n",
      "        \"baseline\": {\n",
      "            \"batch_size\": 256,\n",
      "            \"prim_lr\": 0.1\n",
      "        },\n",
      "        \"DRO\": {\n",
      "            \"batch_size\": 256,\n",
      "            \"prim_lr\": 1,\n",
      "            \"eta\": 0.6\n",
      "        },\n",
      "        \"ARL\": {\n",
      "            \"batch_size\": 256,\n",
      "            \"prim_lr\": 0.1\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(optimal_hparams, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use your own hyperparameters instead, you can therefore just change or overwrite the `optimal_hparams` variable. You are also not limited to setting learning rate and batch size; you can set any of the `args` attributes individually for each model/dataset combination. Any values that are not set use the defaults we specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can adjust here which models and datasets you want to run\n",
    "models = ['ARL', 'DRO', 'baseline']\n",
    "datasets = ['Adult', 'LSAC', 'COMPAS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(seed):\n",
    "    result_dict = {}\n",
    "    for dataset, model in itertools.product(datasets, models):\n",
    "        # don't overwrite the defaults:\n",
    "        temp_args = Namespace(**vars(args))\n",
    "        temp_args.model = model\n",
    "        temp_args.dataset = dataset\n",
    "        temp_args.seed = seed\n",
    "        # set the optimal hyperparameters:\n",
    "        for k, v in optimal_hparams[dataset][model]:\n",
    "            setattr(args, k, v)\n",
    "\n",
    "        # train and evaluate the model:\n",
    "        result_dict[(dataset, model)] = main(args)\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimal_hparams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-25b56d1e3bd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mall_dicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m our_results = {\n",
      "\u001b[0;32m<ipython-input-10-56782bb14915>\u001b[0m in \u001b[0;36mrun_models\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtemp_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# set the optimal hyperparameters:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimal_hparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimal_hparams' is not defined"
     ]
    }
   ],
   "source": [
    "all_dicts = []\n",
    "for seed in range(10):\n",
    "    all_dicts.append(run_models(seed))\n",
    "\n",
    "our_results = {\n",
    "    k: {\n",
    "        'mean': statistics.mean(result_dict[k] for result_dict in all_dicts),\n",
    "        'std': statistics.std(result_dict[k] for result_dict in all_dicts)\n",
    "    } for k in itertools.product(datasets, models)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that loads our own results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you didn't run train your own models in the previous section, the remove the comment in the following line and run it, in order to load our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_results = load_result_dict('training_logs', datasets, models, get_our_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result dictionary contains the mean and standard deviation across runs, for each metric and each dataset/method combination we used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Adult',\n",
       "  'ARL'): {'min_auc': {'mean': 0.8839888334274292,\n",
       "   'std': 0.0009290685557899277}, 'macro_avg_auc': {'mean': 0.9177511766552925,\n",
       "   'std': 0.0007697829337825584}, 'micro_avg_auc': {'mean': 0.909021383523941,\n",
       "   'std': 0.0006983387894980422}, 'minority_auc': {'mean': 0.9392572224140168,\n",
       "   'std': 0.0031013138435674096}, 'accuracy': {'mean': 0.8504084527492524,\n",
       "   'std': 0.0027642594178873365}},\n",
       " ('Adult',\n",
       "  'DRO'): {'min_auc': {'mean': 0.885297292470932,\n",
       "   'std': 0.0006518614005294807}, 'macro_avg_auc': {'mean': 0.918700386583805,\n",
       "   'std': 0.001815989101837644}, 'micro_avg_auc': {'mean': 0.9101847589015961,\n",
       "   'std': 0.0007322508613241217}, 'minority_auc': {'mean': 0.9414238631725311,\n",
       "   'std': 0.0059271811311176065}, 'accuracy': {'mean': 0.8574227571487427,\n",
       "   'std': 0.0015485177592630156}},\n",
       " ('Adult',\n",
       "  'baseline'): {'min_auc': {'mean': 0.8841554284095764,\n",
       "   'std': 0.001854611751477643}, 'macro_avg_auc': {'mean': 0.9192625850439071,\n",
       "   'std': 0.0011282166339539464}, 'micro_avg_auc': {'mean': 0.9093350052833558,\n",
       "   'std': 0.0015697947803360107}, 'minority_auc': {'mean': 0.9439521610736847,\n",
       "   'std': 0.00402528608154091}, 'accuracy': {'mean': 0.8498372375965119,\n",
       "   'std': 0.007023698725317251}},\n",
       " ('LSAC',\n",
       "  'ARL'): {'min_auc': {'mean': 0.7870349168777466,\n",
       "   'std': 0.006611413827233918}, 'macro_avg_auc': {'mean': 0.8062602519989014,\n",
       "   'std': 0.009125115444270854}, 'micro_avg_auc': {'mean': 0.8121891498565674,\n",
       "   'std': 0.0062661862055438415}, 'minority_auc': {'mean': 0.8263799846172333,\n",
       "   'std': 0.01985299888920426}, 'accuracy': {'mean': 0.7501004159450531,\n",
       "   'std': 0.028954358941967292}},\n",
       " ('LSAC',\n",
       "  'DRO'): {'min_auc': {'mean': 0.8024195849895477,\n",
       "   'std': 0.005450376735466344}, 'macro_avg_auc': {'mean': 0.8216530039906502,\n",
       "   'std': 0.005252958069085682}, 'micro_avg_auc': {'mean': 0.8242175877094269,\n",
       "   'std': 0.005053520510365091}, 'minority_auc': {'mean': 0.8360665738582611,\n",
       "   'std': 0.008792840507089778}, 'accuracy': {'mean': 0.856954550743103,\n",
       "   'std': 0.005730779687932392}},\n",
       " ('LSAC',\n",
       "  'baseline'): {'min_auc': {'mean': 0.809587162733078,\n",
       "   'std': 0.007261673989205601}, 'macro_avg_auc': {'mean': 0.8263224929571151,\n",
       "   'std': 0.006320658062567331}, 'micro_avg_auc': {'mean': 0.8308768033981323,\n",
       "   'std': 0.006125600903567921}, 'minority_auc': {'mean': 0.8371349692344665,\n",
       "   'std': 0.008663927583626517}, 'accuracy': {'mean': 0.8603690683841705,\n",
       "   'std': 0.0047114557687987155}},\n",
       " ('COMPAS',\n",
       "  'ARL'): {'min_auc': {'mean': 0.7007744431495666,\n",
       "   'std': 0.0022503030026757194}, 'macro_avg_auc': {'mean': 0.7346980944275856,\n",
       "   'std': 0.003212318848323717}, 'micro_avg_auc': {'mean': 0.736423236131668,\n",
       "   'std': 0.0025655267598587244}, 'minority_auc': {'mean': 0.7481063783168793,\n",
       "   'std': 0.007968981022254707}, 'accuracy': {'mean': 0.6755565941333771,\n",
       "   'std': 0.009070646479130865}},\n",
       " ('COMPAS',\n",
       "  'DRO'): {'min_auc': {'mean': 0.6996595561504364,\n",
       "   'std': 0.0034582863378119053}, 'macro_avg_auc': {'mean': 0.7346805244684219,\n",
       "   'std': 0.0032341360548626716}, 'micro_avg_auc': {'mean': 0.7361120104789733,\n",
       "   'std': 0.003034601991546629}, 'minority_auc': {'mean': 0.7532936871051789,\n",
       "   'std': 0.004436680144277011}, 'accuracy': {'mean': 0.6688311696052551,\n",
       "   'std': 0.0070555726230699944}},\n",
       " ('COMPAS',\n",
       "  'baseline'): {'min_auc': {'mean': 0.6994797766208649,\n",
       "   'std': 0.0029431332945148354}, 'macro_avg_auc': {'mean': 0.7339891374111176,\n",
       "   'std': 0.0026599763383265383}, 'micro_avg_auc': {'mean': 0.7357430100440979,\n",
       "   'std': 0.002495810380890724}, 'minority_auc': {'mean': 0.7475245952606201,\n",
       "   'std': 0.0023939276390155476}, 'accuracy': {'mean': 0.6734693825244904,\n",
       "   'std': 0.005680638192015183}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide helper tools to pretty print these results as Markdown tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "|Dataset|Method|Micro-avg AUC|Macro-avg AUC|Min AUC|Minority AUC|\n",
       "|---|---|---|---|---|---|\n",
       "|Adult | ARL | 0.9090 +- 0.00 | 0.9178 +- 0.00 | 0.8840 +- 0.00 | 0.9393 +- 0.00\\\\\n",
       "|Adult | DRO | 0.9102 +- 0.00 | 0.9187 +- 0.00 | 0.8853 +- 0.00 | 0.9414 +- 0.01\\\\\n",
       "|Adult | baseline | 0.9093 +- 0.00 | 0.9193 +- 0.00 | 0.8842 +- 0.00 | 0.9440 +- 0.00\\\\\n",
       "|LSAC | ARL | 0.8122 +- 0.01 | 0.8063 +- 0.01 | 0.7870 +- 0.01 | 0.8264 +- 0.02\\\\\n",
       "|LSAC | DRO | 0.8242 +- 0.01 | 0.8217 +- 0.01 | 0.8024 +- 0.01 | 0.8361 +- 0.01\\\\\n",
       "|LSAC | baseline | 0.8309 +- 0.01 | 0.8263 +- 0.01 | 0.8096 +- 0.01 | 0.8371 +- 0.01\\\\\n",
       "|COMPAS | ARL | 0.7364 +- 0.00 | 0.7347 +- 0.00 | 0.7008 +- 0.00 | 0.7481 +- 0.01\\\\\n",
       "|COMPAS | DRO | 0.7361 +- 0.00 | 0.7347 +- 0.00 | 0.6997 +- 0.00 | 0.7533 +- 0.00\\\\\n",
       "|COMPAS | baseline | 0.7357 +- 0.00 | 0.7340 +- 0.00 | 0.6995 +- 0.00 | 0.7475 +- 0.00\\\\\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "bold_dict = defaultdict(lambda: defaultdict(bool))\n",
    "keys = ['micro_avg_auc', 'macro_avg_auc', 'min_auc', 'minority_auc']\n",
    "header = \"\"\"\n",
    "|Dataset|Method|Micro-avg AUC|Macro-avg AUC|Min AUC|Minority AUC|\n",
    "|---|---|---|---|---|---|\n",
    "\"\"\"\n",
    "table = create_table(our_results, keys, bold_dict, create_markdown_line_with_std)\n",
    "display(Markdown(header + table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables in our report were generated similarly, just using LaTeX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult & ARL & 0.9090 $\\pm$ 0.00 & 0.9178 $\\pm$ 0.00 & 0.8840 $\\pm$ 0.00 & 0.9393 $\\pm$ 0.00\\\\\n",
      "Adult & DRO & 0.9102 $\\pm$ 0.00 & 0.9187 $\\pm$ 0.00 & 0.8853 $\\pm$ 0.00 & 0.9414 $\\pm$ 0.01\\\\\n",
      "Adult & baseline & 0.9093 $\\pm$ 0.00 & 0.9193 $\\pm$ 0.00 & 0.8842 $\\pm$ 0.00 & 0.9440 $\\pm$ 0.00\\\\\n",
      "LSAC & ARL & 0.8122 $\\pm$ 0.01 & 0.8063 $\\pm$ 0.01 & 0.7870 $\\pm$ 0.01 & 0.8264 $\\pm$ 0.02\\\\\n",
      "LSAC & DRO & 0.8242 $\\pm$ 0.01 & 0.8217 $\\pm$ 0.01 & 0.8024 $\\pm$ 0.01 & 0.8361 $\\pm$ 0.01\\\\\n",
      "LSAC & baseline & 0.8309 $\\pm$ 0.01 & 0.8263 $\\pm$ 0.01 & 0.8096 $\\pm$ 0.01 & 0.8371 $\\pm$ 0.01\\\\\n",
      "COMPAS & ARL & 0.7364 $\\pm$ 0.00 & 0.7347 $\\pm$ 0.00 & 0.7008 $\\pm$ 0.00 & 0.7481 $\\pm$ 0.01\\\\\n",
      "COMPAS & DRO & 0.7361 $\\pm$ 0.00 & 0.7347 $\\pm$ 0.00 & 0.6997 $\\pm$ 0.00 & 0.7533 $\\pm$ 0.00\\\\\n",
      "COMPAS & baseline & 0.7357 $\\pm$ 0.00 & 0.7340 $\\pm$ 0.00 & 0.6995 $\\pm$ 0.00 & 0.7475 $\\pm$ 0.00\\\\\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(create_table(our_results, keys, bold_dict, create_latex_line_with_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to results from the paper\n",
    "Next, we compare our results to those from the paper we're reproducing. Since that paper doesn't report accuracy values, we don't include those in the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in our_results:\n",
    "    del our_results[k][\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate the differences between our and their results (`absolute_errors`) and the standard error of those differences (`total_stds`). From those, we can calculate relative errors (`relative_errors`) that immediately show how significant the deviations are. We use some helper tools that we imported above to add, subtract, etc. dictionaries key-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_means = {k1: {k2: v2[\"mean\"] for k2, v2 in v1.items()} for k1, v1 in our_results.items()}\n",
    "our_stds = {k1: {k2: v2[\"std\"] for k2, v2 in v1.items()} for k1, v1 in our_results.items()}\n",
    "\n",
    "# if you want to compare to some other results instead, for example ones that you got using\n",
    "# different hparams, you can load from other json files here instead\n",
    "# and don't need to change anything else\n",
    "their_results = load_result_dict('paper_results', datasets, models, get_their_path)\n",
    "\n",
    "their_means = {k1: {k2: v2[\"mean\"] for k2, v2 in v1.items()} for k1, v1 in their_results.items()}\n",
    "their_stds = {k1: {k2: v2[\"std\"] for k2, v2 in v1.items()} for k1, v1 in their_results.items()}\n",
    "\n",
    "absolute_errors = subtract(our_means, their_means)\n",
    "total_stds = valmap(add(square(our_stds), square(their_stds)), math.sqrt)\n",
    "relative_errors = div(absolute_errors, total_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again print the results as a Markdown table. `bold_dict` determines which fields of the table will be bolded. In this case, we bold all those relative errors that are at least two standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "|Dataset|Method|Micro-avg AUC|Macro-avg AUC|Min AUC|Minority AUC|\n",
       "|---|---|---|---|---|---|\n",
       "| Adult | ARL | 1.77 | 0.41 | 1.97 | -0.20 |\n",
       "| Adult | DRO | **14.13** | **5.10** | **17.04** | 1.41 |\n",
       "| Adult | baseline | **6.88** | **8.08** | 0.38 | **2.82** |\n",
       "| LSAC | ARL | -1.45 | -1.14 | -1.26 | -0.21 |\n",
       "| LSAC | DRO | **22.78** | **21.16** | **21.90** | **12.00** |\n",
       "| LSAC | baseline | **2.70** | **2.11** | **2.46** | 1.29 |\n",
       "| COMPAS | ARL | -1.77 | **2.08** | **4.47** | **-3.39** |\n",
       "| COMPAS | DRO | **6.44** | **10.73** | **6.22** | **3.88** |\n",
       "| COMPAS | baseline | **-2.85** | 1.66 | **8.20** | **-2.35** |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bold_dict = valmap(relative_errors, lambda x: abs(x) >= 2)\n",
    "\n",
    "keys = ['micro_avg_auc', 'macro_avg_auc', 'min_auc', 'minority_auc']\n",
    "header = \"\"\"\n",
    "|Dataset|Method|Micro-avg AUC|Macro-avg AUC|Min AUC|Minority AUC|\n",
    "|---|---|---|---|---|---|\n",
    "\"\"\"\n",
    "table = create_table(relative_errors, keys, bold_dict, create_markdown_line)\n",
    "display(Markdown(header + table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# Grid search\n",
    "Our `main.py` script can run the entire grid search that we used to determine learning rate and batch size. Simply run `python main.py --model MODEL --dataset DATASET --tf_mode --disable_warnings`. You may also want to set `--num_cpus` and `--num_workers`. Do this for all the models and datasets you want to check. The optimal hyperparameters will be printed and also saved to disk. Then you can enter those in the `optimal_hparams` dictionary above and continue in the section \"Runs for all models and datasets\" to do several different runs with those hyperparameters and average the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fact-ai]",
   "language": "python",
   "name": "conda-env-fact-ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
