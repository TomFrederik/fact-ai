config:
  batch_size: 512
  eta: 0.5
  lr: 2.0
hidden_units:
- 64
- 32
num_features: 101
opt_kwargs:
  initial_accumulator_value: 0.1
optimizer: !!python/name:torch.optim.adagrad.Adagrad ''
